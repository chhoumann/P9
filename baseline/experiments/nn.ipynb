{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.reproduction import major_oxides\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "import torch\n",
    "import keras\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, optimizers\n",
    "\n",
    "def build_model(input_dim, output_dim):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "    model.add(layers.Dense(1024, activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(output_dim))  # No activation, linear output\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['root_mean_squared_error', 'mae'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 6144\n",
    "OUTPUT_DIM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = major_oxides + [\"ID\", \"Sample Name\"]\n",
    "target_cols = major_oxides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.cross_validation import (\n",
    "    get_cross_validation_metrics,\n",
    ")\n",
    "from lib.metrics import rmse_metric, std_dev_metric\n",
    "from functools import partial\n",
    "from lib.deep_learning_utils import get_preprocess_fn, MLFlowCallback\n",
    "from experiments.optuna_run import get_data\n",
    "from lib.norms import Norm3Scaler\n",
    "\n",
    "\n",
    "early_stopping_callback = partial(\n",
    "    keras.callbacks.EarlyStopping, monitor=\"val_loss\", patience=25, restore_best_weights=True\n",
    ")\n",
    "\n",
    "mlflow.set_experiment(f'ANN_{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "\n",
    "\n",
    "args = {\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "\n",
    "\n",
    "def check_group_overlap(train_set, validation_set, group_column=\"Sample Name\"):\n",
    "    train_groups = set(train_set[group_column].unique())\n",
    "    validation_groups = set(validation_set[group_column].unique())\n",
    "    overlap = train_groups.intersection(validation_groups)\n",
    "    assert len(overlap) == 0, f\"Data leakage detected: {overlap}\"\n",
    "\n",
    "\n",
    "def split_dataset_by_group(dataset, split_ratio: float, group_column=\"Sample Name\"):\n",
    "    unique_groups = dataset[group_column].unique()\n",
    "    selected_groups = np.random.choice(unique_groups, size=int(split_ratio * len(unique_groups)), replace=False)\n",
    "    training_set = dataset[dataset[group_column].isin(selected_groups)]\n",
    "    remaining_set = dataset[~dataset[group_column].isin(selected_groups)]\n",
    "\n",
    "    # Check for group overlap\n",
    "    check_group_overlap(training_set, remaining_set, group_column)\n",
    "\n",
    "    return training_set, remaining_set\n",
    "\n",
    "\n",
    "SPLIT_RATIO = 0.2\n",
    "for target in major_oxides:\n",
    "    folds, train, test = get_data(target)\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"ANN_{target}\"):\n",
    "        # == CROSS VALIDATION ==\n",
    "        cv_metrics = []\n",
    "        for cv_train_data, cv_test_data in folds:\n",
    "            train_cv, val_cv = split_dataset_by_group(cv_train_data, SPLIT_RATIO)\n",
    "            check_group_overlap(train_cv, val_cv, \"Sample Name\")\n",
    "\n",
    "            model = build_model(INPUT_DIM, OUTPUT_DIM)\n",
    "\n",
    "            preprocess_fn = get_preprocess_fn([target], drop_cols)\n",
    "\n",
    "            scaler = Norm3Scaler()\n",
    "            scaler.fit(train_cv)\n",
    "            train_cv = scaler.transform(train_cv)\n",
    "            val_cv = scaler.transform(val_cv)\n",
    "            cv_test_data = scaler.transform(cv_test_data)\n",
    "\n",
    "            X_train, y_train = preprocess_fn(train_cv)\n",
    "            X_val, y_val = preprocess_fn(val_cv)\n",
    "            X_test, y_test = preprocess_fn(cv_test_data)\n",
    "\n",
    "            model.fit(\n",
    "                X_train, y_train, **args, callbacks=[early_stopping_callback()], validation_data=(X_val, y_val)\n",
    "            )  # don't want to use mlflow callback here\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            rmse = rmse_metric(y_test, y_pred)\n",
    "            std_dev = std_dev_metric(y_test, y_pred)\n",
    "            cv_metrics.append([rmse, std_dev])\n",
    "\n",
    "        mlflow.log_metrics(get_cross_validation_metrics(cv_metrics).as_dict())\n",
    "\n",
    "        # == TRAIN ON ALL DATA ==\n",
    "        model = build_model(INPUT_DIM, OUTPUT_DIM)\n",
    "        preprocess_fn = get_preprocess_fn([target], drop_cols)\n",
    "        train, validation = split_dataset_by_group(train, SPLIT_RATIO)\n",
    "\n",
    "        check_group_overlap(train, validation, \"Sample Name\")\n",
    "\n",
    "        scaler = Norm3Scaler()\n",
    "        scaler.fit(train)\n",
    "        train = scaler.transform(train)\n",
    "        validation = scaler.transform(validation)\n",
    "        test = scaler.transform(test)\n",
    "\n",
    "        X_train, y_train = preprocess_fn(train)\n",
    "        X_val, y_val = preprocess_fn(validation)\n",
    "        X_test, y_test = preprocess_fn(test)\n",
    "\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            **args,\n",
    "            callbacks=[MLFlowCallback(), early_stopping_callback()],\n",
    "            validation_data=(X_val, y_val),\n",
    "        )\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        std_dev = std_dev_metric(y_test, y_pred)\n",
    "        rmse = rmse_metric(y_test, y_pred)\n",
    "        mlflow.log_metrics({\"rmse\": rmse, \"std_dev\": std_dev})\n",
    "\n",
    "        mlflow.log_params({**args, \"target\": target})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

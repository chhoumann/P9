{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from lib.reproduction import major_oxides\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import full_flow_dataloader\n",
    "from lib.config import AppConfig\n",
    "\n",
    "mlflow.set_tracking_uri(AppConfig().mlflow_tracking_uri)\n",
    "\n",
    "drop_cols = [\"ID\", \"Sample Name\"]\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler, PowerTransformer\n",
    "\n",
    "train_processed, test_processed = full_flow_dataloader.load_full_flow_data(load_cache_if_exits=True, average_shots=True)\n",
    "target = major_oxides[0]\n",
    "\n",
    "drop_cols.extend([oxide for oxide in major_oxides if oxide != target])\n",
    "train_processed = train_processed.drop(columns=drop_cols)\n",
    "test_processed = test_processed.drop(columns=drop_cols)\n",
    "\n",
    "# Applying MaxAbsScaler and PowerTransformer\n",
    "scaler = MaxAbsScaler()\n",
    "power_transformer = PowerTransformer()\n",
    "\n",
    "# Exclude target column from normalization\n",
    "cols_to_normalize = [col for col in train_processed.columns if col not in drop_cols + [target]]\n",
    "\n",
    "# Normalize only the columns that are not in drop_cols or the target column\n",
    "train_processed[cols_to_normalize] = scaler.fit_transform(train_processed[cols_to_normalize])\n",
    "train_processed[cols_to_normalize] = power_transformer.fit_transform(train_processed[cols_to_normalize])\n",
    "\n",
    "test_processed[cols_to_normalize] = scaler.transform(test_processed[cols_to_normalize])\n",
    "test_processed[cols_to_normalize] = power_transformer.transform(test_processed[cols_to_normalize])\n",
    "\n",
    "# Reconstruct the DataFrame to include the target column\n",
    "train_processed = pd.DataFrame(train_processed, columns=train_processed.columns)\n",
    "test_processed = pd.DataFrame(test_processed, columns=test_processed.columns)\n",
    "\n",
    "X_train = train_processed.drop(columns=[target])\n",
    "y_train = train_processed[target]\n",
    "\n",
    "X_test = test_processed.drop(columns=[target])\n",
    "y_test = test_processed[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.html\n",
    "def get_or_create_experiment(experiment_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve the ID of an existing MLflow experiment or create a new one if it doesn't exist.\n",
    "\n",
    "    This function checks if an experiment with the given name exists within MLflow.\n",
    "    If it does, the function returns its ID. If not, it creates a new experiment\n",
    "    with the provided name and returns its ID.\n",
    "\n",
    "    Parameters:\n",
    "    - experiment_name (str): Name of the MLflow experiment.\n",
    "\n",
    "    Returns:\n",
    "    - str: ID of the existing or newly created MLflow experiment.\n",
    "    \"\"\"\n",
    "\n",
    "    if experiment := mlflow.get_experiment_by_name(experiment_name):\n",
    "        return experiment.experiment_id\n",
    "    else:\n",
    "        return mlflow.create_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'202075117645542195'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_id = get_or_create_experiment(\"optuna_experiment\")\n",
    "\n",
    "experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/202075117645542195', creation_time=1713779016259, experiment_id='202075117645542195', last_update_time=1713779016259, lifecycle_stage='active', name='optuna_experiment', tags={}>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override Optuna's default logging to ERROR only\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "# define a logging callback that will report on only new challenger parameter configurations if a\n",
    "# trial has usurped the state of 'best conditions'\n",
    "\n",
    "\n",
    "# https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.html\n",
    "def champion_callback(study, frozen_trial):\n",
    "    \"\"\"\n",
    "    Logging callback that will report when a new trial iteration improves upon existing\n",
    "    best trial values.\n",
    "\n",
    "    Note: This callback is not intended for use in distributed computing systems such as Spark\n",
    "    or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's\n",
    "    workers or agents.\n",
    "    The race conditions with file system state management for distributed trials will render\n",
    "    inconsistent values with this callback.\n",
    "    \"\"\"\n",
    "\n",
    "    winner = study.user_attrs.get(\"winner\", None)\n",
    "\n",
    "    if study.best_value and winner != study.best_value:\n",
    "        study.set_user_attr(\"winner\", study.best_value)\n",
    "        if winner:\n",
    "            improvement_percent = (abs(winner - study.best_value) / study.best_value) * 100\n",
    "            print(\n",
    "                f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"\n",
    "                f\"{improvement_percent: .4f}% improvement\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Define hyperparameters\n",
    "        params = {\n",
    "            \"C\": trial.suggest_float(\"C\", 1e-3, 100.0, log=True),\n",
    "            \"epsilon\": trial.suggest_float(\"epsilon\", 1e-3, 1.0, log=True),\n",
    "            \"kernel\": trial.suggest_categorical(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "        }\n",
    "\n",
    "        if params[\"kernel\"] == \"poly\":\n",
    "            params[\"degree\"] = trial.suggest_int(\"degree\", 2, 5)\n",
    "\n",
    "        # Train SVR model\n",
    "        model = SVR(C=params[\"C\"], epsilon=params[\"epsilon\"], kernel=params[\"kernel\"], degree=params.get(\"degree\", 3))\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        error = mean_squared_error(y_test, preds)\n",
    "\n",
    "        # Log to MLflow\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"mse\", float(error))\n",
    "        mlflow.log_metric(\"rmse\", math.sqrt(error))\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"SVR_{target}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial trial 0 achieved value: 474.3755621384993\n",
      "Trial 1 achieved value: 132.25590196816165 with  258.6801% improvement\n",
      "Trial 2 achieved value: 66.51084247089571 with  98.8486% improvement\n",
      "Trial 3 achieved value: 13.825796864355281 with  381.0634% improvement\n",
      "Trial 48 achieved value: 13.82212665508972 with  0.0266% improvement\n",
      "Trial 49 achieved value: 13.818523543958706 with  0.0261% improvement\n"
     ]
    }
   ],
   "source": [
    "# Initiate the parent run and call the hyperparameter tuning child run logic\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=run_name, nested=True):\n",
    "    # Initialize the Optuna study\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "    # Execute the hyperparameter optimization trials.\n",
    "    # Note the addition of the `champion_callback` inclusion to control our logging\n",
    "    study.optimize(objective, n_trials=500, callbacks=[champion_callback])\n",
    "\n",
    "    mlflow.log_params(study.best_params)\n",
    "    mlflow.log_metric(\"best_mse\", study.best_value)\n",
    "    mlflow.log_metric(\"best_rmse\", math.sqrt(study.best_value))\n",
    "\n",
    "    # Log tags\n",
    "    mlflow.set_tags(\n",
    "        tags={\n",
    "            \"project\": \"Stacking SVR\",\n",
    "            \"optimizer_engine\": \"optuna\",\n",
    "            \"model_family\": \"SVR\",\n",
    "            \"feature_set_version\": 1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Log a fit model instance\n",
    "    model = SVR(**study.best_params)\n",
    "\n",
    "    artifact_path = \"model\"\n",
    "\n",
    "    mlflow.sklearn.log_model(\n",
    "        sklearn_model=model,\n",
    "        artifact_path=artifact_path,\n",
    "        input_example=train_processed.iloc[[0]],\n",
    "        model_format=\"ubj\",\n",
    "        metadata={\"model_data_version\": 1},\n",
    "    )\n",
    "\n",
    "    # Get the logged model uri so that we can load it from the artifact store\n",
    "    model_uri = mlflow.get_artifact_uri(artifact_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

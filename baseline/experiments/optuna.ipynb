{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from lib.reproduction import major_oxides\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import full_flow_dataloader\n",
    "from lib.config import AppConfig\n",
    "\n",
    "mlflow.set_tracking_uri(AppConfig().mlflow_tracking_uri)\n",
    "\n",
    "drop_cols = [\"ID\", \"Sample Name\"]\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler, PowerTransformer\n",
    "\n",
    "train_processed, test_processed = full_flow_dataloader.load_full_flow_data(load_cache_if_exits=True, average_shots=True)\n",
    "target = major_oxides[0]\n",
    "\n",
    "drop_cols.extend([oxide for oxide in major_oxides if oxide != target])\n",
    "train_processed = train_processed.drop(columns=drop_cols)\n",
    "test_processed = test_processed.drop(columns=drop_cols)\n",
    "\n",
    "# Applying MaxAbsScaler and PowerTransformer\n",
    "scaler = MaxAbsScaler()\n",
    "power_transformer = PowerTransformer()\n",
    "\n",
    "# Exclude target column from normalization\n",
    "cols_to_normalize = [col for col in train_processed.columns if col not in drop_cols + [target]]\n",
    "\n",
    "# Normalize only the columns that are not in drop_cols or the target column\n",
    "train_processed[cols_to_normalize] = scaler.fit_transform(train_processed[cols_to_normalize])\n",
    "train_processed[cols_to_normalize] = power_transformer.fit_transform(train_processed[cols_to_normalize])\n",
    "\n",
    "test_processed[cols_to_normalize] = scaler.transform(test_processed[cols_to_normalize])\n",
    "test_processed[cols_to_normalize] = power_transformer.transform(test_processed[cols_to_normalize])\n",
    "\n",
    "# Reconstruct the DataFrame to include the target column\n",
    "train_processed = pd.DataFrame(train_processed, columns=train_processed.columns)\n",
    "test_processed = pd.DataFrame(test_processed, columns=test_processed.columns)\n",
    "\n",
    "X_train = train_processed.drop(columns=[target])\n",
    "y_train = train_processed[target]\n",
    "\n",
    "X_test = test_processed.drop(columns=[target])\n",
    "y_test = test_processed[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.html\n",
    "def get_or_create_experiment(experiment_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve the ID of an existing MLflow experiment or create a new one if it doesn't exist.\n",
    "\n",
    "    This function checks if an experiment with the given name exists within MLflow.\n",
    "    If it does, the function returns its ID. If not, it creates a new experiment\n",
    "    with the provided name and returns its ID.\n",
    "\n",
    "    Parameters:\n",
    "    - experiment_name (str): Name of the MLflow experiment.\n",
    "\n",
    "    Returns:\n",
    "    - str: ID of the existing or newly created MLflow experiment.\n",
    "    \"\"\"\n",
    "\n",
    "    if experiment := mlflow.get_experiment_by_name(experiment_name):\n",
    "        return experiment.experiment_id\n",
    "    else:\n",
    "        return mlflow.create_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'202075117645542195'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_id = get_or_create_experiment(\"optuna_experiment\")\n",
    "\n",
    "experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/202075117645542195', creation_time=1713779016259, experiment_id='202075117645542195', last_update_time=1713779016259, lifecycle_stage='active', name='optuna_experiment', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override Optuna's default logging to ERROR only\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "# define a logging callback that will report on only new challenger parameter configurations if a\n",
    "# trial has usurped the state of 'best conditions'\n",
    "\n",
    "\n",
    "# https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.html\n",
    "def champion_callback(study, frozen_trial):\n",
    "    \"\"\"\n",
    "    Logging callback that will report when a new trial iteration improves upon existing\n",
    "    best trial values.\n",
    "\n",
    "    Note: This callback is not intended for use in distributed computing systems such as Spark\n",
    "    or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's\n",
    "    workers or agents.\n",
    "    The race conditions with file system state management for distributed trials will render\n",
    "    inconsistent values with this callback.\n",
    "    \"\"\"\n",
    "\n",
    "    winner = study.user_attrs.get(\"winner\", None)\n",
    "\n",
    "    if study.best_value and winner != study.best_value:\n",
    "        study.set_user_attr(\"winner\", study.best_value)\n",
    "        if winner:\n",
    "            improvement_percent = (abs(winner - study.best_value) / study.best_value) * 100\n",
    "            print(\n",
    "                f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"\n",
    "                f\"{improvement_percent: .4f}% improvement\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Define hyperparameters\n",
    "        params = {\n",
    "            \"C\": trial.suggest_float(\"C\", 1e-3, 100.0, log=True),\n",
    "            \"epsilon\": trial.suggest_float(\"epsilon\", 1e-3, 1.0, log=True),\n",
    "            \"kernel\": trial.suggest_categorical(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "            \"gamma\": trial.suggest_categorical(\"gamma\", [\"scale\", \"auto\"]),\n",
    "            \"coef0\": trial.suggest_float(\"coef0\", 0.0, 10.0),\n",
    "            \"shrinking\": trial.suggest_categorical(\"shrinking\", [True, False]),\n",
    "            \"tol\": trial.suggest_float(\"tol\", 1e-5, 1e-1, log=True),\n",
    "            \"max_iter\": trial.suggest_int(\"max_iter\", -1, 1000)\n",
    "        }\n",
    "\n",
    "        if params[\"kernel\"] == \"poly\":\n",
    "            params[\"degree\"] = trial.suggest_int(\"degree\", 2, 5)\n",
    "\n",
    "        # Train SVR model\n",
    "        model = SVR(\n",
    "            C=params[\"C\"], \n",
    "            epsilon=params[\"epsilon\"], \n",
    "            kernel=params[\"kernel\"], \n",
    "            degree=params.get(\"degree\", 3),\n",
    "            gamma=params[\"gamma\"],\n",
    "            coef0=params[\"coef0\"],\n",
    "            shrinking=params[\"shrinking\"],\n",
    "            tol=params[\"tol\"],\n",
    "            max_iter=params[\"max_iter\"]\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        error = mean_squared_error(y_test, preds)\n",
    "\n",
    "        # Log to MLflow\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"mse\", float(error))\n",
    "        mlflow.log_metric(\"rmse\", math.sqrt(error))\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"SVR_{target}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the parent run and call the hyperparameter tuning child run logic\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=run_name, nested=True):\n",
    "    # Initialize the Optuna study\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "    # Execute the hyperparameter optimization trials.\n",
    "    # Note the addition of the `champion_callback` inclusion to control our logging\n",
    "    study.optimize(objective, n_trials=500, callbacks=[champion_callback])\n",
    "\n",
    "    mlflow.log_params(study.best_params)\n",
    "    mlflow.log_metric(\"best_mse\", study.best_value)\n",
    "    mlflow.log_metric(\"best_rmse\", math.sqrt(study.best_value))\n",
    "\n",
    "    # Log tags\n",
    "    mlflow.set_tags(\n",
    "        tags={\n",
    "            \"project\": \"Stacking SVR\",\n",
    "            \"optimizer_engine\": \"optuna\",\n",
    "            \"model_family\": \"SVR\",\n",
    "            \"feature_set_version\": 1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Log a fit model instance\n",
    "    model = SVR(**study.best_params)\n",
    "\n",
    "    artifact_path = \"model\"\n",
    "    \n",
    "    mlflow.sklearn.log_model(\n",
    "        model,\n",
    "        artifact_path=artifact_path,\n",
    "        input_example=train_processed.iloc[[0]],\n",
    "        serialization_format=\"pickle\",\n",
    "        registered_model_name=\"SVR_model_version_1\"\n",
    "    )\n",
    "    \n",
    "    # Get the logged model uri so that we can load it from the artifact store\n",
    "    model_uri = mlflow.get_artifact_uri(artifact_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing for SiO2\n",
      "Initial trial 0 achieved value: 4.291536300411349\n",
      "Trial 1 achieved value: 3.859550415965798 with  11.1926% improvement\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "import mlflow\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from optuna_models import (\n",
    "    instantiate_extra_trees,\n",
    "    instantiate_gbr,\n",
    "    instantiate_pls,\n",
    "    instantiate_svr,\n",
    "    instantiate_xgboost,\n",
    ")\n",
    "from optuna_preprocessors import (\n",
    "    instantiate_min_max_scaler,\n",
    "    instantiate_power_transformer,\n",
    "    instantiate_robust_scaler,\n",
    "    instantiate_standard_scaler,\n",
    ")\n",
    "from sklearn.preprocessing import MaxAbsScaler, PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from lib import full_flow_dataloader\n",
    "from lib.config import AppConfig\n",
    "from lib.reproduction import major_oxides\n",
    "\n",
    "mlflow.set_tracking_uri(AppConfig().mlflow_tracking_uri)\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "drop_cols = [\"ID\", \"Sample Name\"]\n",
    "\n",
    "\n",
    "train_processed, test_processed = full_flow_dataloader.load_full_flow_data(load_cache_if_exits=True, average_shots=True)\n",
    "target = major_oxides[0]\n",
    "\n",
    "drop_cols.extend([oxide for oxide in major_oxides if oxide != target])\n",
    "train_processed = train_processed.drop(columns=drop_cols)\n",
    "test_processed = test_processed.drop(columns=drop_cols)\n",
    "\n",
    "X_train = train_processed.drop(columns=[target])\n",
    "y_train = train_processed[target]\n",
    "\n",
    "X_test = test_processed.drop(columns=[target])\n",
    "y_test = test_processed[target]\n",
    "\n",
    "\n",
    "# https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.html\n",
    "def get_or_create_experiment(experiment_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve the ID of an existing MLflow experiment or create a new one if it doesn't exist.\n",
    "\n",
    "    This function checks if an experiment with the given name exists within MLflow.\n",
    "    If it does, the function returns its ID. If not, it creates a new experiment\n",
    "    with the provided name and returns its ID.\n",
    "\n",
    "    Parameters:\n",
    "    - experiment_name (str): Name of the MLflow experiment.\n",
    "\n",
    "    Returns:\n",
    "    - str: ID of the existing or newly created MLflow experiment.\n",
    "    \"\"\"\n",
    "\n",
    "    if experiment := mlflow.get_experiment_by_name(experiment_name):\n",
    "        return experiment.experiment_id\n",
    "    else:\n",
    "        return mlflow.create_experiment(experiment_name)\n",
    "\n",
    "\n",
    "# define a logging callback that will report on only new challenger parameter configurations if a\n",
    "# trial has usurped the state of 'best conditions'\n",
    "\n",
    "\n",
    "# https://mlflow.org/docs/latest/traditional-ml/hyperparameter-tuning-with-child-runs/notebooks/hyperparameter-tuning-with-child-runs.html\n",
    "def champion_callback(study, frozen_trial):\n",
    "    \"\"\"\n",
    "    Logging callback that will report when a new trial iteration improves upon existing\n",
    "    best trial values.\n",
    "\n",
    "    Note: This callback is not intended for use in distributed computing systems such as Spark\n",
    "    or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's\n",
    "    workers or agents.\n",
    "    The race conditions with file system state management for distributed trials will render\n",
    "    inconsistent values with this callback.\n",
    "    \"\"\"\n",
    "\n",
    "    winner = study.user_attrs.get(\"winner\", None)\n",
    "\n",
    "    if study.best_value and winner != study.best_value:\n",
    "        study.set_user_attr(\"winner\", study.best_value)\n",
    "        if winner:\n",
    "            improvement_percent = (abs(winner - study.best_value) / study.best_value) * 100\n",
    "            print(\n",
    "                f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"\n",
    "                f\"{improvement_percent: .4f}% improvement\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\")\n",
    "\n",
    "\n",
    "def combined_objective(trial):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Select and instantiate a model\n",
    "        model_selector = trial.suggest_categorical(\"model_type\", [\"gbr\", \"svr\", \"xgboost\", \"extra_trees\", \"pls\"])\n",
    "        if model_selector == \"gbr\":\n",
    "            model = instantiate_gbr(trial, lambda params: mlflow.log_params(params))\n",
    "        elif model_selector == \"svr\":\n",
    "            model = instantiate_svr(trial, lambda params: mlflow.log_params(params))\n",
    "        elif model_selector == \"xgboost\":\n",
    "            model = instantiate_xgboost(trial, lambda params: mlflow.log_params(params))\n",
    "        elif model_selector == \"extra_trees\":\n",
    "            model = instantiate_extra_trees(trial, lambda params: mlflow.log_params(params))\n",
    "        elif model_selector == \"pls\":\n",
    "            model = instantiate_pls(trial, lambda params: mlflow.log_params(params))\n",
    "\n",
    "        # Select and instantiate a preprocessor\n",
    "        preprocessor_selector = trial.suggest_categorical(\n",
    "            \"preprocessor_type\", [\"robust_scaler\", \"standard_scaler\", \"min_max_scaler\", \"power_transformer\"]\n",
    "        )\n",
    "        if preprocessor_selector == \"robust_scaler\":\n",
    "            preprocessor = instantiate_robust_scaler(trial, lambda params: mlflow.log_params(params))\n",
    "        elif preprocessor_selector == \"standard_scaler\":\n",
    "            preprocessor = instantiate_standard_scaler(trial, lambda params: mlflow.log_params(params))\n",
    "        elif preprocessor_selector == \"min_max_scaler\":\n",
    "            preprocessor = instantiate_min_max_scaler(trial, lambda params: mlflow.log_params(params))\n",
    "        elif preprocessor_selector == \"power_transformer\":\n",
    "            preprocessor = instantiate_power_transformer(trial, lambda params: mlflow.log_params(params))\n",
    "        mlflow.log_params({\n",
    "            \"model_type\": model_selector,\n",
    "            \"preprocessor_type\": preprocessor_selector\n",
    "        })\n",
    "\n",
    "        # Preprocess the data\n",
    "        X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "        X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train_transformed, y_train)\n",
    "        preds = model.predict(X_test_transformed)\n",
    "        mse = mean_squared_error(y_test, preds)\n",
    "        rmse = math.sqrt(mse)\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"mse\", float(mse))\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    experiment_id = get_or_create_experiment(\"optuna_experiment_1\")\n",
    "    mlflow.set_experiment(experiment_id=experiment_id)\n",
    "\n",
    "    for oxide in major_oxides:\n",
    "        run_name = oxide\n",
    "        print(f\"Optimizing for {oxide}\")\n",
    "        with mlflow.start_run(experiment_id=experiment_id, run_name=run_name, nested=True):\n",
    "            # Initialize the Optuna study\n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "            # Execute the hyperparameter optimization trials.\n",
    "            # Note the addition of the `champion_callback` inclusion to control our logging\n",
    "            study.optimize(combined_objective, n_trials=500, callbacks=[champion_callback])\n",
    "\n",
    "            mlflow.log_params(study.best_params)\n",
    "            mlflow.log_metric(\"best_mse\", study.best_value)\n",
    "            mlflow.log_metric(\"best_rmse\", math.sqrt(study.best_value))\n",
    "\n",
    "            # Log tags\n",
    "            mlflow.set_tags(\n",
    "                tags={\n",
    "                    \"project\": \"AutoML Experiments\",\n",
    "                    \"optimizer_engine\": \"optuna\",\n",
    "                    \"feature_set_version\": 1,\n",
    "                }\n",
    "            )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

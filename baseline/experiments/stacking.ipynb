{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "from lib.full_flow_dataloader import load_full_flow_data\n",
    "from lib.reproduction import major_oxides\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler, PowerTransformer, StandardScaler, RobustScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import StackingRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_preprocess_data(train: pd.DataFrame, test: pd.DataFrame, preprocesser_pipeline: Pipeline) -> tuple:\n",
    "    drop_cols = major_oxides + [\"ID\", \"Sample Name\"]\n",
    "\n",
    "    # Split data\n",
    "    X_train = train.drop(columns=drop_cols)\n",
    "    X_test = test.drop(columns=drop_cols)\n",
    "    y_train = train[major_oxides]\n",
    "    y_test = test[major_oxides]\n",
    "\n",
    "    # Preprocess data\n",
    "    X_train = preprocesser_pipeline.fit_transform(X_train)\n",
    "    X_test = preprocesser_pipeline.transform(X_test)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible idea: store preprocess pipeline and related model pipeline in hashmap so that you could give it a key of the \n",
    "# target and apply the corresponding pipeline\n",
    "\n",
    "preprocessor_pipeline = Pipeline([\n",
    "    (\"scaler\", MaxAbsScaler()),\n",
    "    #(\"scaler\", StandardScaler()),\n",
    "    #(\"scaler\", RobustScaler(quantile_range=(10, 90))),\n",
    "    (\"transformer\", PowerTransformer()),\n",
    "    (\"pca\", KernelPCA(n_components=60, kernel=\"poly\"))\n",
    "    #('pca', PCA(n_components=34))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_full_flow_data()\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_and_preprocess_data(train, test, preprocessor_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating models\n",
    "\n",
    "# ---- SVR ----\n",
    "kernel=\"poly\"\n",
    "C=100\n",
    "eps=0.1\n",
    "gamma=\"scale\"\n",
    "degree=2\n",
    "coef0=1.0\n",
    "\n",
    "svr = SVR(kernel=kernel, C=C, epsilon=eps, gamma=gamma, degree=degree, coef0=coef0)\n",
    "\n",
    "# ---- ExtraTreesRegressor ----\n",
    "n_estimators = 5\n",
    "max_depth = None\n",
    "min_samples_split = 2\n",
    "min_samples_leaf = 1\n",
    "max_features = 0.3\n",
    "\n",
    "etr = ExtraTreesRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features, random_state=42)\n",
    "\n",
    "# ---- Gradient boosting regressor ----\n",
    "\n",
    "gbr_params = {\n",
    "    'loss': 'squared_error',\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 1.0,\n",
    "    'criterion': 'friedman_mse',\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'max_depth': 3,\n",
    "    'max_features': None,\n",
    "    'random_state': 42,\n",
    "    'verbose': 0,\n",
    "    'validation_fraction': 0.1,\n",
    "    'n_iter_no_change': None,\n",
    "    'tol': 1e-4,\n",
    "    'ccp_alpha': 0.0\n",
    "}\n",
    "\n",
    "gbr = GradientBoostingRegressor(**gbr_params)\n",
    "\n",
    "# ---- XGBoost ----\n",
    "\n",
    "\n",
    "xgb_params = {\n",
    "    \"max_depth\": 4,  # Slightly deeper trees since data is high-dimensional\n",
    "    \"min_child_weight\": 5,  # Higher to control over-fitting\n",
    "    \"gamma\": 0.1,  # Minimum loss reduction required to make further partition\n",
    "    \"subsample\": 0.7,  # Subsample ratio of the training instances\n",
    "    \"colsample_bytree\": 0.5,  # Subsample ratio of columns when constructing each tree\n",
    "    \"colsample_bylevel\": 0.5,  # Subsample ratio of columns for each level\n",
    "    \"colsample_bynode\": 0.5,  # Subsample ratio of columns for each split\n",
    "    \"lambda\": 1,  # L2 regularization term on weights (lambda)\n",
    "    \"alpha\": 0.5,  # L1 regularization term on weights (alpha)\n",
    "    \"learning_rate\": 0.05,  # Step size shrinkage used in update to prevent overfitting\n",
    "    \"n_estimators\": 100,  # Number of boosting rounds\n",
    "    \"objective\": \"reg:squarederror\",  # Regression with squared loss\n",
    "    \"eval_metric\": \"rmse\",  # Evaluation metric for validation data\n",
    "}\n",
    "\n",
    "xgb = xgb.XGBRegressor(**xgb_params)\n",
    "\n",
    "\n",
    "# ---- PLS ----\n",
    "\n",
    "n_components = 15\n",
    "\n",
    "pls = PLSRegression(n_components=n_components)\n",
    "\n",
    "\n",
    "# ---- ElasticNet ----\n",
    "\n",
    "alpha = 0.01\n",
    "l1_ratio = 0.3\n",
    "\n",
    "eln = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "    model.add(layers.Reshape((48, 128, 1)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Additional convolutional block for better feature extraction\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(output_dim))\n",
    "    \n",
    "    # Using L2 regularization\n",
    "    model.add(layers.Dense(output_dim, kernel_regularizer=regularizers.l2(0.01)))\n",
    "    \n",
    "    # Optimizer with a custom learning rate\n",
    "    optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss=MeanSquaredError())\n",
    "    return model\n",
    "\n",
    "INPUT_DIM = 6144  # Number of features per sample\n",
    "OUTPUT_DIM = 1    # Number of continuous values as output\n",
    "\n",
    "cnn = KerasRegressor(build_fn=lambda: build_model(INPUT_DIM, OUTPUT_DIM), loss=MeanSquaredError() ,epochs=100, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing pipelines for each oxide\n",
    "sio2_base_estimators = [\n",
    "    ('svr', svr),\n",
    "    ('etr', etr)\n",
    "]\n",
    "\n",
    "tio2_base_estimators = [\n",
    "    #('gbr', gbr),\n",
    "    ('pls', pls),\n",
    "    ('xgb', xgb)\n",
    "]\n",
    "\n",
    "al203_base_estimators = [\n",
    "    #('gbr', gbr),\n",
    "    ('svr', svr),\n",
    "    ('xgb', xgb),\n",
    "    ('pls', pls),\n",
    "]\n",
    "\n",
    "feot_base_estimators = [\n",
    "    ('gbr', gbr),\n",
    "    ('svr', svr)\n",
    "    #('xgb', xgb)\n",
    "]\n",
    "\n",
    "mgo_base_estimators = [\n",
    "    ('gbr', gbr),\n",
    "    ('pls', pls),\n",
    "    ('eln', eln)\n",
    "]\n",
    "\n",
    "cao_base_estimators = [\n",
    "    ('svr', svr),\n",
    "    ('pls', pls),\n",
    "    ('xgb', xgb),\n",
    "]\n",
    "\n",
    "nao_base_estimators = [\n",
    "    ('svr', svr),\n",
    "    ('gbr', gbr)\n",
    "]\n",
    "\n",
    "estimators = {\n",
    "    \"SiO2\": sio2_base_estimators,\n",
    "    \"TiO2\": tio2_base_estimators,\n",
    "    \"Al2O3\": al203_base_estimators,\n",
    "    \"FeOT\" : feot_base_estimators,\n",
    "    \"MgO\" : mgo_base_estimators,\n",
    "    \"CaO\" : cao_base_estimators,\n",
    "    \"Na2O\" : nao_base_estimators\n",
    "}\n",
    "\n",
    "meta_kernel=\"poly\"\n",
    "meta_C=100\n",
    "meta_eps=0.1\n",
    "meta_gamma=\"scale\"\n",
    "meta_degree=2\n",
    "meta_coef0=0.1\n",
    "\n",
    "meta_learner = SVR(kernel=meta_kernel, C=meta_C, epsilon=meta_eps, gamma=meta_gamma, degree=meta_degree, coef0=meta_coef0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target: 'SiO2, 'TiO2', 'Al2O3', 'FeOT', 'MgO', 'CaO', 'Na2O', 'K2O'\n",
    "selected_targets = ['Na2O'] \n",
    "\n",
    "for target in selected_targets:\n",
    "    print(target)\n",
    "    if target in y_train.columns:\n",
    "        current_base_estimators = estimators[target]\n",
    "        stacking_regresor = StackingRegressor(estimators=current_base_estimators, final_estimator=meta_learner, cv=5)\n",
    "        stacking_regresor.fit(X_train, y_train[target])\n",
    "        y_pred = stacking_regresor.predict(X_test) \n",
    "        rmse = mean_squared_error(y_test[target], y_pred, squared=False)\n",
    "        print(f\"RMSE for {target}: {rmse}\")\n",
    "    else:\n",
    "        print(f\"Target {target} not found in dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

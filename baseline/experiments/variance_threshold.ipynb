{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lib import full_flow_dataloader\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from lib.config import AppConfig\n",
    "from lib.data_handling import load_split_data, CustomSpectralPipeline, WavelengthMaskTransformer, NonNegativeTransformer, SpectralDataReshaper\n",
    "from lib.reproduction import major_oxides, masks\n",
    "\n",
    "from lib.norms import Norm1Scaler, Norm3Scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AppConfig()\n",
    "\n",
    "composition_data_loc = config.composition_data_path\n",
    "dataset_loc = config.data_path\n",
    "\n",
    "# if not train_data or not test_data: # type: ignore\n",
    "train_data, test_data = load_split_data(\n",
    "\tstr(dataset_loc), average_shots=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = CustomSpectralPipeline(\n",
    "\tmasks=masks,\n",
    "\tcomposition_data_loc=composition_data_loc,\n",
    "\tmajor_oxides=major_oxides,\n",
    ")\n",
    "\n",
    "# pipeline.pipeline.steps.insert(-2, ('variance_threshold', VarianceThresholdTransformer(threshold=0)))\n",
    "# pipeline.pipeline.steps.append(('variance_threshold', VarianceThresholdTransformer(threshold=0.9)))\n",
    "\n",
    "train_processed = pipeline.fit_transform(train_data)\n",
    "test_processed = pipeline.fit_transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Norm3Scaler()\n",
    "train_processed = scaler.fit_transform(train_processed)\n",
    "test_processed = scaler.transform(test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed.iloc[:, 200:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_non_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\tcols_to_drop = major_oxides + [\"Sample Name\", \"ID\"]\n",
    "\treturn df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quantiles(min_quantile, max_quantile, num_points):\n",
    "    return np.linspace(min_quantile, max_quantile, num_points)\n",
    "    # # Generate linearly spaced values in the exponent space between log(min_quantile) and log(max_quantile)\n",
    "    # exponent_space = np.linspace(np.log(min_quantile), np.log(max_quantile), num_points)\n",
    "\n",
    "    # # Calculate the actual quantiles\n",
    "    # quantiles = np.exp(exponent_space)\n",
    "\n",
    "    # return quantiles\n",
    "\n",
    "min_q = 1e-9  # starting quantile\n",
    "max_q = 0.15   # ending quantile\n",
    "points = 100   # number of points\n",
    "\n",
    "quantile_values = generate_quantiles(min_q, max_q, points)\n",
    "print(quantile_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_non_numeric = drop_non_numeric_columns(train_processed)\n",
    "test_non_numeric = drop_non_numeric_columns(test_processed)\n",
    "\n",
    "zero_sum_columns = train_non_numeric.columns[train_non_numeric.sum() == 0]\n",
    "test_no_zero_columns = test_non_numeric.drop(columns=zero_sum_columns)\n",
    "\n",
    "flattened_series = test_no_zero_columns.values.ravel()\n",
    "quantiles = pd.Series(flattened_series).quantile(quantile_values)\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_dropped_colums_per_iteration = {} # threshold: num_dropped_columns\n",
    "\n",
    "for i, threshold in enumerate(quantiles.values):\n",
    "\t# Identify columns where the sum is zero (these columns contain only zero)\n",
    "\tzero_sum_columns = train_non_numeric.columns[(train_non_numeric.sum(axis=0) == 0)]\n",
    "\n",
    "\t# Drop zero sum columns from train and test, keeping the original for later\n",
    "\ttrain_reduced = train_non_numeric.drop(columns=zero_sum_columns)\n",
    "\ttest_reduced = test_non_numeric.drop(columns=zero_sum_columns)\n",
    "\n",
    "\tselector = VarianceThreshold(threshold=threshold)\n",
    "\ttrain_transformed = selector.fit_transform(train_reduced)\n",
    "\ttest_transformed = selector.transform(test_reduced)\n",
    "\n",
    "\t# Prepare DataFrame from the numpy arrays returned by VarianceThreshold\n",
    "\ttrain_transformed_df = pd.DataFrame(train_transformed, columns=train_reduced.columns[selector.get_support()])\n",
    "\ttest_transformed_df = pd.DataFrame(test_transformed, columns=test_reduced.columns[selector.get_support()])\n",
    "\n",
    "\t# Add back the zero-sum columns with all zeros\n",
    "\tfor column in zero_sum_columns:\n",
    "\t\ttrain_transformed_df[column] = 0\n",
    "\t\ttest_transformed_df[column] = 0\n",
    "\n",
    "\t# Ensure the column order is the same as the original DataFrames\n",
    "\t# train_final = train_transformed_df[train.columns]\n",
    "\t# test_final = test_transformed_df[test.columns]\n",
    "\n",
    "\t# Identify columns dropped due to zero variance (after removing zero sum columns)\n",
    "\tdropped_variance_columns = train_reduced.columns[~selector.get_support()]\n",
    "\n",
    "\t# Printing the shape of the data before and after transformation\n",
    "\t# print(f\"Threshold: {threshold}\")\n",
    "\t# Print the variance\n",
    "\t# print(f\"Variance of each column: {selector.variances_}\")\n",
    "\n",
    "\t# Print the min of the variance\n",
    "\t# print(f\"Min Variance: {selector.variances_.min()}\")\n",
    "\n",
    "\t# print(f\"Original Training data shape: {train_non_numeric.shape}\")\n",
    "\t# print(f\"Original Test data shape: {test_non_numeric.shape}\")\n",
    "\t# print(f\"Transformed Training data shape: {train_transformed_df.shape}\")\n",
    "\t# print(f\"Transformed Test data shape: {test_transformed_df.shape}\")\n",
    "\t# print(f\"Sum: {train_transformed_df.sum().sum()}\")\n",
    "\n",
    "\t# Print the number of columns dropped due to zero variance\n",
    "\t# print(f\"Number of columns dropped due to zero variance: {len(dropped_variance_columns)}\")\n",
    "\n",
    "\t# num_dropped_colums_per_iteration.append(len(dropped_variance_columns))\n",
    "\tnum_dropped_colums_per_iteration[threshold] = len(dropped_variance_columns)\n",
    "\n",
    "\t# plt.plot(selector.variances_)\n",
    "\t# plt.xlabel(\"Column index\")\n",
    "\t# plt.ylabel(\"Variance\")\n",
    "\t# plt.title(\"Variance of columns\")\n",
    "\t# plt.axhline(y=threshold, color='r', linestyle='--')\n",
    "\n",
    "\t# plt.show()\n",
    "\n",
    "# For each unique number of dropped columns, store the threshold that caused it in a list\n",
    "thresholds_per_num_dropped_columns = {}\n",
    "\n",
    "for threshold, num_dropped_columns in num_dropped_colums_per_iteration.items():\n",
    "\tif num_dropped_columns not in thresholds_per_num_dropped_columns:\n",
    "\t\tthresholds_per_num_dropped_columns[num_dropped_columns] = [threshold]\n",
    "\telse:\n",
    "\t\tthresholds_per_num_dropped_columns[num_dropped_columns].append(threshold)\n",
    "\n",
    "print(len(num_dropped_colums_per_iteration))\n",
    "\n",
    "# plot the number of dropped columns per iteration\n",
    "# plt.plot(quantiles, num_dropped_colums_per_iteration)\n",
    "# plt.xlabel(\"Threshold\")\n",
    "# plt.ylabel(\"Number of dropped columns\")\n",
    "# plt.title(\"Number of dropped columns per threshold\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f\"num_dropped_cols.json\", \"w\") as f:\n",
    "\tjson.dump(thresholds_per_num_dropped_columns, f, indent=4)\n",
    "\n",
    "unique = []\n",
    "\n",
    "for key in thresholds_per_num_dropped_columns:\n",
    "\tunique.append(thresholds_per_num_dropped_columns[key][0])\n",
    "\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformed_df.iloc[:, 200:300]\n",
    "print(train_non_numeric.sum(axis=0).sum())\n",
    "print(train_transformed_df.sum(axis=0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(selector.variances_)\n",
    "plt.xlabel(\"Column index\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.title(\"Variance of columns\")\n",
    "plt.axhline(y=threshold, color='r', linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.full_flow_dataloader import load_and_scale_data\n",
    "norm = 3\n",
    "\n",
    "train, test = load_and_scale_data(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroSumVarianceThreshold(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.0):\n",
    "        self.threshold = threshold\n",
    "        self.zero_sum_columns = None\n",
    "        self.variance_selector = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Identify columns where the sum is zero (these columns contain only zero)\n",
    "        self.zero_sum_columns = X.columns[X.sum(axis=0) == 0]\n",
    "\n",
    "        # Drop zero sum columns from X\n",
    "        reduced_X = X.drop(columns=self.zero_sum_columns)\n",
    "\n",
    "        # Apply VarianceThreshold to the remaining data\n",
    "        self.variance_selector = VarianceThreshold(self.threshold)\n",
    "        self.variance_selector.fit(reduced_X)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Apply VarianceThreshold to the reduced data\n",
    "        reduced_X = X.drop(columns=self.zero_sum_columns)\n",
    "        transformed_data = self.variance_selector.transform(reduced_X)\n",
    "\n",
    "        # Convert array back to DataFrame, adding zero-sum columns back with zeros\n",
    "        transformed_data = pd.DataFrame(transformed_data, index=X.index, columns=reduced_X.columns[self.variance_selector.get_support()])\n",
    "        for column in self.zero_sum_columns:\n",
    "            transformed_data[column] = 0\n",
    "\n",
    "        return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Custom scorer for RMSE\n",
    "def rmse_scorer(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse = make_scorer(rmse_scorer, greater_is_better=False)\n",
    "\n",
    "svr_params = {\n",
    "    'kernel': 'poly',\n",
    "    'degree': 2,\n",
    "    'C': 100,\n",
    "    'epsilon': 0.1,\n",
    "    'coef0': 1.0,\n",
    "    'gamma': 'scale'\n",
    "}\n",
    "\n",
    "# Define the pipeline\n",
    "pipe = Pipeline([\n",
    "    # ('var_thresh', VarianceThreshold()),\n",
    "    ('var_thresh', ZeroSumVarianceThreshold()),\n",
    "    ('svm', SVR(**svr_params)),\n",
    "])\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    # 'var_thresh__threshold': quantiles.values  # Using the quantiles you've already calculated\n",
    "    'var_thresh__threshold': unique  # Using the quantiles you've already calculated\n",
    "}\n",
    "\n",
    "drop_cols = major_oxides + [\"ID\", \"Sample Name\"]\n",
    "\n",
    "X_train = train.drop(columns=drop_cols)\n",
    "y_train = train[major_oxides]\n",
    "\n",
    "X_test = test.drop(columns=drop_cols)\n",
    "y_test = test[major_oxides]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for target in major_oxides:\n",
    "    # Set up GridSearchCV\n",
    "    print(f\"Optimizing for {target}\")\n",
    "    results[target] = {}\n",
    "\n",
    "    # Fit the grid search object\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring=make_scorer(rmse_scorer, greater_is_better=False), verbose=10, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train[target])\n",
    "\n",
    "    # Train a model without the variance thresholding for comparison\n",
    "    svm = SVR(**svr_params)\n",
    "    svm.fit(X_train, y_train[target])\n",
    "    y_pred = svm.predict(X_test)\n",
    "    rmse_no_variance = np.sqrt(mean_squared_error(y_test[target], y_pred))\n",
    "\n",
    "    print(\"RMSE without variance thresholding:\", rmse_no_variance)\n",
    "    results[target]['rmse_no_variance'] = rmse_no_variance\n",
    "\n",
    "    # Print the best parameters\n",
    "    print(f\"Best parameters: {grid_search.best_params_['var_thresh__threshold']:.20f}\")\n",
    "    print(\"Best RMSE:\", -grid_search.best_score_)\n",
    "\n",
    "    # Save the grid search object with oxide\n",
    "    results[target]['grid_search'] = {\n",
    "        \"variance_threshold\": grid_search.best_params_[\"var_thresh__threshold\"],\n",
    "        \"best_rmse\": -grid_search.best_score_\n",
    "    }\n",
    "\n",
    "    # For each run save the results as to a file\n",
    "    with open(f\"var_threshold_results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for oxide in major_oxides:\n",
    "\tprint(f\"{oxide}: {results[oxide].best_params_['var_thresh__threshold']:.20f}. RMSE: {-results[oxide].best_score_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "experiment_ids = [\n",
    "\t256, # ETR\n",
    "\t231, # Random Forest\n",
    "\t215, # Elastic Net,\n",
    "\t214, # Ridge\n",
    "\t210, # LASSO\n",
    "\t257, # ANN\n",
    "\t258, # CNN\n",
    "\t144, # NGB\n",
    "\t140, # PLS\n",
    "\t136, # XGBoost\n",
    "\t134, # SVR\n",
    "\t45, # GBR\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "\t\"Ridge\": \"Ridge\",\n",
    "\t\"LASSO\": \"\\\\gls{lasso}\",\n",
    "\t\"ElasticNet\": \"\\\\gls{enet}\",\n",
    "\t\"PLS\": \"\\\\gls{pls}\",\n",
    "\t\"SVR\": \"\\\\gls{svr}\",\n",
    "\t\"RandomForest\": \"\\\\gls{rf}\",\n",
    "\t\"NGB\": \"\\\\gls{ngboost}\",\n",
    "\t\"GBR\": \"\\\\gls{gbr}\",\n",
    "\t\"XGB\": \"\\\\gls{xgboost}\",\n",
    "\t\"ExtraTrees\": \"\\\\gls{etr}\",\n",
    "\t\"ANN\": \"\\\\gls{ann}\",\n",
    "\t\"CNN\": \"\\\\gls{cnn}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from lib.reproduction import major_oxides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "data = {}\n",
    "\n",
    "for experiment_id in experiment_ids:\n",
    "\tdata[experiment_id] = client.search_runs(experiment_id)\n",
    "\n",
    "data[experiment_ids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "model_means = {}\n",
    "\n",
    "# Iterate over the data dictionary and print the runs\n",
    "for experiment_id, runs in data.items():\n",
    "    for run in runs:\n",
    "        # Check if the target parameter is present\n",
    "        if \"target\" not in run.data.params:\n",
    "            continue\n",
    "\n",
    "        model_name = run.data.tags[\"mlflow.runName\"].split(\"_\")[0]\n",
    "        latex_name = model_name\n",
    "\n",
    "        if model_name in models:\n",
    "            latex_name = models[latex_name]\n",
    "\n",
    "        target = run.data.params[\"target\"]\n",
    "        rmse = run.data.metrics[\"rmse\"]\n",
    "        rmse_cv = run.data.metrics[\"rmse_cv\"]\n",
    "        std_dev = run.data.metrics[\"std_dev\"]\n",
    "        std_dev_cv = run.data.metrics[\"std_dev_cv\"]\n",
    "\n",
    "        # print(f\"{model_name} - {target}, RMSE: {rmse}, RMSE CV: {rmse_cv}, STD DEV: {std_dev}, STD DEV CV: {std_dev_cv}\")\n",
    "\n",
    "        if model_name not in results:\n",
    "            results[model_name] = {}\n",
    "            model_means[model_name] = {\"rmse\": [], \"rmse_cv\": [], \"std_dev\": [], \"std_dev_cv\": []}\n",
    "\n",
    "        results[model_name][target] = {\n",
    "            \"latex_name\": latex_name,\n",
    "            \"rmse\": rmse,\n",
    "            \"rmse_cv\": rmse_cv,\n",
    "            \"std_dev\": std_dev,\n",
    "            \"std_dev_cv\": std_dev_cv,\n",
    "        }\n",
    "\n",
    "# Calculate the mean of the results\n",
    "for model_name, targets in results.items():\n",
    "    for target, metrics in targets.items():\n",
    "        model_means[model_name][\"rmse\"].append(metrics[\"rmse\"])\n",
    "        model_means[model_name][\"rmse_cv\"].append(metrics[\"rmse_cv\"])\n",
    "        model_means[model_name][\"std_dev\"].append(metrics[\"std_dev\"])\n",
    "        model_means[model_name][\"std_dev_cv\"].append(metrics[\"std_dev_cv\"])\n",
    "\n",
    "    for metric, values in model_means[model_name].items():\n",
    "        model_means[model_name][metric] = np.mean(values)\n",
    "\n",
    "model_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store the rows\n",
    "rows = []\n",
    "\n",
    "# Iterate over the results dictionary to populate the rows\n",
    "for model, oxides in results.items():\n",
    "    for oxide, metrics in oxides.items():\n",
    "        for metric, value in metrics.items():\n",
    "            if metric != 'latex_name':  # Exclude 'latex_name' from the metrics\n",
    "                rows.append({\n",
    "                    'model': model,\n",
    "                    'oxide': oxide,\n",
    "                    'metric': metric,\n",
    "                    'value': value\n",
    "                })\n",
    "\n",
    "# Convert the list of rows to a DataFrame\n",
    "results_df = pd.DataFrame(rows)\n",
    "\n",
    "# Pivot the DataFrame to get the desired structure\n",
    "results_df = results_df.pivot_table(index=['model', 'metric'], columns='oxide', values='value').reset_index()\n",
    "\n",
    "# Reorder the columns for better readability\n",
    "results_df = results_df[['model', 'metric'] + [col for col in results_df.columns if col not in ['model', 'metric']]]\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_cv_results = results_df[results_df[\"metric\"] == \"rmse_cv\"]\n",
    "rmse_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Al2O3\"\n",
    "rmse_cv_results_sorted = rmse_cv_results.sort_values(by=target).reset_index(drop=True).rename_axis('index')\n",
    "rmse_cv_results_sorted[['model', 'metric', target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_means_df = pd.DataFrame(model_means).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_means_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rmse_cv = model_means_df[\"rmse_cv\"].min()\n",
    "relative_performance = (model_means_df[\"rmse_cv\"].sort_values() / best_rmse_cv) * 100\n",
    "\n",
    "# Calculate the jump vs. the next\n",
    "relative_performance_sorted = relative_performance.sort_values()\n",
    "jump_vs_next = relative_performance_sorted.diff().fillna(0)\n",
    "\n",
    "# Combine the relative performance and jump vs. next into a DataFrame\n",
    "performance_comparison = pd.DataFrame({\n",
    "    'Relative Performance (%)': relative_performance_sorted,\n",
    "    'Jump vs. Next (%)': jump_vs_next\n",
    "})\n",
    "\n",
    "performance_comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "n_decimals = 3\n",
    "\n",
    "latex_table = \"\\\\begin{table*}[]\\n\"\n",
    "latex_table += \"\\\\centering\\n\"\n",
    "latex_table += \"\\\\resizebox{1\\\\textwidth}{!}{%\\n\"\n",
    "latex_table += \"\\\\begin{tabular}{l|cccc|cccc|cccc}\\n\"\n",
    "\n",
    "model_keys = list(models.keys())\n",
    "\n",
    "for i in range(0, len(model_keys), 3):\n",
    "    chunk = model_keys[i:i+3]\n",
    "\n",
    "    # Header row\n",
    "    header_row = \"Model\"\n",
    "    for model in chunk:\n",
    "        header_row += f\" & \\\\multicolumn{{4}}{{c}}{{{models[model]}}}\"\n",
    "\n",
    "    latex_table += header_row + \" \\\\\\\\\\n\"\n",
    "\n",
    "    # Metric row\n",
    "    metric_row = \"Metric\"\n",
    "    for _ in chunk:\n",
    "        metric_row += \" & \\\\multicolumn{1}{c}{RMSEP} & \\\\multicolumn{1}{c}{RMSECV} & \\\\multicolumn{1}{c}{Std. dev.} & \\\\multicolumn{1}{c}{Std. dev. CV}\"\n",
    "\n",
    "    latex_table += metric_row + \" \\\\\\\\\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "\n",
    "    # Data rows\n",
    "    for target in major_oxides:\n",
    "        row = f\"$\\\\ce{{{target}}}$\"\n",
    "\n",
    "        for model in chunk:\n",
    "            if model in results and target in results[model]:\n",
    "                metrics = results[model][target]\n",
    "                row += f\" & {metrics['rmse']:.{n_decimals}f} & {metrics['rmse_cv']:.{n_decimals}f} & {metrics['std_dev']:.{n_decimals}f} & {metrics['std_dev_cv']:.{n_decimals}f}\"\n",
    "            else:\n",
    "                print(f\"Missing data for {model} - {target}\")\n",
    "                row += \" & - & - & - & -\"\n",
    "\n",
    "        latex_table += row + \" \\\\\\\\\\n\"\n",
    "\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "\n",
    "    # Mean rows\n",
    "    mean_row = \"Mean\"\n",
    "    for model in chunk:\n",
    "        if model in model_means:\n",
    "            mean_metrics = model_means[model]\n",
    "            mean_row += f\" & {mean_metrics['rmse']:.{n_decimals}f} & {mean_metrics['rmse_cv']:.{n_decimals}f} & {mean_metrics['std_dev']:.{n_decimals}f} & {mean_metrics['std_dev_cv']:.{n_decimals}f}\"\n",
    "        else:\n",
    "            mean_row += \" & - & - & - & -\"\n",
    "\n",
    "    latex_table += mean_row + \" \\\\\\\\\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "\n",
    "latex_table += \"\\\\end{tabular}%\\n\"\n",
    "latex_table += \"}\\n\"\n",
    "latex_table += \"\\\\caption{Initial results for the different models and metrics.}\\n\"\n",
    "latex_table += \"\\\\label{tab:init_results}\\n\"\n",
    "latex_table += \"\\\\end{table*}\\n\"\n",
    "\n",
    "# Write the LaTeX table string to a file\n",
    "# path = Path(\"./../report_thesis/src/sections/results/init_results_table.tex\")\n",
    "\n",
    "# with open(path, \"w\") as file:\n",
    "#     file.write(latex_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

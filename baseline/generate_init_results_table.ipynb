{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "experiment_ids = [\n",
    "\t256, # ETR\n",
    "\t231, # Random Forest\n",
    "\t215, # Elastic Net,\n",
    "\t214, # Ridge\n",
    "\t210, # LASSO\n",
    "\t257, # ANN\n",
    "\t258, # CNN\n",
    "\t144, # NGB\n",
    "\t140, # PLS\n",
    "\t136, # XGBoost\n",
    "\t134, # SVR\n",
    "\t45, # GBR\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "\t\"Ridge\": \"Ridge\",\n",
    "\t\"LASSO\": \"\\\\gls{lasso}\",\n",
    "\t\"ElasticNet\": \"\\\\gls{enet}\",\n",
    "\t\"PLS\": \"\\\\gls{pls}\",\n",
    "\t\"SVR\": \"\\\\gls{svr}\",\n",
    "\t\"RandomForest\": \"\\\\gls{rf}\",\n",
    "\t\"NGB\": \"\\\\gls{ngboost}\",\n",
    "\t\"GBR\": \"\\\\gls{gbr}\",\n",
    "\t\"XGB\": \"\\\\gls{xgboost}\",\n",
    "\t\"ExtraTrees\": \"\\\\gls{etr}\",\n",
    "\t\"ANN\": \"\\\\gls{ann}\",\n",
    "\t\"CNN\": \"\\\\gls{cnn}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from lib.reproduction import major_oxides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "data = {}\n",
    "\n",
    "for experiment_id in experiment_ids:\n",
    "\tdata[experiment_id] = client.search_runs(experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "model_means = {}\n",
    "\n",
    "# Iterate over the data dictionary and print the runs\n",
    "for experiment_id, runs in data.items():\n",
    "    for run in runs:\n",
    "        # Check if the target parameter is present\n",
    "        if \"target\" not in run.data.params:\n",
    "            continue\n",
    "\n",
    "        model_name = run.data.tags[\"mlflow.runName\"].split(\"_\")[0]\n",
    "        latex_name = model_name\n",
    "\n",
    "        if model_name in models:\n",
    "            latex_name = models[latex_name]\n",
    "\n",
    "        target = run.data.params[\"target\"]\n",
    "        rmse = run.data.metrics[\"rmse\"]\n",
    "        rmse_cv = run.data.metrics[\"rmse_cv\"]\n",
    "        std_dev = run.data.metrics[\"std_dev\"]\n",
    "        std_dev_cv = run.data.metrics[\"std_dev_cv\"]\n",
    "\n",
    "        # print(f\"{model_name} - {target}, RMSE: {rmse}, RMSE CV: {rmse_cv}, STD DEV: {std_dev}, STD DEV CV: {std_dev_cv}\")\n",
    "\n",
    "        if model_name not in results:\n",
    "            results[model_name] = {}\n",
    "            model_means[model_name] = {\"rmse\": [], \"rmse_cv\": [], \"std_dev\": [], \"std_dev_cv\": []}\n",
    "\n",
    "        if target not in results[model_name] or rmse_cv < results[model_name][target][\"rmse_cv\"]:\n",
    "            results[model_name][target] = {\n",
    "                \"latex_name\": latex_name,\n",
    "                \"rmse\": rmse,\n",
    "                \"rmse_cv\": rmse_cv,\n",
    "                \"std_dev\": std_dev,\n",
    "                \"std_dev_cv\": std_dev_cv,\n",
    "            }\n",
    "\n",
    "# Calculate the mean of the results\n",
    "for model_name, targets in results.items():\n",
    "    for target, data in targets.items():\n",
    "        model_means[model_name][\"rmse\"].append(data[\"rmse\"])\n",
    "        model_means[model_name][\"rmse_cv\"].append(data[\"rmse_cv\"])\n",
    "        model_means[model_name][\"std_dev\"].append(data[\"std_dev\"])\n",
    "        model_means[model_name][\"std_dev_cv\"].append(data[\"std_dev_cv\"])\n",
    "\n",
    "    for metric, values in model_means[model_name].items():\n",
    "        model_means[model_name][metric] = np.mean(values)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Generate the full table showing all the metrics\n",
    "n_decimals = 3\n",
    "\n",
    "latex_table = \"\\\\begin{table*}[]\\n\"\n",
    "latex_table += \"\\\\centering\\n\"\n",
    "latex_table += \"\\\\resizebox{1\\\\textwidth}{!}{%\\n\"\n",
    "latex_table += \"\\\\begin{tabular}{l|cccc|cccc|cccc}\\n\"\n",
    "\n",
    "model_keys = list(models.keys())\n",
    "\n",
    "for i in range(0, len(model_keys), 3):\n",
    "    chunk = model_keys[i:i+3]\n",
    "\n",
    "    # Header row\n",
    "    header_row = \"Model\"\n",
    "    for model in chunk:\n",
    "        header_row += f\" & \\\\multicolumn{{4}}{{c}}{{{models[model]}}}\"\n",
    "\n",
    "    latex_table += header_row + \" \\\\\\\\\\n\"\n",
    "\n",
    "    # Metric row\n",
    "    metric_row = \"Metric\"\n",
    "    for _ in chunk:\n",
    "        metric_row += \" & \\\\multicolumn{1}{c}{RMSEP} & \\\\multicolumn{1}{c}{RMSECV} & \\\\multicolumn{1}{c}{Std. dev.} & \\\\multicolumn{1}{c}{Std. dev. CV}\"\n",
    "\n",
    "    latex_table += metric_row + \" \\\\\\\\\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "\n",
    "    # Data rows\n",
    "    for target in major_oxides:\n",
    "        row = f\"$\\\\ce{{{target}}}$\"\n",
    "\n",
    "        for model in chunk:\n",
    "            if model in results and target in results[model]:\n",
    "                data = results[model][target]\n",
    "                row += f\" & {data['rmse']:.{n_decimals}f} & {data['rmse_cv']:.{n_decimals}f} & {data['std_dev']:.{n_decimals}f} & {data['std_dev_cv']:.{n_decimals}f}\"\n",
    "            else:\n",
    "                print(f\"Missing data for {model} - {target}\")\n",
    "                row += \" & - & - & - & -\"\n",
    "\n",
    "        latex_table += row + \" \\\\\\\\\\n\"\n",
    "\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "\n",
    "    # Mean rows\n",
    "    mean_row = \"Mean\"\n",
    "    for model in chunk:\n",
    "        if model in model_means:\n",
    "            mean_metrics = model_means[model]\n",
    "            mean_row += f\" & {mean_metrics['rmse']:.{n_decimals}f} & {mean_metrics['rmse_cv']:.{n_decimals}f} & {mean_metrics['std_dev']:.{n_decimals}f} & {mean_metrics['std_dev_cv']:.{n_decimals}f}\"\n",
    "        else:\n",
    "            mean_row += \" & - & - & - & -\"\n",
    "\n",
    "    latex_table += mean_row + \" \\\\\\\\\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "\n",
    "latex_table += \"\\\\end{tabular}%\\n\"\n",
    "latex_table += \"}\\n\"\n",
    "latex_table += \"\\\\caption{Initial results for the different models and metrics.}\\n\"\n",
    "latex_table += \"\\\\label{tab:init_results}\\n\"\n",
    "latex_table += \"\\\\end{table*}\\n\"\n",
    "\n",
    "# Write the LaTeX table string to a file\n",
    "path = Path(\"./../report_thesis/src/sections/results/init_results_table.tex\")\n",
    "\n",
    "with open(path, \"w\") as file:\n",
    "    file.write(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Identify the best model for each target including the metric value\n",
    "best_models = {}\n",
    "\n",
    "for target in major_oxides:\n",
    "    for metric in [\"rmse\", \"rmse_cv\", \"std_dev\", \"std_dev_cv\"]:\n",
    "        best_value = None\n",
    "        best_model = None\n",
    "\n",
    "        for model, data in results.items():\n",
    "            if target in data:\n",
    "                if best_value is None or data[target][metric] < best_value:\n",
    "                    best_value = data[target][metric]\n",
    "                    best_model = model\n",
    "\n",
    "        if target not in best_models:\n",
    "            best_models[target] = {}\n",
    "\n",
    "        best_models[target][metric] = (best_model, best_value)\n",
    "\n",
    "best_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Generate a LaTeX table showing the best metric for each oxide (RMSEP, RMSECV, Std. dev., Std. dev. CV)\n",
    "latex_table = \"\\\\begin{tabular}{l|llll}\\n\"\n",
    "latex_table += \"Oxide & RMSEP & RMSECV & Std. dev. & Std. dev. CV \\\\\\\\\\n\"\n",
    "latex_table += \"\\\\hline\\n\"\n",
    "\n",
    "# Write the best value followed by the model in parentheses, for example \"0.123 (Ridge)\"\n",
    "\n",
    "for target in major_oxides:\n",
    "\trow = f\"$\\\\ce{{{target}}}$\"\n",
    "\n",
    "\tfor metric in [\"rmse\", \"rmse_cv\", \"std_dev\", \"std_dev_cv\"]:\n",
    "\t\tbest_model, best_value = best_models[target][metric]\n",
    "\t\trow += f\" & {best_value:.{n_decimals}f} ({models[best_model]})\"\n",
    "\n",
    "\tlatex_table += row + \" \\\\\\\\\\n\"\n",
    "\n",
    "latex_table += \"\\\\end{tabular}%\\n\"\n",
    "latex_table += \"\\\\label{tab:best_results}\\n\"\n",
    "\n",
    "# Write the LaTeX table string to a file\n",
    "path = Path(\"./../report_thesis/src/sections/results/best_results_table.tex\")\n",
    "\n",
    "with open(path, \"w\") as file:\n",
    "\tfile.write(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create a dataframe showing how many times each of the best models was the best\n",
    "best_model_counts = {}\n",
    "\n",
    "for model in best_models.values():\n",
    "\tfor best_model, _ in model.values():\n",
    "\t\tif best_model not in best_model_counts:\n",
    "\t\t\tbest_model_counts[best_model] = 0\n",
    "\n",
    "\t\tbest_model_counts[best_model] += 1\n",
    "\n",
    "# Turn it into a dataframe\n",
    "df = pd.DataFrame.from_dict(best_model_counts, orient=\"index\", columns=[\"Occurrences\"])\n",
    "df = df.sort_values(by=\"Occurrences\", ascending=False)\n",
    "\n",
    "# Turn the dataframe into a LaTeX table\n",
    "latex_table = \"\\\\begin{tabular}{lc}\\n\"\n",
    "latex_table += \"Model & Occurrences \\\\\\\\\\n\"\n",
    "latex_table += \"\\\\hline\\n\"\n",
    "\n",
    "for model, count in df.iterrows():\n",
    "\tlatex_table += f\"{models[model]} & {count['Occurrences']} \\\\\\\\\\n\"\n",
    "\n",
    "latex_table += \"\\\\end{tabular}\\n\"\n",
    "latex_table += \"\\\\label{tab:best_model_occurrences}\\n\"\n",
    "\n",
    "# Write the LaTeX table string to a file\n",
    "path = Path(\"./../report_thesis/src/sections/results/best_model_occurrences_table.tex\")\n",
    "\n",
    "with open(path, \"w\") as file:\n",
    "\tfile.write(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_names = list(model_means.keys())\n",
    "rmse_values = [metrics[\"rmse\"] for metrics in model_means.values()]\n",
    "rmse_cv_values = [metrics[\"rmse_cv\"] for metrics in model_means.values()]\n",
    "std_dev_values = [metrics[\"std_dev\"] for metrics in model_means.values()]\n",
    "std_dev_cv_values = [metrics[\"std_dev_cv\"] for metrics in model_means.values()]\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'RMSE': rmse_values,\n",
    "    'RMSE_CV': rmse_cv_values,\n",
    "    'Std_Dev': std_dev_values,\n",
    "    'Std_Dev_CV': std_dev_cv_values\n",
    "})\n",
    "\n",
    "metrics_sorted = {\n",
    "    'RMSE': data.sort_values(by='RMSE'),\n",
    "    'RMSE_CV': data.sort_values(by='RMSE_CV'),\n",
    "    'Std_Dev': data.sort_values(by='Std_Dev'),\n",
    "    'Std_Dev_CV': data.sort_values(by='Std_Dev_CV')\n",
    "}\n",
    "\n",
    "color = sns.color_palette(\"Blues\")[2]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "titles = [\"Mean RMSEP\", \"Mean RMSECV\", \"Mean Standard Deviation of Prediction Errors\", \"Mean of Standard Deviation of Cross-Validation Prediction Errors\"]\n",
    "y_labels = [\"Mean RMSEP\", \"Mean RMSECV\", \"Mean Standard Deviation of Prediction Errors\", \"Mean of Standard Deviation of Cross-Validation Prediction Errors\"]\n",
    "columns = ['RMSE', 'RMSE_CV', 'Std_Dev', 'Std_Dev_CV']\n",
    "\n",
    "for i, (col, ax) in enumerate(zip(columns, axes.flatten())):\n",
    "    sns.barplot(x='Model', y=col, data=metrics_sorted[col], color=color, ax=ax)\n",
    "    ax.set_title(titles[i], fontsize=16)\n",
    "    ax.set_ylabel(y_labels[i], fontsize=14)\n",
    "    ax.set_xlabel(\"Model\", fontsize=14)\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=12)\n",
    "    ax.tick_params(axis='y', labelsize=12)\n",
    "    ax.grid(True, which='both', axis='y', linestyle='--', linewidth=0.7)\n",
    "    ax.grid(False, which='both', axis='x')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.75)\n",
    "plt.show()\n",
    "\n",
    "path = Path(\"./../report_thesis/src/images/init_results_means.png\")\n",
    "fig.savefig(path, dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data_list = []\n",
    "\n",
    "for model, oxides in results.items():\n",
    "    for oxide, metrics in oxides.items():\n",
    "        data_list.append({\n",
    "            'Model': model,\n",
    "            'Oxide': oxide,\n",
    "            'RMSE_CV': metrics['rmse_cv']\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df['Normalized_RMSE_CV'] = df.groupby('Oxide')['RMSE_CV'].transform(lambda x: scaler.fit_transform(x.values.reshape(-1, 1)).flatten())\n",
    "\n",
    "df['Average_Normalized_RMSE_CV'] = df.groupby('Model')['Normalized_RMSE_CV'].transform('mean')\n",
    "\n",
    "df_sorted = df.sort_values(by='Average_Normalized_RMSE_CV').drop(columns='Average_Normalized_RMSE_CV')\n",
    "\n",
    "heatmap_data = df_sorted.pivot(index=\"Model\", columns=\"Oxide\", values=\"Normalized_RMSE_CV\")\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "ax = sns.heatmap(heatmap_data,\n",
    "                 annot=df_sorted.pivot(index=\"Model\", columns=\"Oxide\", values=\"RMSE_CV\"),\n",
    "                 fmt=\".2f\", cmap=\"Blues\", linewidths=.5, cbar=False)\n",
    "\n",
    "plt.title('RMSECV Comparison Across Models and Oxides', fontsize=20, pad=20)\n",
    "plt.xlabel('Oxide', fontsize=16, labelpad=20)\n",
    "plt.ylabel('Model', fontsize=16, labelpad=20)\n",
    "plt.xticks(rotation=45, fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data_list = []\n",
    "\n",
    "for model, oxides in results.items():\n",
    "    for oxide, metrics in oxides.items():\n",
    "        data_list.append({\n",
    "            'Model': model,\n",
    "            'Oxide': oxide,\n",
    "            'RMSE_CV': metrics['rmse_cv']\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "average_rmse_cv = df.groupby('Model')['RMSE_CV'].mean().reset_index()\n",
    "average_rmse_cv.columns = ['Model', 'Average_RMSE_CV']\n",
    "\n",
    "best_performance = average_rmse_cv['Average_RMSE_CV'].min()\n",
    "average_rmse_cv['Relative_Performance (%)'] = (average_rmse_cv['Average_RMSE_CV'] / best_performance) * 100\n",
    "\n",
    "average_rmse_cv = average_rmse_cv.sort_values(by='Relative_Performance (%)')\n",
    "average_rmse_cv['Percent Difference vs Next (%)'] = average_rmse_cv['Relative_Performance (%)'].diff(-1).abs()\n",
    "average_rmse_cv['Percent Difference vs Next (%)'].iloc[-1] = np.nan  # or ''\n",
    "\n",
    "final_table = average_rmse_cv[['Model', 'Relative_Performance (%)', 'Percent Difference vs Next (%)']].reset_index(drop=True)\n",
    "latex_table = final_table.to_latex(index=False, escape=False)\n",
    "\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def prepare_data_for_plotting(results, metric):\n",
    "    data_list = []\n",
    "    for model, oxides in results.items():\n",
    "        for oxide, values in oxides.items():\n",
    "            data_list.append({'Model': model, 'Oxide': oxide, 'Value': values[metric]})\n",
    "\n",
    "    return pd.DataFrame(data_list)\n",
    "\n",
    "metrics = ['rmse', 'rmse_cv', 'std_dev', 'std_dev_cv']\n",
    "\n",
    "palette = sns.color_palette(\"tab10\")\n",
    "\n",
    "for metric in metrics:\n",
    "    df = prepare_data_for_plotting(results, metric)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.barplot(data=df, x='Oxide', y='Value', hue='Model', palette=palette)\n",
    "    plt.title(f'{metric.upper()} for each Oxide and Model')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

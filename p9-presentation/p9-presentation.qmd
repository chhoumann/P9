---
title: "Identifying Limitations in the ChemCam Multivariate Oxide Composition Model for Elemental Quantification in Martian Geological Samples"
# subtitle: ""
author: "Christian Bager Bach Houmann<br/>Patrick Frostholm Ã˜stergaard<br/>Ivik Lau Dalgas Hostrup<br/> @ AAU"
date: 2024-02-13
date-format: "D MMM YYYY"
slide-number: true
bibliography: static/references.bib
format:
    revealjs:
        theme: serif
        progress: true
        toc: true
        css: styles.css
        transition: slide
        transition-speed: fast
        auto-animate-duration: 0.5
---

# Introduction
{{< include sections/_introduction.qmd >}}

# MOC Pipeline Replica
{{< include sections/_methodology.qmd >}}

::: {.notes}
My name is Christian, and I'll be presenting our replica of the MOC pipeline.
This is what we used to establish baselines for our experiments.
:::

## MOC Pipeline {auto-animate="true"}
::: {style="width=100%;"}
![](/static/methodology/pipeline.png){fig-align="center" width="35%"}
:::
::: {.fragment data-id="moc-box"}
<div style="position: absolute; top: 34.45%; left: 30.5%; width: 18%; height: 35.25%; border: 2px solid red; box-sizing: border-box; pointer-events: none;"></div>
:::

::: {.fragment data-id="moc-box"}
<div style="position: absolute; top: 34.45%; left: 51.25%; width: 18%; height: 35.25%; border: 2px solid red; box-sizing: border-box; pointer-events: none;"></div>
:::

::: {.notes}
This is the entire MOC pipeline at inference-time.

We have attempted to replicate this pipeline as faithfully as we could, but have had to make some design choices that differ from the original --- mostly due to a lack of information regarding some aspects of the original design.

It's important to keep in mind that this pipeline is for multivariate regression.
That means we create predictions for each of 8 major oxides.
However, most of the steps actually involve data being processed for each oxide individually.
For both ICA and PLS, you can imagine that after some preprocessing steps, we branch out into 8 branches, each of which predict its own values for its associated oxide.
And in the end, we put the outputs together to produce the MOC prediction.

I start by walking through the PLS-SM side, and then the ICA side.
:::

## PLS-SM
- Recreating the PLS-SM method presented by @cleggRecalibrationMarsScience2017 and @andersonImprovedAccuracyQuantitative2017
    - Preprocessing
    - Outlier removal
    - Training
    - Inference via submodels

::: {.notes}
Starting with the PLS-SM method, we have attempted to recreate the system presented by both Clegg and Anderson et al.
:::

## PLS-SM Motivation {.unlisted auto-animate="true"}
- **PLS**
    - Good for chemometrics
    - Readily intrepretable

::: {.notes}
Let me start by explaining what motivated PLS-SM, because that will help us understand why it does what it does.

First: they chose PLS because it can handle noisy data with many more variables than observations, and with significant correlation between variables.
It's also readily intrepretable by plotting the regression coefficients as a function of the spectral channels. This shows which wavelength ranges show correlations with the prediction composition.
:::

## PLS-SM Motivation {.unlisted auto-animate="true"}
- PLS
- **PLS2 vs. PLS1**
    - PLS2: Multivariate
    - PLS1: Univariate
    - Better results with separate PLS1 models

::: {.notes}
For the original ChemCam calibration, the team used PLS2. This case corresponds to the case where there are several dependent variables, meaning it can perform multivariate predictions.

However, they found that they're seeing better results by using a separate PLS1 model for each of the major oxides.

PLS1 is the case where there's only a single dependent variable --- so it's for univariate regression.
:::

## PLS-SM Motivation {.unlisted auto-animate="true"}
- PLS
- PLS2 vs. PLS1
- **Solution: Submodels**
    - More accurate when focusing on specific concentration ranges
    - Single regression model: good general performance, worse on some individual samples
    - So combine several models trained on subsets of the full compositional range

::: {.notes}
**OK. So why submodels?**

Different parts of the LIBS spectrum react differently to varying amounts of elements in the sample.
So, a spectrum from a sample with a medium amount of an element might show patterns that aren't there when there's a very high or low amount of that element.
This can be because when there's too much of an element, it can overshadow its own signal, and the presence of other elements can also change the response.

Using sub-models lets them focus on specific concentration ranges for more accurate readings.

It's hard for a single regression model to account for these variations. It often makes a tradeoff where it has good general performance, but performance worse on some samples. And specialist models (trained on a restricted range) will do good for that range, but much worse outside the range than a model trained on the full set.

So the solution was to combine several regression models trained on subsets of the full compositional range to improve overall performance across the full range, which overcomes the limitations of a single model, and then combine the results via blending.

In this case, they created 3 overlapping submodels for most of the major oxides (low, mid, high), and a full model training on the full compositional range.
:::

## Compositional Ranges {.unlisted}

| Oxide | Full     | Low     | Mid      | High     |
|-------|----------|---------|----------|----------|
| SiO2  | (0, 100) | (0, 50) | (30, 70) | (60, 100)|
| TiO2  | (0, 100) | (0, 2)  | (1, 5)   | (3, 100) |
| Al2O3 | (0, 100) | (0, 12) | (10, 25) | (20, 100)|
| FeOT  | (0, 100) | (0, 15) | (5, 25)  | (15, 100)|
| MgO   | (0, 100) | (0, 3.5)| (0, 20)  | (8, 100) |
| CaO   | (0, 42)  | (0, 7)  | (0, 15)  | (30, 100)|
| Na2O  | (0, 100) | (0, 4)  | *N/A*    | (3.5, 100)|
| K2O   | (0, 100) | (0, 2)  | *N/A*    | (1.5, 100)|

::: {.footer}
All submodel PLS configurations are taken from @cleggRecalibrationMarsScience2017.
:::

::: {.notes}
These are the compositional ranges that @cleggRecalibrationMarsScience2017 presented in their paper.

As can be seen, the ranges overlap.
This is to avoid discontinuities in the final combined results.
They would occur because the samples in each sub-model with the most "extreme" compositions aren't necessarily weighted as strongly when the model is trained, so may be less accurately predicted.
:::


## PLS-SM Inference {.unlisted}
![](/static/methodology/pls_inference.png){fig-align="center" width="70%"}

::: {.notes}
Let me illustrate.

I'm starting out by showing you how the model makes predictions, because that is the simplest view to start from.

It's important to understand that, in practice, this flow is executed for each oxide.
For simplicity's sake, we present it as being done wholly.

The full model is used to estimate the composition of an unknown target, and the appropriate sub-model(s) is then chosen based on this estimation for a more accurate prediction.

If the initial estimate falls within the the range of only a single model, we use the full prediction from that model.
Conversely, if the estimate falls within an overlapping range (blending range), we blend the predictions of the corresponding submodels such that the prediction is weighted favoring the submodel whose range the initial estimate is closest to.
:::

## Preprocessing {auto-animate="true" auto-animate-id="p1" transition="slide-in fade-out" .unlisted}
:::::: {.columns}
::::: {.column width="50%"}
- Remove 'dust' shots
- Average shot intensities
:::::

::::: {.column width="50%"}
![PLS Preprocessing](static/methodology/pls_preprocessing.png){top="-400px" style="position: relative;" data-id="x1"}
<div style="position: absolute; top: 25%; left: 55%; width: 38%; height: 50%; border: 2px solid red; box-sizing: border-box; pointer-events: none;"></div>
:::::
::::::

::: {.notes}
The initial preprocessing step is to remove the first five shots.
These shots usually serve to remove dust from samples, and are therefore often less informative than the remaining shots: the dust doesn't say much about what's in the sample.

Then we proceed to averaging the shot intensity values for each wavelength.
The final result is a single column representing an intensity value for each wavelength.
:::

## Preprocessing {auto-animate="true" auto-animate-id="p1" transition="slide-in fade-out" .unlisted}
::::: {.columns}
:::: {.column width="50%"}
- Remove 'dust' shots
- Average shot intensities
- Mask noisy wavelenghts
::::

:::: {.column width="50%"}
![PLS Preprocessing](static/methodology/pls_preprocessing.png){style="position: relative; top: -400px;" data-id="x1"}
<div style="position: absolute; top: 25%; left: 55%; width: 38%; height: 50%; border: 2px solid red; box-sizing: border-box; pointer-events: none;"></div>
::::
:::::

::: {.notes}
The preprocessing continues by masking noisy wavelengths.
The masking process itself simply involves setting the intensity values for the wavelengths in the masking ranges to zero.
:::

## Preprocessing {auto-animate="true" auto-animate-id="p1" transition="slide-in fade-out" .unlisted}
![](/static/methodology/masked_regions.png){fig-align="center"}

::: {.notes}
As we see here, the mask regions are placed at the edges of the spectrometer ranges.
This is because they were found to be untrustworthy & to generate outliers in the intensity values.
:::


## Preprocessing {auto-animate="true" auto-animate-id="p1" transition="slide-in fade-out" .unlisted}
::::: {.columns}
:::: {.column width="50%"}
- Remove 'dust' shots
- Average shot intensities
- Mask noisy wavelenghts
- Zero out negative values
::::

:::: {.column width="50%"}
![PLS Preprocessing](static/methodology/pls_preprocessing.png){style="position: relative; top: -400px;" data-id="x1"}
<div style="position: absolute; top: 25%; left: 55%; width: 38%; height: 50%; border: 2px solid red; box-sizing: border-box; pointer-events: none;"></div>
::::
:::::

::: {.notes}
After we've masked, we proceed to set negative values to zero.
These negative values represent noise that stems from the pre-CCS preprocessing phase --- likely the continuum removal step.
:::

## Preprocessing {auto-animate="true" auto-animate-id="p1" transition="slide-in fade-out" .unlisted}
:::: {.columns}
::: {.column width="50%"}
- Remove 'dust' shots
- Average shot intensities
- Mask noisy wavelenghts
- Zero out negative values
- Tranpose
- Submodels filter
:::

::: {.column width="50%"}
![PLS Preprocessing](static/methodology/pls_preprocessing.png){style="position: relative; top:-800px;" data-id="x1"}
<div style="position: absolute; top: 25%; left: 55%; width: 38%; height: 57%; border: 2px solid red; box-sizing: border-box; pointer-events: none;"></div>
:::
::::

::: {.notes}
We transpose the data, meaning the wavelengths are now the columns and the rows are the averaged sample location intensity values for the given wavelengths.
Essentially, this means we have 5 locations per samples times 408 samples, giving us 2040 rows.
Of course, in practice, we divide the data into five folds, where one represents the test set.

From here on, the steps become easier to imagine if you 

We now filter the data.
At this point, as illustrated in the figure, we process the data for each oxide.
So for each oxide, we filter it by the compositional range associated with a given submodel.
This gives us the training data for this model's compositional range.
:::


## Preprocessing {auto-animate="true" auto-animate-id="p1" transition="slide-in fade-out" .unlisted}
:::: {.columns}
::: {.column width="50%"}
- Remove 'dust' shots
- Average shot intensities
- Mask noisy wavelenghts
- Zero out negative values
- Tranpose
- Submodels filter
- Normalize
:::

::: {.column width="50%"}
![PLS Preprocessing](static/methodology/pls_preprocessing.png){style="position: relative; top:-1200px;" data-id="x1"}
<div style="position: absolute; top: 25%; left: 55%; width: 38%; height: 50%; border: 2px solid red; box-sizing: border-box; pointer-events: none;"></div>
:::
::::

::: {.notes}
Now we normalize.

**Norm 1** normalizes full spectrum by total across the total across all three spectrometers, so the resulting spectrum adds up to 1.

**Norm 3** normalizes on a per-spectrometer basis, resulting in a full normalized spectrum that sups to 3 (because there are 3 separate detectors...).
:::


## Training Phase {.unlisted}
:::: {.columns}
::: {.column width="50%"}
- Outlier removal
- Each model trained on subset of data
- Cross validated
- Train on all training folds & evaluate
:::

::: {.column width="50%"}
![PLS Training](/static/methodology/pls_training.png){width="50%"}
:::
::::

## Outlier Removal {.unlisted}
Iterative, automated outlier removal process:

1. Compute leverage & spectral residuals
2. Calculate Mahalanobis distance $d$
3. Get critical value $c$
4. Classify sample as outlier if $d>c$
5. Remove outliers & train new model
6. Repeat until model performance ceases to improve

::: {.notes}
The outlier removal process is iterative and works using influence plots of leverage and spectral residuals.
It is iterative because removing spectra that appear as outliers can reveal additional outliers.

We decided to automate this process, whereas the original authors would manually select outliers based on expertise.

The process itself follows the steps seen here.
First we calculate the leverage and spectral residuals from the scores and loadings of a PLS model.

Leverage is a measure of how far the independent variable values of an observation are from other observations. High-leverage points are outliers w.r.t the independent variables, as they have no neighboring points in space.

Spectral residuals measure how well a given spectrum is explained by the model.

Then we calculate the Mahalanobis distance for each point in a matrix of the combined leverage and spectral residuals.

We obtain a critical value that is based on a chi-square distribution for a 97.5% confidence interval given 2 degrees of freedom.

We compare the distances against this critical value, using it as a threshold, where distances above this threshold are classified as outliers.

Then we remove outliers from the dataset, retrain a PLS model, and then repeat the process.
:::



## Outlier Removal {.unlisted}
::: {.r-stack}
![](/static/methodology/FeOT_Full_1.png)

![](/static/methodology/FeOT_Full_2.png){.fragment fragment-index=1}

![](/static/methodology/FeOT_Full_3.png){.fragment fragment-index=2}

![](/static/methodology/FeOT_Full_4.png){.fragment fragment-index=3}

![](/static/methodology/FeOT_Full_5.png){.fragment fragment-index=4}
:::

::: {.notes}
The process itself is easily seen as we step through these influence plots.

As you can see, those items with both a high error and leverage are classified as outliers and are promptly removed.
:::

## ICA
:::: {.columns}
::: {.column width="50%"}
- Similar preprocessing to PLS-SM, except:
    - No averaging
    - No compositional range filtering
    - Uses only one location per sample
:::
::: {.column width="50%"}
![ICA Preprocessing](static/methodology/ica_preprocessing.png){width="55%"}
:::
::::

::: {.notes}
I won't go into much detail about the preprocessing stage in PLS.
The process mostly follows the same steps as in PLS-SM, except we don't filter for any compositional ranges here.

We also do not average the shots, and instead preserve the intensity values for each shot, except the first five.

Finally, we don't use all five location datasets for each sample. That is left as an experiment, which Ivik will go into details about. We chose to do it this way because it was unclear whether the original authors used all location datasets or just one per sample.
:::


## ICA {.unlisted}
:::: {.columns}
::: {.column width="50%"}
- Uses JADE
:::
::: {.column width="50%"}
![JADE](static/methodology/ica_jade.png){width="55%"}
:::
::::

::: {.notes}
At this point, we use the Joint Approximate Diagonalization of Eigenmatrices algorithm to compute ICA scores.

The JADE algorithm starts by computing a mixing matrix, consisting of independent component values for each sample.
By taking the product of the mixing matrix and the normalized data, we get the estimated sources.

We use this to train regression models later.
:::


## ICA {.unlisted}
:::: {.columns}
::: {.column width="50%"}
- Calculate correlation between ICs and wavelengths

:::
::: {.column width="50%"}
![ICA Postprocessing](static/methodology/ica_postprocessing.png){width="50%"}
:::
::::

::: {.notes}
For the postprocessing phase, we start by calculating correlations between the independent components from JADE and the wavelenghts.
Using this, we can identify which wavelengths are associated with which independent components by computing the maximum correlation for each wavelength.
This gives us a matrix of ICA scores that we then use for regression.
This finds the most relevant spectral lines for each component.

We then create a wavelength matrix using the most correlated comopnent for each wavelength
:::


## ICA-Score based Regression Models {.unlisted .smaller}
![a](static/methodology/ica_regression.png)

::: {.notes}
- Train regression models
- Data transformation
- Uses ICA scores
:::


## MOC
- Weighted combination of the predictions
    - Varying weight by oxide
    - Depends on which model performs best

## Differences
- We do not weighing by inverse IRF
- No MAD outlier removal for ICA
- Automated what they did manually (outlier removal PLS)
- Our test fold was not carefully selected to be representative (just randomized)

# Experiments
{{< include sections/_experiments.qmd >}}
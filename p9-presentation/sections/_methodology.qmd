Presenter: Christian Bager Bach Houmann.

::: {.notes}
My name is Christian, and I'll be presenting our replica of the MOC pipeline.<br>
This is what we used to establish baselines for our experiments.
:::

## MOC Pipeline {auto-animate="true"}
::: {style="width=100%;"}
![](/static/methodology/pipeline.png){fig-align="center" width="35%"}
:::
::: {.fragment data-id="moc-box"}
<div style="position: absolute; top: 34.45%; left: 30.5%; width: 18%; height: 35.25%; border: 2px solid red; box-sizing: border-box; pointer-events: none; border-radius: 10px;"></div>
:::

::: {.fragment data-id="moc-box"}
<div style="position: absolute; top: 34.45%; left: 51.25%; width: 18%; height: 35.25%; border: 2px solid red; box-sizing: border-box; pointer-events: none; border-radius: 10px;"></div>
:::

::: {.notes}
This is the entire MOC pipeline at inference-time.<br>
<br>
We have attempted to replicate this pipeline as faithfully as we could, but have had to make some design choices that differ from the original --- mostly due to a lack of information regarding some aspects of the original design.<br>
<br>
It's important to keep in mind that this pipeline is for multivariate regression.<br>
That means we create predictions for each of 8 major oxides.<br>
However, most of the steps actually involve data being processed for each oxide individually.<br>
For both ICA and PLS, you can imagine that after some preprocessing steps, we branch out into 8 branches, each of which predict its own values for its associated oxide.<br>
And in the end, we put the outputs together to produce the MOC prediction.<br>
<br>
I start by walking through the PLS-SM side, and then the ICA side.
:::

<!-- ## PLS-SM 
- Recreating the PLS-SM method presented by @cleggRecalibrationMarsScience2017 and @andersonImprovedAccuracyQuantitative2017
    - Preprocessing
    - Outlier removal
    - Training
    - Inference via submodels

::: {.notes}
Starting with the PLS-SM method, we have attempted to recreate the system presented by both Clegg and Anderson et al.
::: -->

## PLS-SM Motivation {.unlisted auto-animate="true"}
- **PLS**
    - Good for chemometrics
    - Readily intrepretable

::: {.notes}
Let me start by explaining what motivated PLS-SM, because that will help us understand why it does what it does.<br>
<br>
First: they chose PLS because it can handle noisy data with far more variables than observations, and with significant correlation between variables.<br>
It's also readily intrepretable by plotting the regression coefficients as a function of the spectral channels. This shows which wavelength ranges show correlations with the prediction composition.
:::

## PLS-SM Motivation {.unlisted auto-animate="true"}
- PLS
- **PLS2 vs. PLS1**
    - PLS2: Multivariate
    - PLS1: Univariate
    - Better results with separate PLS1 models

::: {.notes}
For the original ChemCam calibration, the team used PLS2. This case corresponds to the case where there are several dependent variables, meaning it can perform multivariate predictions.<br>
<br>
However, they achieved better results by using a separate PLS1 model for each of the major oxides.<br>
<br>
PLS1 is the case where there's only a single dependent variable --- so it's for univariate regression.
:::

## PLS-SM Motivation {.unlisted auto-animate="true"}
- PLS
- PLS2 vs. PLS1
- **Submodels**
    - More accurate when focusing on specific concentration ranges
    - Single regression model: good general performance, worse on some individual samples
    - So combine several models trained on subsets of the full compositional range

::: {.notes}
**OK. So why submodels?**<br>
<br>
Different parts of the LIBS spectrum react differently to varying amounts of elements in the sample.<br>
So, a spectrum from a sample with a medium amount of an element might show patterns that aren't there when there's a very high or low amount of that element.<br>
This can be because when there's too much of an element, it can overshadow its own signal, and the presence of other elements can also change the response.<br>
<br>
Using sub-models lets them focus on specific concentration ranges for more accurate readings.<br>
<br>
It's hard for a single regression model to account for these variations. It often makes a tradeoff where it has good general performance, but performance worse on some samples. And specialist models (trained on a restricted range) will do good for that range, but much worse outside the range than a model trained on the full set.<br>
<br>
So the solution was to combine several regression models trained on subsets of the full compositional range to improve overall performance across the full range, which overcomes the limitations of a single model, and then combine the results via blending.<br>
<br>
In this case, they created 3 overlapping submodels for most of the major oxides (low, mid, high), and a full model training on the full compositional range.
:::

## Compositional Ranges {.unlisted}

| Oxide | Full     | Low     | Mid      | High     |
|-------|----------|---------|----------|----------|
| SiO2  | (0, 100) | (0, 50) | (30, 70) | (60, 100)|
| TiO2  | (0, 100) | (0, 2)  | (1, 5)   | (3, 100) |
| Al2O3 | (0, 100) | (0, 12) | (10, 25) | (20, 100)|
| FeOT  | (0, 100) | (0, 15) | (5, 25)  | (15, 100)|
| MgO   | (0, 100) | (0, 3.5)| (0, 20)  | (8, 100) |
| CaO   | (0, 42)  | (0, 7)  | (0, 15)  | (30, 100)|
| Na2O  | (0, 100) | (0, 4)  | *N/A*    | (3.5, 100)|
| K2O   | (0, 100) | (0, 2)  | *N/A*    | (1.5, 100)|

::: {.footer}
All submodel PLS configurations are taken from @cleggRecalibrationMarsScience2017.
:::

::: {.notes}
These are the compositional ranges that @cleggRecalibrationMarsScience2017 presented in their paper.<br>
<br>
As can be seen, the ranges overlap.<br>
This is to avoid discontinuities in the final combined results.<br>
They would occur because the samples in each sub-model with the most "extreme" compositions aren't necessarily weighted as strongly when the model is trained, so may be less accurately predicted.<br>
<br>
The N/As here mean that there are no compositional ranges defined there.<br>
Essentially meaning there are no Mid submodels for sodium and potassium oxide.
:::


## PLS-SM Inference {.unlisted}
![](/static/methodology/pls_inference.png){fig-align="center" width="70%"}

::: {.notes}
Let me illustrate.<br>
<br>
<!-- I'm starting out by showing you how the model makes predictions, because that is the simplest view to start from.<br>
<br> -->
It's important to understand that, in practice, this flow is executed for each oxide.<br>
For simplicity's sake, we present it as being done wholly.<br>
<br>
The full model is used to estimate the composition of an unknown target, and the appropriate sub-model(s) is then chosen based on this estimate for a more accurate prediction.<br>
<br>
If the initial estimate falls within the the range of only a single model, we use the full prediction from that model.<br>
Conversely, if the estimate falls within an overlapping range (blending range), we blend the predictions of the corresponding submodels such that the prediction is weighted favoring the submodel whose range the initial estimate is closest to.
:::

## Preprocessing {auto-animate="true" auto-animate-id="p1" transition="slide-in fade-out" .unlisted .smaller}
:::::: {.columns}
::::: {.column width="50%"}
- Remove 'dust' shots
- Average shot intensities
:::::

::::: {.column width="50%"}
![PLS Preprocessing](static/methodology/pls_preprocessing.png){top="-400px" style="position: relative;" data-id="x1"}
<div style="position: absolute; top: 25%; left: 55%; width: 38%; height: 50%; border: 2px solid red; box-sizing: border-box; pointer-events: none; border-radius: 10px;"></div>
:::::
::::::

::: {.notes}
The initial preprocessing step is to remove the first five shots.<br>
These shots usually serve to remove dust from samples, and are therefore often less informative than the remaining shots: the dust doesn't say much about what's in the sample.<br>
<br>
Then we proceed to averaging the shot intensity values for each wavelength.<br>
The final result is a single column representing an intensity value for each wavelength.
:::

## Preprocessing {auto-animate="true" auto-animate-id="p1" transition="slide-in fade-out" .unlisted .smaller}
::::: {.columns}
:::: {.column width="50%"}
- Remove 'dust' shots
- Average shot intensities
- Mask noisy wavelenghts
::::

:::: {.column width="50%"}
![PLS Preprocessing](static/methodology/pls_preprocessing.png){style="position: relative; top: -400px;" data-id="x1"}
<div style="position: absolute; top: 25%; left: 55%; width: 38%; height: 50%; border: 2px solid red; box-sizing: border-box; pointer-events: none; border-radius: 10px;" data-id="pbox"></div>
::::
:::::

::: {.notes}
The preprocessing continues by masking noisy wavelengths.<br>
The masking process itself simply involves setting the intensity values for the wavelengths in the masking ranges to zero.
:::

## Preprocessing {auto-animate="true" auto-animate-id="p1" transition="slide-in fade-out" .unlisted .smaller}
![](/static/methodology/masked_regions.png){fig-align="center"}

::: {.notes}
As we see here, the mask regions are placed at the edges of the spectrometer ranges.<br>
This is because they were found to be untrustworthy & to generate outliers in the intensity values.
:::


## Preprocessing {auto-animate="true" auto-animate-id="p1" transition="slide-in fade-out" .unlisted .smaller}
::::: {.columns}
:::: {.column width="50%"}
- Remove 'dust' shots
- Average shot intensities
- Mask noisy wavelenghts
- Zero out negative values
::::

:::: {.column width="50%"}
![PLS Preprocessing](static/methodology/pls_preprocessing.png){style="position: relative; top: -400px;" data-id="x1"}
<div style="position: absolute; top: 25%; left: 55%; width: 38%; height: 50%; border: 2px solid red; box-sizing: border-box; pointer-events: none; border-radius: 10px; border-radius: 10px;" data-id="pbox"></div>
::::
:::::

::: {.notes}
After we've masked, we proceed to set negative values to zero.<br>
These negative values represent noise that stems from the pre-CCS preprocessing phase --- likely the continuum removal step.<br>
<br>
Removing the values would imply we don't know what they mean, but we do.<br>
They represent noise, so we set them to zero.
:::

## Preprocessing {auto-animate="true" auto-animate-id="p1" transition="slide-in fade-out" .unlisted .smaller}
:::: {.columns}
::: {.column width="50%"}
- Remove 'dust' shots
- Average shot intensities
- Mask noisy wavelenghts
- Zero out negative values
- Tranpose
- Submodels filter
:::

::: {.column width="50%"}
![PLS Preprocessing](static/methodology/pls_preprocessing.png){style="position: relative; top:-800px;" data-id="x1"}
<div style="position: absolute; top: 25%; left: 55%; width: 38%; height: 57%; border: 2px solid red; box-sizing: border-box; pointer-events: none; border-radius: 10px;" data-id="pbox"></div>
:::
::::

::: {.notes}
We transpose the data, meaning the wavelengths are now the columns and the rows are the averaged sample location intensity values for the given wavelengths.<br>
Essentially, this means we have 5 locations per samples times 408 samples, giving us 2040 rows.<br>
Of course, in practice, we divide the data into five folds, where one represents the test set.<br>
<br>
We now filter the data.<br>
At this point, as illustrated in the figure, we process the data for each oxide.<br>
So for each oxide, we filter it by the compositional range associated with a given submodel.<br>
This gives us the training data for this model's compositional range.
:::


## Preprocessing {auto-animate="true" auto-animate-id="p1" transition="slide-in fade-out" .unlisted .smaller}
:::: {.columns}
::: {.column width="50%"}
- Remove 'dust' shots
- Average shot intensities
- Mask noisy wavelenghts
- Zero out negative values
- Tranpose
- Submodels filter
- Normalize
    - Norm 1: *normalize full spectrum,* $\text{sum}=1$
    - Norm 3: *normalize per spectrometer,* $\text{sum}=3$
:::

::: {.column width="50%"}
![PLS Preprocessing](static/methodology/pls_preprocessing.png){style="position: relative; top:-1200px;" data-id="x1"}
<div style="position: absolute; top: 25%; left: 55%; width: 38%; height: 45%; border: 2px solid red; box-sizing: border-box; pointer-events: none; border-radius: 10px;" data-id="pbox"></div>
:::
::::

::: {.notes}
Now we normalize.<br>
<br>
**Norm 1** normalizes full spectrum by total across the total across all three spectrometers, so the resulting spectrum adds up to 1.<br>
<br>
**Norm 3** normalizes on a per-spectrometer basis, resulting in a full normalized spectrum adds up to 3 (because there are 3 separate spectrometers...).
:::


## Training Phase {.unlisted}
:::: {.columns}
::: {.column width="50%"}
- Outlier removal
- Each model trained on subset of data
- Cross validated
- Train on all training folds & evaluate
:::

::: {.column width="50%"}
![PLS Training](/static/methodology/pls_training.png){width="50%"}
:::
::::

## Outlier Removal {.unlisted}
Iterative, automated outlier removal process using Influence Plots of Leverage and Spectral Residuals

::: {width="100%"}
![](/static/methodology/FeOT_Full_5.png){width="70%" fig-align="center"}
:::

::: {.notes}
The outlier removal process is iterative and works using influence plots of leverage and spectral residuals.<br>
It is iterative because removing spectra that appear as outliers can reveal additional outliers.<br>
:::

## Outlier Removal {.unlisted}
::: {.incremental}
1. Compute leverage & spectral residuals
2. Calculate Mahalanobis distance $d$
3. Get critical value $c$
4. Classify sample as outlier if $d>c$
5. Remove outliers & train new model
6. Repeat until model performance ceases to improve
:::

::: {.notes}
We decided to automate this process, whereas the original authors would manually select outliers based on their expertise.<br>
<br>
The process itself follows the steps seen here.<br>
<br>
First we calculate the leverage and spectral residuals from the scores and loadings of a PLS model.<br>
<br>
<!-- Leverage is a measure of how far the independent variable values of an observation are from other observations. High-leverage points are outliers w.r.t the independent variables, as they have no neighboring points in space.<br>
<br>
Spectral residuals measure how well a given spectrum is explained by the model. -->
Then we calculate the Mahalanobis distance for each point in a matrix of the combined leverage and spectral residuals.<br>
<br>
We obtain a critical value that is based on a chi-square distribution for a 97.5% confidence interval given 2 degrees of freedom.<br>
<br>
We compare the distances against this critical value, using it as a threshold, where distances above this threshold are classified as outliers.<br>
<br>
Then we remove outliers from the dataset, retrain a PLS model, and then repeat the process.
:::

## Outlier Removal {.unlisted}
::: {.r-stack}
![](/static/methodology/FeOT_Full_1.png)

![](/static/methodology/FeOT_Full_2.png){.fragment fragment-index=1}

![](/static/methodology/FeOT_Full_3.png){.fragment fragment-index=2}

![](/static/methodology/FeOT_Full_4.png){.fragment fragment-index=3}

![](/static/methodology/FeOT_Full_5.png){.fragment fragment-index=4}
:::

::: {.notes}
The process itself is easily seen as we step through these influence plots.<br>
<br>
As you can see, those items with both a high error and leverage are classified as outliers and are promptly removed.
:::

## MOC Pipeline {auto-animate="true"}
::: {style="width=100%;"}
![](/static/methodology/pipeline.png){fig-align="center" width="35%"}
:::

::: {data-id="moc-box"}
<div style="position: absolute; top: 34.45%; left: 51.25%; width: 18%; height: 35.25%; border: 2px solid red; box-sizing: border-box; pointer-events: none; border-radius: 10px;"></div>
:::

## ICA Preprocessing
:::: {.columns}
::: {.column width="50%"}
- Similar preprocessing to PLS-SM, except:
    - No averaging
    - No compositional range filtering
    - Uses only one location per sample
:::
::: {.column width="50%"}
![ICA Preprocessing](static/methodology/ica_preprocessing.png){width="55%"}
:::
::::

::: {.notes}
I won't go into much detail about the preprocessing stage in the ICA phase.<br>
The process mostly follows the same steps as in PLS-SM, except we don't filter for any compositional ranges here.<br>
<br>
We also do not average the shots, and instead preserve the intensity values for each shot, except the first five.<br>
<br>
Finally, we don't use all five location datasets for each sample. That is left as an experiment, which Ivik will go into details about. We chose to do it this way because it was unclear whether the original authors used all location datasets or just one per sample.
:::


## ICA - JADE {.unlisted}
:::: {.columns}
::: {.column width="50%"}
- ICA
- Using JADE
- Get estimated sources from the data
:::
::: {.column width="50%"}
![JADE](static/methodology/ica_jade.png){width="55%"}
:::
::::

::: {.notes}
To actually do Independent Component Analysis (ICA), we use the JADE algorithm, just as the original authors.<br>
ICA is conceptually similar to PCA in that it decomposes data to a smaller number of variables.<br>
The main idea to to take some original set of data, consisting of mixed signals. <br>

Then reconstruct these via:

- a mixing matrix, each column measuring how much each source contributes to the mixed signals, and
- a matrix of independent components, called the estimated sources matrix

<br>
The result of running JADE is the estimated sources.<br>
We'll use these in the next step, postprocessing.
:::


## ICA Postprocessing {.unlisted}
:::: {.columns}
::: {.column width="50%"}
- Calculate correlation between ICs and wavelengths
- Find most correlated
- Get composition values for samples

:::
::: {.column width="50%"}
![ICA Postprocessing](static/methodology/ica_postprocessing.png){width="50%"}
:::
::::

::: {.notes}
For the postprocessing phase, we start by calculating correlations between the estimated sources from JADE and the wavelenghts.<br>
Using this, we can identify which wavelengths are associated with which independent components by computing the maximum correlation for each wavelength.<br>
This gives us a matrix of ICA scores that we then use for regression.<br>
This finds the most relevant spectral lines for each component.<br>
<br>
Finally, we curate the compositions for the samples we've processed, which will be used to train the regression models.
:::


## ICA-Score based Regression Models {.unlisted .smaller}
![a](static/methodology/ica_regression.png)

::: {.notes}
- We train regression models on the ICA scores and composition data
- First, the ICA scores go through data transformation to one of the regression laws seen here
- The regression models are then trained with the scores as inputs and composition data as targets
:::


## Multivariate Oxide Compositions {auto-animate="true"}
::: {style="width=100%;"}
![](/static/methodology/pipeline.png){fig-align="center" width="35%"}
:::
<div style="position: absolute; top: 75.5%; left: 40.9%; width: 18%; height: 21%; border: 2px solid red; box-sizing: border-box; pointer-events: none; border-radius: 10px;" data-id="moc-box"></div>

::: {.notes}
- Varying weight by oxide
- Depends on which model performs best
:::

## MOC Pipeline {auto-animate="true" .smaller}
::::: {.columns}
:::: {.column width="50%"}
| Oxide  | PLS1-SM (%) | ICA (%) |
|--------|-------------|---------|
| Al2O3  | 75          | 25      |
| FeOT   | 75          | 25      |
| SiO2   | 50          | 50      |
| Na2O   | 40          | 60      |
| K2O    | 25          | 75      |
| TiO2   | 50          | 50      |
| MgO    | 50          | 50      |
| CaO    | 50          | 50      |

::::

:::: {.column width="50%"}
![](/static/methodology/pipeline.png){fig-align="center" width="75%"}

<div style="position: absolute; top: 79%; left: 65%; width: 18%; height: 21%; border: 2px solid red; box-sizing: border-box; pointer-events: none; border-radius: 10px;" data-id="moc-box"></div>
::::
:::::

::: {.notes}
The weights can be seen here.<br>
:::

## Differences
::: {.incremental}
- We do not weighing by inverse IRF
- No MAD outlier removal for ICA
- Automated what they did manually (outlier removal PLS)
- Our test fold was not carefully selected to be representative (just randomized)
:::

::: {.notes}
1. **We Do Not Weigh by Inverse IRF**: 
   - **Reason**: Avoiding the inverse IRF weighting is based on criticism that it distorts the alignment between spectral data from different instruments. The inverse IRF could introduce more noise and disrupt the comparison between spectra collected by different instruments, one in Los Alamos and another on Mars.
2. **No MAD Outlier Removal for ICA**:
   - **Reason**: The decision to omit MAD-based outlier removal in the ICA phase is due to insufficient description of the method by Clegg et al., making replication without assumptions difficult. Additionally, retaining a comprehensive dataset is preferred to ensure the integrity of analysis.
3. **Automated Outlier Removal for PLS**:
4. **Randomized Test Fold Selection**:
   - This approach, while simpler, might not ensure that the test set is fully representative of the broader dataset.
:::


\section{Experiments}\label{sec:experiments}
In this section, we detail the experiments conducted to evaluate the performance of each component within the MOC pipeline by using our replica as a baseline for comparison and assessment of experimental results.

The experiments are structured as follows:

\begin{enumerate}
    \item Evaluating the necessity of automated outlier removal in the PLS1-SM component by comparing performance with and without this process.
    \item Investigating the effect of maintaining the leverage and residuals in the outlier removal process of PLS1-SM from the second iteration onwards.
    \item Assessing the impact of the MAD-based method for outlier removal in the ICA phase.
    \item Determining the effect on ICA performance when aggregating datasets from five locations compared to a single dataset.
    \item Comparing the performance of PLS1-SM and ICA models against alternative models, such as XGBoost and ANN.
\end{enumerate}

These experiments were selected to explore the significance of each MOC component and the comparative effectiveness of different modeling approaches.
The first experiment focuses on the automated outlier removal process in PLS1-SM, examining its necessity by comparing outcomes with and without this step.
The second experiment looks at the implications of using fixed threshold values for outlier removal in PLS1-SM, opting for a conservative approach by not updating these values after each iteration.
In the third experiment, we apply MAD-based outlier removal in ICA, comparing its effectiveness against the baseline.
The fourth experiment evaluates ICA's performance using aggregated datasets from multiple locations, aiming to understand the balance between representativeness and information loss.
The final experiment extends the analysis to include comparisons with other models, providing a broader perspective on the MOC pipeline's performance.

\subsection{Replication of the MOC Pipeline}\label{sec:replica_moc}
We present the baseline RMSEs of the original models and our replicas in Table~\ref{tab:results_rmses}.
Additionally, we show the results of the PLS1-SM replica sub-models in Table~\ref{table:rmsecv_results}. This table presents the RMSE across different composition ranges, the number of spectra used in the model, the total number of outliers removed, and the number of outlier removal iterations for each sub-model (low, mid, high, full). We show both the average and minimum RMSE of the cross-validation folds. RMSEP is the RMSE of the test set.
Figure~\ref{fig:rmse_histograms} illustrates the distribution of the RMSEs of the original and our replicas of the PLS1-SM, ICA, and MOC models as a grouped histogram.
The results show that the RMSEs of our replicas are similar to the original models.
However, there are some notable differences --- in some cases, our replicas outperform the original models, while in other cases, the original models outperform our replicas.

These differences can be attributed to a number of factors.

Firstly, the original models were trained on two datasets, one acquired at a 1600mm standoff distance and one acquired at a 3000mm standoff distance.
We have only used the 1600mm dataset for our replicas since we do not have access to the 3000mm dataset.
As mentioned in Section~\ref{sec:outlier_removal}, we also chose to automate the outlier removal process for the PLS1-SM phase, whereas the original authors performed this manually.
Moreover, we chose to exclude the outlier removal step during the ICA phase to avoid introducing unsubstantiated assumptions, as described in Section~\ref{sec:ica_data_preprocessing}, and our analysis was also limited to a single dataset per sample due to the absence of details on their integration.

For training the PLS models, \citet{andersonImprovedAccuracyQuantitative2017} methodically organized their training and test sets by sorting samples based on the major oxide, sequentially assigning them to folds, removing outliers, and deliberately including extreme compositions in the training folds to enhance the model's ability to handle a broad range of elemental variations.
Since we lack the domain expertize to replicate this process, we instead randomly split the dataset into training and test sets without any further curation using an 80/20 split.
Additionally, without going into speculations, it is possible that some of the differences are due to implementation details, such as the use of different programming languages and libraries.
Lastly, it is worth noting that RMSE is simply a statistical measure of the differences between the actual and predicted values, and does not necessarily reflect the true accuracy of the models on unseen data from Mars, and so the results should be interpreted with this in mind.

\begin{table*}[h]
\centering
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lllllll}
\hline
Element    & PLS1-SM (original) & PLS1-SM (replica) & ICA (original) & ICA (replica) & MOC (original) & MOC (replica) \\
\hline
\ce{SiO2}  & 4.33               & 5.81              & 8.31           & 10.68         & 5.30           & 7.29 \\
\ce{TiO2}  & 0.94               & 0.47              & 1.44           & 0.63          & 1.03           & 0.49 \\
\ce{Al2O3} & 2.85               & 1.94              & 4.77           & 5.55          & 3.47           & 2.39 \\
\ce{FeO_T} & 2.01               & 4.35              & 5.17           & 8.30          & 2.31           & 5.21 \\
\ce{MgO}   & 1.06               & 1.17              & 4.08           & 2.90          & 2.21           & 1.67 \\
\ce{CaO}   & 2.65               & 1.43              & 3.07           & 3.52          & 2.72           & 1.81 \\
\ce{Na2O}  & 0.62               & 0.66              & 2.29           & 1.72          & 0.62           & 1.10 \\
\ce{K2O}   & 0.72               & 0.72              & 0.98           & 1.37          & 0.82           & 1.09 \\
\hline
\end{tabular*}
\caption{RMSE of the original and our replicas of the PLS1-SM, ICA, and MOC models.}
\label{tab:results_rmses}
\end{table*}

\begin{figure*}[b]
	\centering
	\includegraphics[width=0.85\textwidth]{images/rmse_historgram.png}
	\caption{Grouped histogram of the RMSEs of the original and our replicas of the PLS1-SM, ICA, and MOC models.}
	\label{fig:rmse_histograms}
\end{figure*}

\citet{andersonImprovedAccuracyQuantitative2017} used the Student's $t$-test to show their new model outperformed the old one. In our study, we apply the same test but with a different aim: to verify if there is no statistically significant difference between our replicated models and the original models presented in \citet{cleggRecalibrationMarsScience2017}. This approach allows us to assess whether our models demonstrate equivalence rather than improvement.
In conducting our analysis with the Student's $t$-test, we define our hypotheses and choose a significance level to guide our interpretation of the results.
Our null hypothesis (\(H_0\)) posits that there is no significant difference between the mean performance of our replicated models and that of the originals.
Conversely, the alternative hypothesis (\(H_1\)) suggests that there is a significant difference between the two sets of models.
We set a significance level (\(\alpha\)) of 5\%, which establishes the threshold for determining statistical significance.
If the p-values obtained from our $t$-tests are less than 5\% (\(p < 0.05\)), we will reject the null hypothesis, indicating that there is a significant difference between our replicated models and those of the original pipeline.
Conversely, if the p-value is greater than or equal to 5\% (\(p \geq 0.05\)), we fail to reject the null hypothesis, suggesting that our models and the original models are statistically similar, thereby achieving our goal of demonstrating equivalence rather than disparity.

Following the approach delineated in \citet{andersonImprovedAccuracyQuantitative2017}, we start by calculating the uncertainty of the RMSE values, $S_{RMSE}^2$:
$$
S_{\text{RMSE}}^2 = \left(\frac{\text{RMSE}^2}{n}\right) \left[n - 1 - \frac{2\Gamma^2\left(\frac{n}{2}\right)}{\Gamma^2\left(\frac{n - 1}{2}\right)}\right], \\
$$
where $n$ is the number of samples, and $\Gamma$ is the gamma function. 
This formula captures the variance of the RMSE, reflecting the dispersion of error magnitudes.
Subsequently, the t-statistic $t$ is calculated to evaluate the statistical significance of the difference between the RMSEs of the original and replicated models:
$$
t = \frac{\text{RMSE}_A - \text{RMSE}_B}{\sqrt{S_{\text{RMSE}_A}^2 + S_{\text{RMSE}_B}^2}},
$$
where $\text{RMSE}_A$ and $\text{RMSE}_B$ correspond to the RMSE values of the original and replicated models, respectively.
The degrees of freedom $f$ associated with this comparison are determined by:
$$
f = \frac{\left(S_{\text{RMSE}_A}^2 + S_{\text{RMSE}_B}^2\right)^2}{\frac{S^4_{\text{RMSE}_A}}{n_A - 1} + \frac{S^4_{\text{RMSE}_B}}{n_B - 1}},
$$
which accounts for the variances in RMSE values and the respective sample sizes ($n_A$ and $n_B$) of the models being compared.
Finally, the p-value is derived from the $t$-distribution's cumulative distribution function:
% Note to selves:
% This can be calculated using the cumulative distribution function of the t-distribution.
% https://stackoverflow.com/questions/17559897/python-p-value-from-t-statistic
% https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html
% We've used: p_value = stats.t.sf(np.abs(t_value), degrees_of_freedom) * 2
% where t_value is the t-statistic and degrees_of_freedom is the degrees of freedom.
% But you could use: p_value = 2 * ( 1 - stats.t.cdf(abs(t_value), degrees_of_freedom))
% Just as well.
$$p\text{-value} = 2 \times \left(1 - F_{\text{T}}(|t|)\right)$$
where $F_{\text{T}}$ is the cumulative distribution function of the t-distribution with $f$ degrees of freedom.
Higher p-values indicate greater compatibility with the null hypothesis.
We present the results of our $t$-tests in Table~\ref{table:results_ttests}.

\begin{table}[h]
\centering
\begin{tabular}{llll}
\hline
Oxide & PLS1-SM & ICA & MOC \\
\hline
\ce{SiO2} & 42.00\% & 48.75\% & 38.45\% \\
\ce{TiO2} & 9.73\% & 6.31\% & 8.23\% \\
\ce{Al2O3} & 30.23\% & 67.06\% & 31.58\% \\
\ce{FeO_T} & 7.48\% & 21.68\% & 6.57\% \\
\ce{MgO} & 78.05\% & 35.44\% & 44.08\% \\
\ce{CaO} & 12.73\% & 70.04\% & 27.79\% \\
\ce{Na2O} & 85.96\% & 43.16\% & 14.93\% \\
\ce{K2O} & 100.00\% & 36.27\% & 43.40\% \\
\hline
\end{tabular}
\caption{Results of the PLS1-SM, ICA, and MOC $t$-tests.}
\label{table:results_ttests}
\end{table}

The results from our $t$-tests are summarized as follows:

\begin{itemize}
    \item \textbf{PLS1-SM:} The p-values observed across the spectrum, most notably the 100.00\% for \ce{K2O}, unequivocally suggest a strong statistical proximity to the original models for a majority of the parameters. This is particularly indicative of a successful replication effort. However, the relatively lower p-value for \ce{FeO_T} (7.48\%) underscores an area where the model's performance diverges from the original. Given the high variance noted in Section~\ref{sec:data_overview} for \ce{FeO_T} compositions, this deviation could be ascribed to inherent data variability rather than model inaccuracy.

    \item \textbf{ICA:} This model demonstrates considerable alignment with the original models across several constituents, as evidenced by p-values like 67.06\% for \ce{Al2O3} and 70.04\% for \ce{CaO}. These figures indicate a noteworthy approximation to the original models' performance. Conversely, the p-value for \ce{TiO2} (6.31\%) marks an exception, indicating a region where the ICA model might benefit from refinement. The noted high variance in \ce{TiO2} values within our dataset likely contributes to this outlier, pointing to data variability as a contributing factor.

    \item \textbf{MOC:} The replicated MOC model's p-values, especially the 43.40\% for \ce{K2O}, reiterate its consistency with the original model. This aligns with our hypothesis of statistical equivalence. Yet, as with PLS1-SM, the \ce{FeO_T} component exhibits a lower p-value (6.57\%), highlighting an area of divergence potentially attributed to the previously discussed variance in \ce{FeO_T} compositions.
\end{itemize}

This comparative analysis affirms a satisfying statistical equivalence between our replicated models and the original.

However, the lower p-values associated with \ce{FeO_T} for both PLS1-SM and MOC, in addition to \ce{TiO_2} for ICA, flag these elements as focal points for potential refinement.
Given our dataset's inherent variability in the compositions of these specific elements, as substantiated in Section~\ref{sec:data_overview}, these findings are interpretable and not wholly unexpected.

In conclusion, our $t$-tests demonstrate that our replicated models are statistically similar to the original models, thereby achieving our goal of demonstrating equivalence rather than disparity.
As such, they serve their purpose as a baseline for identifying which aspects of the pipeline contribute the most to the overall error.

\begin{table*}[htbp] % Use htbp for more placement flexibility
\centering
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} lllllll}
\hline
Model & RMSECV (avg) & RMSECV (min) & RMSEP & \# of spectra & \# outliers removed & Outlier removal iterations  \\
\hline
\ce{SiO2} &&&&&& \\
  Low & 8.55 & 6.14 & 6.57 & 439 & 0 & 2 \\
  Mid & 4.71 & 3.64 & 4.20 & 1268 & 15 & 1 \\
  High & 3.59 & 3.01 & 4.26 & 605 & 1 & 2 \\
  Full & 5.98 & 4.75 & 7.18 & 1538 & 8 & 1 \\
\\
\ce{TiO2} &&&&&& \\
  Low & 0.29 & 0.28 & 0.39 & 1359 & 8 & 1 \\
  Mid & 0.62 & 0.57 & 0.44 & 418 & 6 & 4 \\
  High & 0.74 & 0.11 & 0.09 & 40 & 0 & 2 \\
  Full & 0.48 & 0.37 & 0.50 & 1538 & 16 & 4 \\
\\
\ce{Al2O3} &&&&&& \\
  Low & 2.40 & 1.68 & 1.99 & 324 & 0 & 2 \\
  Mid & 2.27 & 1.57 & 2.04 & 1198 & 12 & 2 \\
  High & 3.97 & 1.99 & 2.03 & 240 & 0 & 2 \\
  Full & 3.31 & 2.59 & 2.43 & 1538 & 9 & 1 \\
\\
\ce{FeO_T} &&&&&& \\
  Low & 1.81 & 1.72 & 1.55 & 1438 & 1 & 3 \\
  Mid & 2.64 & 2.15 & 1.69 & 978 & 28 & 9 \\
  High & 4.00 & 1.52 & 11.89 & 105 & 0 & 2 \\
  Full & 2.86 & 2.67 & 4.08 & 1538 & 23 & 5 \\
\\
\ce{MgO} &&&&&& \\
  Low & 0.49 & 0.45 & 0.63 & 1000 & 7 & 4 \\
  Mid & 1.32 & 0.88 & 1.16 & 1488 & 10 & 1 \\
  High & 4.42 & 1.99 & 3.14 & 135 & 0 & 1 \\
  Full & 1.74 & 1.36 & 1.25 & 1538 & 41 & 6 \\
\\
\ce{CaO} &&&&&& \\
  Low & 0.79 & 0.57 & 0.78 & 1070 & 22 & 6 \\
  Mid & 1.40 & 1.07 & 1.02 & 1428 & 20 & 4 \\
  High & 1.89 & 0.72 & 1.85 & 35 & 0 & 2 \\
  Full & 1.62 & 1.19 & 1.81 & 1538 & 41 & 8 \\
\\
\ce{Na2O} &&&&&& \\
  Low & 0.57 & 0.48 & 0.57 & 1278 & 8 & 3 \\
  High & 1.27 & 0.65 & 0.60 & 375 & 0 & 1 \\
  Full & 1.05 & 0.59 & 1.00 & 1538 & 37 & 7 \\
\\
\ce{K2O} &&&&&& \\
  Low & 0.39 & 0.28 & 0.34 & 773 & 5 & 3 \\
  High & 0.98 & 0.58 & 0.71 & 920 & 13 & 2 \\
  Full & 1.03 & 0.92 & 0.81 & 1538 & 48 & 8 \\
\\

\end{tabular*}
\caption{Summary of PLS model performance for the major oxides.}
\label{table:rmsecv_results}
\end{table*}


\input{sections/experiments/pls_outlier.tex}
\input{sections/experiments/pls_thresholds.tex}
\input{sections/experiments/ica_outlier.tex}
\input{sections/experiments/ica_aggregated.tex}
\input{sections/experiments/other_models.tex}

\subsection{Summary}\label{sec:experiments_summary}
The experiments conducted in this section provide an assessment of the MOC pipeline's components, offering insights into their individual and collective impacts on the system's overall performance.

% ----- summary of the experiments
Firstly, our experiments demonstrated that the MAD-based outlier removal method significantly improves the ICA models' accuracy for most oxides, underscoring the critical role of outlier removal in enhancing predictive performance of the ICA component.
This finding contrasts with the PLS1-SM models, which exhibited robustness to outliers, indicating that the impact of outlier removal varies across different modeling approaches within the MOC pipeline.

Secondly, the study highlighted the double-edged nature of data aggregation for ICA.
While aggregation can reduce noise for certain oxides, leading to improved predictive accuracy, it can also obscure information or introduce noise for others, resulting in decreased performance.
This emphasizes considering the specific characteristics and variability of each oxide and location, highlighting the importance of tailored data preprocessing methods to achieve optimal model performance.

Furthermore, we found that the XGBoost model performed exceptionally well, only being outperformed by the original MOC model on \ce{FeO_T}.
This suggests that gradient boosting methods, with their ability to handle complex data well, offer substantial benefits over traditional models.
The ANN model also showed promising results, often outperforming the original MOC model, and we expect that it could perform even better with a larger training dataset.

Our replica models underperform in predicting \ce{FeO_T} compared to the original MOC model, a discrepancy likely due to the high variance in \ce{FeO_T} compositions identified in our calibration samples, as described in Section~\ref{sec:data_overview}.
The randomized split of training and testing sets in our approach likely failed to encompass the comprehensive range of \ce{FeO_T} compositions, explaining our models' predictive shortcomings --- an issue not encountered by the original study's methodical sample division.

% --- key conclusions
We can draw several key conclusions from the experiments conducted in this section regarding the limitations of the MOC pipeline components:

\vspace{2mm}\noindent\textit{The choice of model and the volume of training data are paramount in influencing the pipeline's accuracy and reliability.}

\noindent
Specifically, our findings underscore the superior robustness of PLS1-SM to outliers compared to ICA, making it a more favorable choice.
However, the efficacy of PLS1-SM is surpassed by ensemble models such as XGBoost across a variety of oxides.
This delineates the significance of ensemble models in enhancing predictive accuracy.
Additionally, while ANNs demonstrate potential, their performance is dependent on the availability of substantial training data. 
Despite the efforts of NASA to collect high quality data, it is a time consuming effort, and as such, the currently available data is comparatively small.
This is supported by the claims made by \citet{lepore_quantitative_2022}, discussed in Section~\ref{sec:related_works}, that more data leads to more accurate geochemical analysis.
Therefore, our ability to fully assess the ANN's capabilities is, in part, limited by this factor.

Furthermore, we showed that choosing a model that is robust against outliers exhibits superior performance, as demonstrated by the major improvement in the ICA models' performances when MAD-based outlier removal was introduced.
This indicates that the key to advancing the MOC pipeline lies not within outlier removal methods but in judicious model selection and the enhancement of the training data corpus.
Moving forward, prioritizing the exploration of efficient ensemble models and the expansion of the training dataset emerges as a crucial strategy for overcoming the most significant barriers to reducing prediction error within the MOC pipeline.

\vspace{2mm}\noindent\textit{The ICA component of the MOC pipeline is a candidate for replacement in future iterations, given its high RMSEs and poorer performance compared to other models.}

\noindent
Both the insufficiency of ICA, as well as the promising performance of gradient boosting, is further reinforced by \citet{andersonPostlandingMajorElement2022}. In their model selection for the SuperCam instrument, they chose not to go forward with the existing MOC pipeline, favoring alternative modeling approaches instead.

\vspace{2mm}\noindent
Finally, an aspect that is out of scope for this study is the inherent limitations stemming from the calibration dataset itself.
The fact that it was acquired on Earth and not on Mars, and that it was acquired using a different LIBS instrument than the one used on Mars, causes a misalignment between the calibration dataset and the unseen data gathered by Mars rovers.
In fact, NASA acknowledges this limitation, and is currently working on a mission to bring samples from Mars back to Earth, where they can be analyzed in laboratories around the world \cite{mars-sample-return}.
This would allow for the creation of a calibration dataset that is more representative of the unseen data, which would allow for more accurate predictions.
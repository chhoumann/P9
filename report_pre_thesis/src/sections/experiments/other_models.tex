\subsection{Experiment: Other Models}\label{sec:experiment_other_models}
\citet{cleggRecalibrationMarsScience2017} have only compared their new approach with the original method presented by \citet{wiensPreFlight3}, and have not conducted experiments using alternative methods to establish the superiority of their chosen approach.
Therefore, we decided to compare the performance of the PLS1-SM and ICA models to other models.
The objective was to evaluate two distinct scenarios.
In the first scenario, we aimed to conduct a direct comparison between the MOC model and an alternative model. The second scenario revolves around substituting either PLS or ICA with a different model and then calculating a weighted average.
We conducted the experiments using the following models:

\begin{itemize}
	\item \textbf{XGBoost}, a gradient boosting algorithm \cite{chen_xgboost_2016}
	\item \textbf{ANN}, a neural network model
\end{itemize}

Building on the findings of \citet{andersonPostlandingMajorElement2022}, who demonstrated Gradient Boosting Regression (GBR) models' superior predictive accuracy in analyzing major oxide compositions in geological samples, we adopted XGBoost for its refined GBR implementation.
This decision was informed by XGBoost's documented success in enhancing model precision, as detailed in Section~\ref{sec:related_works}.

In light of \citeauthor{takahashi_quantitative_2017}'s findings on the limitations of traditional LIBS calibration methods and the potential of ANNs to address non-linearities in solid sample analysis, described in Section~\ref{sec:related_works}, we also decided to include ANNs in our experiments.
This approach was motivated by ANNs' capacity to discern complex, non-linear relationships within LIBS spectra.

The ANN comprises of a sequence of fully connected (dense) layers and dropout layers, aimed at reducing overfitting through regularization.
The architecture begins with an input layer of 6144 units, reflecting the high-dimensional nature of the input data.
This is immediately followed by a series of linear transformations and non-linear activations (ReLU), reducing the dimensionality in a stepwise fashion to 1024, 512, 256, and finally 128 units across four layers.
Each of these reductions is aimed at distilling the essential features required for accurate regression outcomes.
The inclusion of dropout layers with a dropout rate of 0.3 after the first and second dense layers further aids in mitigating the risk of overfitting by randomly omitting a subset of features during the training phase.
The final layer of ANN outputs 8 variables, corresponding to the eight major oxides we are trying to predict the compositional values of.
Notably, this layer does not employ an activation function, as it aims to produce linear outputs that directly correspond to the predicted values for each of the regression targets.


\begin{table*}[h]
\centering
\begin{tabular}{lllllll}
\hline
Element    & ANN (Norm1)   & ANN (Norm3) & XGBoost (Norm1) & XGBoost (Norm3) & MOC (original) & MOC (replica) \\
\hline
\ce{SiO2}  & 5.62          & 5.01        & 5.12            & \textbf{4.67}   & 5.30           & 7.29 \\
\ce{TiO2}  & 0.58          & 0.62        & \textbf{0.44}   & 0.45            & 1.03           & 0.49 \\
\ce{Al2O3} & 2.12          & 2.27        & \textbf{1.93}   & 1.97            & 3.47           & 2.39 \\
\ce{FeO_T} & 4.05          & 4.00        & 4.40            & 5.02            & \textbf{2.31}  & 5.21 \\
\ce{MgO}   & 1.61          & 1.49        & 0.99            & \textbf{0.96}   & 2.21           & 1.67 \\
\ce{CaO}   & 1.33          & 1.26        & \textbf{1.23}   & 1.26            & 2.72           & 1.81 \\
\ce{Na2O}  & 1.17          & 1.09        & \textbf{0.49}   & 0.51            & 0.62           & 1.10 \\
\ce{K2O}   & 1.05          & 0.88        & \textbf{0.50}   & 0.51            & 0.82           & 1.09 \\
\hline
\end{tabular}
\caption{RMSEs for the ANN and XGBoost models using Norm 1 and Norm 3. The RMSEs for the MOC models are included for comparison.}
\label{tab:other_models_rmses}
\end{table*}

The results presented in Table~\ref{tab:other_models_rmses} provide a comparison of the RMSE metric performance between the artificial neural network (ANN), XGBoost models, and the MOC (Model of Calibration) under two normalization conditions (Norm 1 and Norm 3), across the eight major oxides.

Overall, the results suggest that XGBoost, particularly under the Norm 1 condition, consistently provides superior predictive accuracy for most of the oxides analyzed, with the exception of \ce{FeO_T} where the original MOC model excels.
This indicates the potential benefits of integrating advanced machine learning techniques, such as gradient boosting, into the analysis of LIBS data for geological samples.
The superior performance of XGBoost could be attributed to its sophisticated handling of complex data structures and its ability to minimize overfitting, making it a robust choice for predictive modeling in this context.

ANNs also show promising results, particularly in scenarios where traditional models might not capture complex non-linear relationships effectively.
However, their performance is generally outperformed by XGBoost, suggesting that while ANNs have the capacity to model complex phenomena within LIBS spectra, the specific architecture and training procedures might need further optimization to fully leverage their potential.
The relatively small training dataset might also be a contributing factor to the ANN's performance, as ANNs typically require a large amount of data to train effectively.

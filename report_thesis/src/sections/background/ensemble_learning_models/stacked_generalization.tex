\subsubsection{Stacked Generalization}\label{subsec:stacked-generalization}
Stacked generalization, introduced by \citet{wolpertstacked_1992}, is an ensemble method that combines the predictions of multiple base models, which are trained on the original dataset, as input to a meta-model.
This meta-model is trained to make the final prediction.
The strategy allows the meta-model to learn the optimal way to combine the predictions of the base models to minimize the generalization error.

Formally, let $\mathbf{X}$ denote the input data and $\mathbf{y}$ the target variable.
Initially, $N$ base models $G_1, G_2, \ldots, G_N$ are trained on the dataset $\mathbf{X}$.
Each base model generates a set of predictions $\hat{\mathbf{y}}_i = G_i(\mathbf{X})$.

The predictions from the base models are then compiled into a new dataset $\mathbf{Z}$, where each column $\mathbf{z}_i \in \mathbf{Z}$ represents the predictions from the $i$-th base model:

$$
\mathbf{Z} = [\hat{\mathbf{y}}_1, \hat{\mathbf{y}}_2, \ldots, \hat{\mathbf{y}}_N]
$$

A meta-model $F$ is subsequently trained on this new dataset $\mathbf{Z}$ to predict the target variable $\mathbf{\hat{y}}$:

$$
\mathbf{\hat{y}} = F(\mathbf{Z})
$$

The final prediction is provided by the meta-model $F$, which effectively integrates the outputs of the base models to enhance overall performance.
The effectiveness of stacked generalization stems from its ability to leverage the unique strengths of different base models while mitigating their individual weaknesses, thereby producing a more accurate and generalizable ensemble model.
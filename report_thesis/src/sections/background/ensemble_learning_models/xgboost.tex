\subsubsection{XGBoost}
We now describe XGBoost based on \citet{ChenGuestrin2016}.

\gls{xgboost} is an advanced implementation of gradient boosting designed for speed and performance. 
It builds upon traditional boosting techniques by introducing several algorithmic enhancements to improve execution speed and model performance.

The core mechanism of \gls{xgboost} can be summarized in the following way:
The algorithm first sorts the feature values and visits the data in sorted order to evaluate all possible split by accumulating gradient statistics.
Then, each possible split is evaluated using a regularized objective function. 
This function incorporates both a loss function that measures the difference between predicted and actual values and a regularization term that penalizes the complexity of the model.
This additional regularization term helps prevent overfitting by smoothening the final learned weights.
For each candidate split, the algorithm calculates a split score based on the accumulated statistics. 
The algorithm chooses the split with the best split score, which minimizes the regularized objective function.
This chosen split it used to partition the data into two groups.
This process is recursively applied to each partition, building a tree structure by continually splitting the data to minimize the regularized objective function at each node.

To speed up the task of evaluating split points, \gls{xgboost} leverages parallelism to distribute these computations across multiple processors. 
By letting each processor handle a subset of features and possible split points, the algorithm can evaluate many potential split points simultaneously.
Additionally, techniques such as shrinkage and column subsampling are employed. 
When predictions from newly added trees are incorporated, shrinkage scales these predictions by a factor $\eta$ after each step.
This reduces the influence of each tree and allows for new trees to improve the model.
Column subsampling involves using a random subset of features for splitting which, for some problems, improves the model's computational efficiency and prevents overfitting.


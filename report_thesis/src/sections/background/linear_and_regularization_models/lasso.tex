\subsubsection{Least Absolute Shrinkage and Selection Operator (LASSO) Regression}
Having introduced ridge regression, we now turn our attention to the \gls{lasso} regression based on \citet{James2023AnIS}.

\gls{lasso} regression addresses the same issue of multicollinearity as ridge regression but does so by introducing a different regularization term to the \gls{ols} regression objective function.
While ridge regression uses the $L_2$ norm, \gls{lasso} uses the $L_1$ norm, which has the distinct effect of performing feature selection by shrinking some coefficients to exactly zero:

$$
L_1 = \sum_{j=1}^{p} |\beta_j|,
$$

where $\beta_j$ are the regression coefficients, and $p$ is the number of features.
The \gls{lasso} regression objective function $f(\beta)$ is defined as:

$$
f_{LASSO}(\beta) = \text{RSS} + \lambda L_1,
$$

where $\text{RSS}$ is the residual sum of squares defined in Section~\ref{sec:ols} and $\lambda$ is the regularization parameter.
By introducing this $L_1$ regularization term, \gls{lasso} not only penalizes large coefficients but also has the property of setting some coefficients to zero, effectively selecting a simpler model that only includes the most important features.
This makes \gls{lasso} particularly useful when dealing with high-dimensional data where feature selection is crucial.
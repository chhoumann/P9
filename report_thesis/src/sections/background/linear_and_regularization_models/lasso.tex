\subsubsection{Least Absolute Shrinkage and Selection Operator (LASSO) Regression}
A disadvantage of ridge regression is that it includes all features in the final model.
While it does shrink the coefficients towards zero, it does not eliminate any.
This may not impact prediction accuracy but makes interpretation harder, especially with many variables.
For example, in datasets with numerous features, we might prefer a model that includes only the most important ones.
However, ridge regression includes all features, and while increasing the penalty reduces coefficient sizes, it does not exclude variables\cite{James2023AnIS}.

We therefore introduce \gls{lasso} regression based on \citet{James2023AnIS}, a regression technique which addresses this issue by adding a different regularization term to the \gls{ols} regression objective function.
While ridge regression uses the $L_2$ norm, \gls{lasso} uses the $L_1$ norm, which has the distinct effect of performing feature selection by shrinking some coefficients to exactly zero:

$$
L_1 = \sum_{j=1}^{p} |\beta_j|,
$$

where $\beta_j$ are the regression coefficients, and $p$ is the number of features.
The \gls{lasso} regression objective function $f_{LASSO}(\beta)$ is defined as:

$$
f_{LASSO}(\beta) = \text{RSS} + \lambda L_1,
$$

where $\text{RSS}$ is the residual sum of squares defined in Section~\ref{sec:ols} and $\lambda$ is the regularization parameter.
By introducing this $L_1$ regularization term, \gls{lasso} not only penalizes large coefficients but also has the property of setting some coefficients to zero, effectively selecting a simpler model that only includes the most important features.
This makes \gls{lasso} particularly useful when dealing with high-dimensional data where feature selection is crucial.
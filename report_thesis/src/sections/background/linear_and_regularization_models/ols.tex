\subsubsection{Ordinary Least Squares Regression}\label{sec:ols}
We briefly cover the \gls{ols} regression algorithm based on \citet{James2023AnIS} to provide context for the subsequent discussions on linear and regularization models.

\gls{ols} regression is a linear regression technique that fits a linear model, $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$, to the data by minimizing the residual sum of squares, where $\hat{y}$ is the predicted target value, $\hat{\beta}_0$ is the intercept, $\hat{\beta}_1$ is the coefficient for the input feature, and $x$ is the input feature.
In \gls{ols} regression, the objective is to estimate the coefficients that minimize the sum of squared differences between the observed target values and the predicted values, known as the \gls{rss}:

$$
\text{RSS} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2,
$$

where $y_i$ is the observed target value, $\hat{y}_i$ is the predicted target value, and $n$ is the number of samples in the dataset.
To minimize the \gls{rss}, \gls{ols} regression finds the coefficients $\hat{\beta}_0$ and $\hat{\beta}_1$ that minimize the \gls{rss}.
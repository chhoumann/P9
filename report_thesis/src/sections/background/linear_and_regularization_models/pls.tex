\subsubsection{Principal Component Regression (PCR) and Partial Least Squares (PLS)}\label{subsec:pls}
We now describe \gls{pls} based on \citet{James2023AnIS}.
In order to understand \gls{pls}, it is helpful to first consider \gls{pcr}, as \gls{pls} is an extension of \gls{pcr} that aims to address some of its limitations.

\gls{pcr} extends \gls{pca} in the context of regression analysis.
First, \gls{pca} is performed to identify the $k$ principal components that capture the most variance in the data.
These components are then used in a linear regression model to predict the target variable by fitting a linear model via least squares regression.
The key intuition behind \gls{pcr} is a small number of principal components are sufficient to capture most of the variance in the data, which can then be used to predict the target variable.
That is, the directions of the principal components are assumed to be associated with the target variable.
If this assumption holds, a least squares regression model fitted to the principal components provides better predictions than a model fitted to the original features because the principal components indeed capture the most important information in the data.

One drawback of \gls{pcr} is that it does not consider the target in the decomposition of the features and therefore assumes that smaller components have a weaker correlation with the target than the larger ones.
In other words, the components that capture the most variation may not be the most predictive of the target.
Some data might be highly variable but irrelevant to the target.
To address this limitation, \gls{pls} extends \gls{pcr} by considering the target variable when identifying the components.

\gls{pls} uses an iterative method to identify components that maximize the covariance between the features and the target.
Similar to \gls{pcr}, \gls{pls} identifies $k$ components that capture the most variance in the data and fits a linear model with least squares regression.
However, unlike \gls{pcr}, \gls{pls} uses the target variable to identify the components that are most predictive of the target, resulting in components that not only approximate the data but also relate to the target variable.
In essence, \gls{pls} aims to find components that are both informative about the data and predictive of the target.

This is an iterative process where the residuals from the previous components are used to calculate the next component.
The $m$-th component, for example, is derived from the residuals of the previous $m-1$ components.
One can interpret these residuals as the part of the data that has not been explained by the previous components.
After some number of iterations, the components are used to predict the target variable by fitting a linear model via least squares regression, just like in \gls{pcr}.
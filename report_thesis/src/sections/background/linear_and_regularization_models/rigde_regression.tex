\subsubsection{Ridge Regression}
As an alternative to \gls{ols}, one can fit a model with all $N$ features by applying techniques that constrain or regularize the coefficient estimates, effectively shrinking them towards zero.
Although it may seem counterintuitive, this constraint can significantly reduce variance in the estimates.
One of the most well-known techniques for this purpose are is ridge regression, which we now provide an overview of, based on \citet{James2023AnIS}.

Ridge regression introduces a regularization term to the \gls{ols} regression objective function to prevent overfitting and reduce the model's variance.
This regularization term is known as the $L_2$ norm and is defined as the sum of the squared regression coefficients:

$$
L_2 = \sum_{j=1}^{p} \beta_j^2,
$$

where $\beta_j$ are the regression coefficients, and $p$ is the number of features.
The ridge regression objective function $f_{ridge}(\beta)$ is defined as:

$$
f_{ridge}(\beta) = \text{RSS} + \lambda L_2
$$

where $\text{RSS}$ is the residual sum of squares defined in Section~\ref{sec:ols} and $\lambda$ is the regularization parameter --- a hyperparameter that controls the strength of the penalty.
Introducing this regularization term causes "shrinkage", which means that the estimated regression coefficients are shrunk towards zero, reducing their variance.
This shrinkage usually results in a model with higher bias compared to one fitted with \gls{ols} regression because the regularization can cause a worse fit to the training data.
However, the model has a lower variance, making it less prone to overfitting and more generalizable to unseen data.
\subsubsection{Ridge Regression}
A limitation of \gls{ols} regression is that it cannot handle multicollinearity effectively.
We now provide an overview of ridge regression, an extension of \gls{ols} regression designed to address this limitation, based on \citet{James2023AnIS}.
Ridge regression introduces a regularization term to the \gls{ols} regression objective function to prevent overfitting and reduce the model's variance.
This regularization term is known as the $L_2$ norm and is defined as the sum of the squared regression coefficients:

$$
L_2 = \sum_{j=1}^{p} \beta_j^2,
$$

where $\beta_j$ are the regression coefficients, and $p$ is the number of features.
The ridge regression objective function $f_{ridge}(\beta)$ is defined as:

$$
f_{ridge}(\beta) = \text{RSS} + \lambda L_2
$$

where $\text{RSS}$ is the residual sum of squares defined in Section~\ref{sec:ols} and $\lambda$ is the regularization parameter --- a hyperparameter that controls the strength of the penalty.
Introducing this regularization term usually results in a model with higher bias compared to one fitted with \gls{ols} regression because the regularization can cause a worse fit to the training data.
However, the model has a lower variance, making it less prone to overfitting and more generalizable to unseen data.
\subsection{Design for Initial Experiment}\label{sec:initial-experiment}
As described in Section~\ref{sec:proposed_approach}, we conducted a series of initial experiments to evaluate the performance of various machine learning models on the prediction of major oxide compositions from our \gls{libs} dataset.
These experiments aimed to provide a preliminary assessment of the models' performance, allowing us to identify the most promising models for further evaluation and inclusion in our stacking ensemble.
All models were trained on the same preprocessed data using the Norm 3 preprocessing method described in Section~\ref{sec:norm3}.
This ensured that the models' performance could be evaluated under consistent and comparable conditions.

All models were trained using our data partitioning and cross-validation strategy, as described in Section~\ref{subsec:validation_testing_procedures}. 
To ensure as fair of a comparison between models as possible, all models were trained using as many default hyperparameters as possible and those hyperparameters that did not have default options were selected based on values found in the literature.
However, due to the nature of the neural network models architecture, some extra time was spent on tuning the models to ensure a fair comparison.
Finally, all the models ran only one trial per oxide. 
Although running multiple trials would allow for an average performance evaluation, we argue that the variability of the results in each trial would be small enough that an average performance would not change the ranking of the models. 
Additionally, as stated, the goal of this experiment is merely to get an initial indication of the performance of the models.
Therefore, along with the choice of hyperparameters, it was not meaningful to run multiple trials for each model.

The hyperparameters used for the models in the initial experiment can be found in the Appendix~\ref{subsec:initial_experiment_hyperparameters}.
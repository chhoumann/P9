\subsection{Optimization Experiment Design}\label{subsec:optimization_experiment_design}
Using the remaining ten models, we conducted an extended experiment to further refine their performance for each oxide. 
The goal was to identify which preprocessing techniques and hyperparameters would yield the best performance for each model by doing a thorough search for each configuration. 
To achieve this, we evaluated multiple permutations of each model with various preprocessors and hyperparameter configurations. 
Each configuration included a mandatory scaler, while data transformation and dimensionality reduction techniques were optional. 
The optimization process was conducted using our optimization framework, outlined in Section~\ref{sec:optimization_framework}

To ensure a fair assessment of each configuration, we needed to balance conducting enough iterations for the optimization to converge with the practical limitations imposed by our time constraints.
Therefore, we decided to perform 200 iterations per model for each oxide, resulting in a total of 16,000 iterations across ten models and eight oxides.
We deemed this to be a reasonable number of iterations to obtain a reliable indication of the performance of each configuration.
As mentioned in Section~\ref{sec:optimization_framework}, we used the \gls{tpe} algorithm for the optimization process.
For this sampler, we set the number of startup trials to 25\%.
The number of startup trials determines the number of random samples drawn before the \gls{tpe} sampler engages.
By choosing 25\%, we would reserved the first quarter of the iterations for exploration.
We believed that this would allow the sampler enough time to explore the search space while still providing enough iterations for refinement. 

For the experiment, we defined a range or set of discrete values in each hyperparameter for the models and preprocessors. 
To determine these ranges, we used a mixture of what was represented in the literature, our own analysis, or the default values for each hyperparameter as a starting point. 
Our methodology was then to expand the hyperparameters that had value ranges to include some reasonable lower and upper extremes. 
For hyperparameters with a discrete set of possible values, we included all options. 
As an example, for the \gls{pls} model, we used the elbow method to approximate the optimal number of components. 
Based on this, we defined the lower extreme as 1 and the upper extreme as 30, as we believed that the optimal number of components would be somewhere within this range. 
A similar approach was used for the preprocessor \gls{kernel-pca}, where we defined the number of components to be between 1 and 100.

A different example is \gls{gbr}, which was based on the default values for the hyperparameters.
The default value for the number of estimators is 100, so we defined this to be the bottom of the range and 1000 as the upper extreme. 
Given the complexity of the patterns in \gls{libs} data, we believed that the ideal number of weak learners would likely be above 100, and therefore believed that 100 was a reasonable lower bound. 
Determining the upper bound was more difficult, but we believed that 1000 was a reasonable upper bound, as it would allow the model to sufficiently capture the patterns in the data. 
Given that we allow for a relatively large number of estimators, we wanted to balance this with a relatively low bound for the learning rate. 
We did this to ensure the search space included a learning rate that could scale with the number of estimators and reduce the likelihood of overfitting. 
The default value for the learning rate is 0.1, so we defined the lower bound to be $10^{-3}$ and the upper bound to be 1. 
The max depth of each weak learner was set between 3 and 10, allowing for varying levels of complexity depending on what is needed based on the number of estimators. 
The subsample parameter was set between 0.5 and 1.0, to accommodate random sampling of the data when fitting each weak learner. 
Finally, the max features parameter was set to either \textit{sqrt} or \textit{log2}. Since this parameter has a discrete set of possible values, we included all options.

Using this approach of considering reasonable lower and upper bounds for each hyperparameter or using all options for discrete hyperparameters, we defined the ranges for each model and preprocessor.

The selected hyperparameter ranges for each model and preprocessor can be found in Table~\ref{tab:optuna_model_configurations} and Table~\ref{tab:optuna_preprocessing_configurations}, respectively.



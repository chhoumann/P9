\section{Future Work}\label{sec:future_work}
The findings of this study present several opportunities for future research.
Firstly, regarding our data partitioning algorithm detailed in Section~\ref{subsubsec:dataset_partitioning}, we observed the significance of identifying the optimal percentile value $p$. 
This value is crucial for minimizing extreme values in the test set while preserving its overall representativeness.
Future work should explore quantitative methods for determining this optimal value.
Such methods could involve incorporating supplementary extreme value testing into the data partitioning algorithm. 
After the primary evaluation, additional testing could be conducted using a small, separate subset of extreme values to assess the model's performance in these critical scenarios.
For example, this might involve slightly reducing the percentile value $p$ and using the extreme values that fall within this reduced range to evaluate the model's effectiveness.

Another point of interest is the limited data availability.
The small dataset size inherently restricts the number of extreme values present.
These extreme values are crucial for enhancing the model's generalizability, as they represent the most challenging cases to predict.
Future research should investigate methods for augmenting the dataset with synthetic extreme value data to provide the model with more exposure to these cases during training.

Future work should also consider further experimentation with the choices of base estimators and meta-learners.
Our study demonstrated that various model and preprocessor configurations perform well.
However, identifying the optimal configurations and meta-learner for a specific oxide remains a challenging task.
In this study, we used a simple grouping method to ensure diversity in our base estimator selection, choosing from the top-performing configurations.
This approach could be improved upon by, for example, developing more advanced selection methods that consider the interactions between base estimators and meta-learners.
\section{Methodology}\label{sec:methodology}
In this section, we outline the methodology used in this study to address the challenges identified in the problem definition. Our objective was to identify the most promising machine learning models and preprocessing techniques proposed in the literature, outlined in Section~\ref{sec:related-work}, for predicting major oxide compositions from LIBS data. Then, using this knowledge, develop a pipeline that utilizes the strengths of these models and preprocessing techniques to improve prediction accuracy and robustness of the predictions.
We first present a description of the datasets used and how it was prepared. Then the preprocessing and model selection process is described followed by the experimental setup and the evaluation metrics employed. Finally, we describe our validation testing procedures and how the final model evaluations were conducted to ensure unbiased results.

\subsection{Description of Datasets}

\subsection{Model Selection}

Based on our findings, we tested various methodologies on our data.
This allowed us to identify and focus on the models that performed best and were most relevant to our specific dataset.

Drawing from the related literature and initial experimentation, we selected the following models for further investigation: \gls{svr}, \gls{gbr}, \gls{pls}, \gls{xgboost}, \gls{ngboost}, \gls{etr}, \gls{enet}, and \gls{sgd}.
This selection was partly guided by conventional criteria and partly by exploratory intuition, aiming to discover potentially innovative applications and performances within our specific dataset.

All models demonstrated robust performance; however, \gls{gbr}, \gls{svr}, \gls{xgboost}, \gls{etr}, and \gls{pls} consistently excelled across all oxides.
Notably, we observed model-specific strengths for certain oxidesâ€”\gls{svr} was particularly effective for \ce{SiO2}, while \gls{gbr} excelled with \ce{FeO_T}.
This differential performance prompted us to explore architectural frameworks that could systematically capitalize on the strengths of each model for specific oxides.
Consequently, we concluded that a stacking ensemble method would optimize outcomes for our dataset.
This approach is analogous to the methodologies employed in the original MOC pipeline, which also tailored predictions for each oxide by blending outputs from the PLS-SM and ICA phases, variably weighting the influence of the ICA predictions depending on the oxide.
However, unlike the MOC model, which required manual determination of the model weightings for each oxide, our stacking approach utilizes a meta learner.
This meta learner learns optimal parameter settings, ensuring that the model integration is not only more efficient but also dynamically adapted to our dataset's characteristics and does not require domain-specific knowledge.
This represents a more sophisticated method, as it automates critical decisions and enhances predictive accuracy through learned rather than preset integrations.

\subsection{Experimental Setup}

\subsection{Evaluation Metrics}
To evaluate the performance of our models in predicting major oxide compositions from \gls{libs} data, we will use two key metrics: \gls{rmse} and standard deviation of prediction errors.

\gls{rmse} will be used as a measure of accuracy, quantifying the difference between the predicted and actual values of the major oxides in the samples. It is defined by the equation:

\begin{equation}
    RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2},
\end{equation}

where $y_i$ represents the actual values, $\hat{y}_i$ the predicted values, and $n$ the number of observations. A lower RMSE indicates better accuracy.

To assess the robustness of our models, we will consider the standard deviation of prediction errors across each oxide and test instance. This metric measures the variability of the prediction errors and provides insight into the consistency of the model's performance. It is defined as:

\begin{equation}
    \sigma_{error} = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (e_i - \bar{e})^2},
\end{equation}

where $e_i = y_i - \hat{y}_i$ and $\bar{e}$ is the mean error. A lower standard deviation indicates better robustness.

By using these two metrics, we aim to evaluate model performance in terms of both accuracy and robustness, which are crucial for the reliable prediction of major oxide compositions from \gls{libs} data.

\subsection{Validation and Testing Procedures}

\subsubsection{Stacking Ensemble Method}

% Explain the stacking ensemble method
% Explain the way we implemented + the final model architecture/pipeline

\subsubsection{PCA, PCR \& PLS}
\gls{pca} is a dimensionality reduction technique that transforms a set of possibly correlated variables into a smaller set of uncorrelated variables called \textit{principal components}.
Mathematically, \gls{pca} involves the eigen decomposition of the covariance matrix of the data, $\mathbf{X}^T \mathbf{X}$.
First, the data is centered by subtracting the mean of each variable.
The eigenvectors and eigenvalues of the covariance matrix are then computed.
The eigenvectors represent the directions of maximum variance, and the eigenvalues indicate the amount of variance captured in each of those directions.
These eigenvectors are ordered by their corresponding eigenvalues in descending order, which helps in identifying which components capture the most variance.
In the context of dimensionality reduction, only the top $n$ components are retained.
These are considered enough to capture the most significant aspects of the data.
Then, the original data points can be projected onto the space defined by these top $n$ components.
This step transforms the high-dimensional data into a lower-dimensional space by retaining only the dimensions that contain the most significant variance.
A property of the principal components is that they are uncorrelated, which means that calculating the covariance matrix of each pair of components results in a diagonal matrix where the off-diagonal elements are zero.

\gls{pcr} extends \gls{pca} in the context of regression analysis.
\gls{pcr} involves performing \gls{pca} on the features $\mathbf{X}$ and then using the principal components with the highest variances as features in a linear regression model.
This approach can be beneficial when the original features are highly correlated, as the principal components are orthogonal to each other, reducing multicollinearity.
If $\mathbf{X}$ is decomposed into $\mathbf{TP}^T + \mathbf{E}$ where $\mathbf{T}$ are the scores and $\mathbf{P}$ are the loadings, \gls{pcr} uses $\mathbf{T}$ to predict the target $\mathbf{y}$ as $\mathbf{y} = \mathbf{Tb} + \mathbf{e}$.

However, one drawback of \gls{pcr} is that it does not consider the target in the decomposition of the features and therefore assumes that smaller components have a weaker correlation with the target than the larger ones. 
This assumption does not always hold, which is what \gls{pls} aims to address.
\gls{pls} uses an iterative method to identify components that maximize the covariance between the features and the target. 
It does so by placing the highest weight on that most strongly correlated with the target using the coefficients derived from the covariance between the target and the features:
$$
    Z = \sum_{j=1}^{p} \phi_j X_j 
$$
Where $Z$ is the component, $X_j$ is the $j$-th feature, and $\phi_j$ is the weight of the $j$-th feature and $\phi_j$ is derived using the following equation:

$$
    \phi_j = \frac{cov(X_j, Y)}{var(X_j)}
$$

To refine the model iteratively, PLS uses the residuals from the previous components to calculate the next component. The $m$-th component, for example, is derived from the residuals of the previous $m-1$ components:

$$
    Z_m = \sum_{j=1}^{p} \phi_{jm} \hat{X}_{j, m-1}
$$

The components are then used to predict the target variable by fitting a linear model via least squares regression.

% Explain PLS mathematically (similar to that one book, but not as detailed)
% Explain why it may perform well on our data

\subsubsection{GBR} % maybe put these in the background section instead for the five models, however that might come out of the blue??
% Explain GBR mathematically (similar to that one book, but not as detailed)
% Explain why it may perform well on our data

\subsubsection{SVR}
% Explain SVR mathematically (similar to that one book, but not as detailed)
% Explain why it may perform well on our data

\subsubsection{XGBoost}
% Explain XGBoost mathematically (similar to that one book, but not as detailed)
% Explain why it may perform well on our data

\subsubsection{ETR}
% Explain ETR mathematically (similar to that one book, but not as detailed)
% Explain why it may perform well on our data

\subsubsection{Results}
% Present the results of the experiments and discuss them


%1. Introduction to Methodology: Begin with an overview that explains the purpose of your methodology section. Briefly describe the goals of your experiments, such as testing different machine learning models to determine which yields the best results in your specific field.
% 
%2. Description of Datasets:
%Data Sources: Describe where your data comes from, ensuring you cover any ethical considerations, permissions, and data integrity.
%Data Preparation: Detail the steps you took to prepare the data for analysis. This includes cleaning, normalizing, feature extraction, and any other preprocessing methods.
%Data Splitting: Explain how you divided the data (e.g., training, validation, test sets). Mention the rationale behind your choice of split ratios.
%
%3. Model Selection:
%Literature Review: Summarize the existing models and architectures from the literature that influenced your choices. Explain why these models are relevant to your field and your specific problem.
%Experimented Models: List the models you have chosen to test. Include variations in architectures or configurations if applicable.
%
%4. Experimental Setup:
%Software and Hardware: Specify the software (including version numbers) and hardware used in the experiments to allow for reproducibility.
%Configuration Details: For each model, describe the configuration settings such as learning rate, number of layers, number of neurons per layer, activation functions, etc. and refer to apppendix for actual values
%Training Process: Describe how each model was trained. Include details on the number of epochs, batch size, optimization techniques, and any regularization methods used.
%
%5. Evaluation Metrics:
%Criteria for Evaluation: Describe the metrics used to evaluate the models. Common metrics might include accuracy, precision, recall, F1 score, ROC-AUC, etc., depending on your specific application.
%Rationale: Explain why these metrics are suitable for assessing model performance in your context.
%
%6. Validation and Testing Procedures:
%Validation Technique: Explain how you validated the models (e.g., cross-validation, hold-out validation).
%Testing: Detail the testing process and how the final model evaluations were conducted to ensure unbiased results.
%
%7. Challenges and Limitations:
%Encountered Issues: Discuss any challenges you faced during the experimental process, such as model convergence issues or data imbalances.
%Limitations of the Approach: Acknowledge any limitations in your methodology, such as potential biases in the data or overfitting in the models.
%
%8. Summary: Conclude the methodology section by summarizing the key points made and briefly stating how this setup helps achieve the objectives stated at the beginning.
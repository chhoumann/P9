\section{Methodology}\label{sec:methodology}
In this section, we outline the methodology used in this study to address the challenges identified in the problem definition. Our objective was to identify the most promising machine learning models and preprocessing techniques proposed from the literature, as outlined in Section~\ref{sec:related-work}, for predicting major oxide compositions from LIBS data. 
Then, using this knowledge, develop a pipeline that utilizes the strengths of these models and preprocessing techniques to improve prediction accuracy and robustness of the predictions.
We first describe the datasets used, including their preparation and the method of splitting for model training. Next, we outline the preprocessing steps and the model selection process, followed by a detailed explanation of the experimental setup and evaluation metrics. Finally, we discuss our validation testing procedures and the approach taken to ensure unbiased final model evaluations.

\subsection{Data Prepraration}
Similarly to our previous work \cite{p9_paper}, we used the publicly available \gls{ccs} data. 
\gls{ccs} refers to \gls{libs} data that has been through a series of preprocessing steps such as subtracting the ambient light background, noise removal and removing the electron continuum to derive data that is more suitable for quantitative analysis. 
The full description of this preprocessing can be found in \citet{wiensPreFlight3}.

While the \gls{ccs} data is in a more suitable form for quantitative analysis, it still requires further preprocessing.
Specifically, the data are still noisy at the spectrometer edges. These edges are defined as the boundaries of each of the three spectrometers, who collectively cover the \gls{uv}, \gls{vio}, and \gls{vnir} light spectrum.
These noisy edges are in the following ranges: 240.811 — 246.635, 338.457 — 340.797, 382.138 — 387.859, 473.184 — 492.427, and 849 — 905.574 nm .
In addition to being noisy regions, they also do not contain diagnostic peaks related to the major oxides. 
Therefore, they are masked out to improve the accuracy and reliability of the quantitative analysis\citet{cleggRecalibrationMarsScience2017}.

Additionally, as a result of the aforementioned preprossing applied to the raw \gls{libs} data, negative values are present in the \gls{ccs} data.
These negative values are not physically meaningful, since you cannot have negative light intensity. Since they are assumed to be calculation errors, these values are set to zero rather than entirely removed, as they are still considered valuable information.

% TODO: We are only using PLS part of the pipeline and we are doing crossvalidation. Therefore below section needs to be rewritten.
Once preprocessed, the data was conventionally split into training and testing sets. 
As detailed in \citet{p9_paper} and \citet{cleggRecalibrationMarsScience2017}, it is essential to include samples with extreme element compositions in the training set to ensure the model can generalize effectively.
To incorporate these extreme values, the training set was constructed to include the two highest and two lowest samples from each oxide category.
Following the inclusion of extreme samples, we adjusted the size of the test split to accommodate the reduced pool of normal samples.
This adjustment was calculated by determining the number of extreme samples, subtracting this from the total sample count, and recalculating the test size proportionally. 
This ensures that the proportion of the test set remains appropriate relative to the now smaller dataset. 
Subsequently, we conducted the train-test split, where the normal samples --- those not categorized as extreme --- were divided according to the newly adjusted test size, with the extreme samples included to ensure exposure to varied data points.

\subsection{Model and Preprocessing Selection}
For the initial investigative experiments, we selected a range of models for further exploration, namely \gls{svr}, \gls{gbr}, \gls{pls}, \gls{xgboost}, \gls{ngboost}, \gls{etr}, \gls{enet}, and \gls{sgd}.
We also included regular \gls{ann} and \gls{cnn} in this phase of experimentation. 
This selection was guided primarily by the literature review but also by exploratory intuition, aiming to discover potentially innovative applications and performances within our specific dataset.

Our literature review highlighted various approaches and their effectiveness in handling the challenges associated with predicting major oxide compositions from \gls{libs} data. 
For instance, \citet{andersonImprovedAccuracyQuantitative2017} discussed the use of multiple regression models, finding that different models excelled with specific oxides, which informed our model-specific approach. 
\citet{song_DF-K-ELM} presented a hybrid model combining domain knowledge with machine learning, which inspired our interest in models that could offer both high performance and interpretability. \citet{rezaei_dimensionality_reduction} demonstrated the beneficial impact of dimensionality reduction techniques like \gls{pca}, which we considered essential for managing our high-dimensional \gls{libs} data.

All models demonstrated robust performance; however, \gls{gbr}, \gls{svr}, \gls{xgboost}, \gls{etr}, and \gls{pls} consistently excelled across all oxides. 
Notably, we observed model-specific strengths for certain oxides --- \gls{svr} was particularly effective for \ce{SiO2}, while \gls{gbr} excelled with \ce{FeO_T}. 
This differential performance prompted us to explore architectural frameworks that could systematically capitalize on the strengths of each model for specific oxides.

A particularly exciting discovery from our literature review was a study on the stacking and chaining of normalization methods initially aimed at classification contexts. 
Inspired by these ideas, we explored the possibility of improving model performance by optimizing the preprocessing chain for each model, per oxide, in order to determine which normalization methods would be most beneficial.
For this purpose, we considered the following preprocessing techniques: Min-Max Scaling, Standard Scaling, Robust Scaling, MaxAbs Scaling, Quantile Transformer and Power Transformer. 
Additionally, we also considered dimensionality reduction techniques such as \gls{pca} and \gls{kernel-pca}.
Through experimentation, we identified which preprocessing techniques were most effective for each model and oxide. 
Realizing the potential of this, we decided to further investigate how the predictive accuracy and robustness could be improved by combining the strengths of these models and preprocessing techniques.
From our investigation, we determined that the stacking ensemble method seemed very promising and novel in the LIBS analysis field, leading us to select it as the primary method for our study.

Stacking ensemble is analogous to the methodologies employed in the original \gls{moc} pipeline, which also tailored predictions for each oxide by blending outputs from the \gls{pls1-sm} and \gls{ica} phases, variably weighting the influence of the \gls{ica} predictions depending on the oxide.
Unlike the \gls{moc} model, which required manual determination of the model weightings for each oxide, our method utilizes a meta learner to learn optimal parameter settings. 
Stacking ensemble is beneficial as it dynamically adapts to our dataset's characteristics without the need for domain-specific knowledge.
This approach represents a more sophisticated method, streamlining complex model configurations and potentially enhancing predictive accuracy through dynamically learned integrations, rather than fixed presets.

\subsection{Experimental Setup}

\subsection{Validation and Testing Procedures}


\subsection{Summary}

\section{Methodology}\label{sec:methodology}
To address the objectives of this study described in the problem definition in Section~\ref{sec:problem_definition}, we adopted an experimental approach.
The initial phase of the project was dedicated to identifying the current state of the art in machine learning methodologies and reviewing recent literature specifically on LIBS data analysis, as outlined in Section~\ref{sec:related-work}.
% Here, we should mention what the literature review told us about preprocessing and feature selection, which guided our experiments with these steps, and the explorative approach to the preprocessing and feature selection steps.

Based on our findings, we tested various methodologies on our data.
This allowed us to identify and focus on the models that performed best and were most relevant to our specific dataset.

Drawing from the related literature and initial experimentation, we selected the following models for further investigation: \gls{svr}, \gls{gbr}, \gls{pls}, \gls{xgboost}, \gls{ngboost}, \gls{etr}, \gls{enet}, and \gls{sgd}.
This selection was partly guided by conventional criteria and partly by exploratory intuition, aiming to discover potentially innovative applications and performances within our specific dataset.

All models demonstrated robust performance; however, \gls{gbr}, \gls{svr}, \gls{xgboost}, \gls{etr}, and \gls{pls} consistently excelled across all oxides.
Notably, we observed model-specific strengths for certain oxidesâ€”\gls{svr} was particularly effective for \ce{SiO2}, while \gls{gbr} excelled with \ce{FeO_T}.
This differential performance prompted us to explore architectural frameworks that could systematically capitalize on the strengths of each model for specific oxides.
Consequently, we concluded that a stacking ensemble method would optimize outcomes for our dataset.
This approach is analogous to the methodologies employed in the original MOC pipeline, which also tailored predictions for each oxide by blending outputs from the PLS-SM and ICA phases, variably weighting the influence of the ICA predictions depending on the oxide.
However, unlike the MOC model, which required manual determination of the model weightings for each oxide, our stacking approach utilizes a meta learner.
This meta learner learns optimal parameter settings, ensuring that the model integration is not only more efficient but also dynamically adapted to our dataset's characteristics and does not require domain-specific knowledge.
This represents a more sophisticated method, as it automates critical decisions and enhances predictive accuracy through learned rather than preset integrations.

In this section, we first describe our evaluation metrics and the rationale behind their selection.
We use these metrics to assess the performance of our models in predicting major oxide compositions from LIBS data.
We then present the architecture of the developed pipeline, which utilizes the proposed stacking ensemble method.
As part of this discussion, we provide a detailed explanation of the five models that we selected for the ensemble as well as the specific preprocessing steps that we applied to our data.
Finally, we discuss the results and their implications for the field of LIBS data analysis.

\subsection{Evaluation Metrics}
To evaluate the performance of our models in predicting major oxide compositions from \gls{libs} data, we will use two key metrics: \gls{rmse} and standard deviation of prediction errors.

\gls{rmse} will be used as a measure of accuracy, quantifying the difference between the predicted and actual values of the major oxides in the samples. It is defined by the equation:

\begin{equation}
    RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2},
\end{equation}

where $y_i$ represents the actual values, $\hat{y}_i$ the predicted values, and $n$ the number of observations. A lower RMSE indicates better accuracy.

To assess the robustness of our models, we will consider the standard deviation of prediction errors across each oxide and test instance. This metric measures the variability of the prediction errors and provides insight into the consistency of the model's performance. It is defined as:

\begin{equation}
    \sigma_{error} = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (e_i - \bar{e})^2},
\end{equation}

where $e_i = y_i - \hat{y}_i$ and $\bar{e}$ is the mean error. A lower standard deviation indicates better robustness.

By using these two metrics, we aim to evaluate model performance in terms of both accuracy and robustness, which are crucial for the reliable prediction of major oxide compositions from \gls{libs} data.

\subsubsection{Stacking Ensemble Method}
% Explain the stacking ensemble method
% Explain the way we implemented + the final model architecture/pipeline

\subsubsection{PCA, PCR \& PLS}
\gls{pca} is a dimensionality reduction technique that transforms a set of possibly correlated variables into a smaller set of uncorrelated variables called \textit{principal components}.
Mathematically, \gls{pca} involves the eigen decomposition of the covariance matrix of the data, $\mathbf{X}^T \mathbf{X}$.
First, the data is centered by subtracting the mean of each variable.
The eigenvectors and eigenvalues of the covariance matrix are then computed.
The eigenvectors represent the directions of maximum variance, and the eigenvalues indicate the amount of variance captured in each of those directions.
These eigenvectors are ordered by their corresponding eigenvalues in descending order, which helps in identifying which components capture the most variance.
In the context of dimensionality reduction, only the top $n$ components are retained.
These are considered enough to capture the most significant aspects of the data.
Then, the original data points can be projected onto the space defined by these top $n$ components.
This step transforms the high-dimensional data into a lower-dimensional space by retaining only the dimensions that contain the most significant variance.
A property of the principal components is that they are uncorrelated, which means that calculating the covariance matrix of each pair of components results in a diagonal matrix where the off-diagonal elements are zero.

\gls{pcr} extends \gls{pca} in the context of regression analysis.
\gls{pcr} involves performing \gls{pca} on the features $\mathbf{X}$ and then using the principal components with the highest variances as features in a linear regression model.
This approach can be beneficial when the original features are highly correlated, as the principal components are orthogonal to each other, reducing multicollinearity.
If $\mathbf{X}$ is decomposed into $\mathbf{TP}^T + \mathbf{E}$ where $\mathbf{T}$ are the scores and $\mathbf{P}$ are the loadings, \gls{pcr} uses $\mathbf{T}$ to predict the target $\mathbf{y}$ as $\mathbf{y} = \mathbf{Tb} + \mathbf{e}$.

% Explain PLS mathematically (similar to that one book, but not as detailed)
% Explain why it may perform well on our data

\subsubsection{GBR} % maybe put these in the background section instead for the five models, however that might come out of the blue??
% Explain GBR mathematically (similar to that one book, but not as detailed)
% Explain why it may perform well on our data

\subsubsection{SVR}
% Explain SVR mathematically (similar to that one book, but not as detailed)
% Explain why it may perform well on our data

\subsubsection{XGBoost}
% Explain XGBoost mathematically (similar to that one book, but not as detailed)
% Explain why it may perform well on our data

\subsubsection{ETR}
% Explain ETR mathematically (similar to that one book, but not as detailed)
% Explain why it may perform well on our data

\subsubsection{Results}
% Present the results of the experiments and discuss them
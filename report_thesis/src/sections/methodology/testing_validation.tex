\subsection{Validation and Testing Procedures}
This section describes the validation and testing procedures that each of our experiments adhere to.
Selecting appropriate testing procedures is crucial for ensuring the validity and reliability of the results.
For that reason, we delineate a methodological approach that ensures our models are accurate and generalizable.

We have chosen to test and evaluate all our experiments using both cross-validation and a separate test set.
Evaluating results solely on the test set could lead to models that are overly specialized to the test set.
This occurs when one searches for the optimal configuration of hyperparameters specifically tailored to the test set.
However, our objective is to develop models that demonstrate high accuracy and robustness, even on entirely unseen data.
To achieve this, we employ k-fold cross-validation to ensure our models have high generalizability, thereby increasing the likelihood that they will perform as expected on new data.

We use an $80\%/20\%$ split for training and testing sets. The training set is further subdivided into $k$ folds, which are used for cross validation.

While we employ conventional techniques like holdout sets and k-fold cross validation, \gls{libs} imposes additional challenges to the process.

One of the primary challenges is preventing data leakage.
As per concentration matrix $\mathbf{C}$ in Section~\ref{sec:problem_definition}, each target only has one ground truth concentration value per oxide.
However, each target is shot at multiple locations, resulting in multiple instances of the same target in the dataset, as shown in Table~\ref{tab:final_dataset_example}.
Although the intensity values vary for each location, they fundamentally represent different measurements of the same target.
If we were to randomly split the dataset, some locations from a target could end up in the testing set while others remain in the training set.
This would cause data leakage, as the testing set would no longer consist solely of unseen targets.
To prevent this, we ensure that each target is represented only once in the dataset by grouping data from all locations on a given target.

\subsubsection{Dataset Partitioning}
To ensure rigorous evaluation of our models and to prevent data leakage, we have implemented a customized k-fold cross-validation procedure.
This procedure ensures that all data points from a given target are either entirely in the training set or the test set.
The following algorithm outlines the steps taken to partition the dataset effectively while addressing the specific challenges posed by our data.

\begin{algorithm}
\caption{Custom k-Fold Cross-Validation with Extreme Value Handling}
\begin{algorithmic}[1]
\Require Dataset \( \mathbf{D} \), group column \( g \), target column \( t \), number of splits \( k \), percentile \( p \), random seed \( \text{seed} \)
\Ensure Cross-validation folds \( \mathbf{F}_\text{cv} \), training set \( \mathbf{D}_\text{train} \), and test set \( \mathbf{D}_\text{test} \)
\State \label{line:seed} Set random seed for reproducibility if \(\text{seed} \) is not None
\State \label{line:remove_duplicates} Remove duplicate entries based on \( g \) and sort by \( t \)
\State \label{line:assign_folds} Assign fold numbers sequentially from 0 to \( k-1 \) to unique targets
\If{extreme values handling is enabled}
    \State \label{line:identify_extremes} Identify extreme values at percentiles \( p \) and \( 1-p \)
    \State \label{line:reassign_extremes} Reassign extreme values to folds \( 0 \) to \( k-2 \)
\EndIf
\State \label{line:merge_folds} Merge fold assignments back into the original dataset
\State \label{line:split_dataset} Split dataset into test set \( \mathbf{D}_\text{test} \) (fold \( k-1 \)) and remaining data \( \mathbf{D}_\text{train} \)
\State \label{line:create_folds} Create \( k-1 \) training and validation folds
\For{each fold \( i \) from 0 to \( k-2 \)}
    \State \( \mathbf{F}_\text{train}[i] \gets \mathbf{D}_\text{train} \setminus \text{fold}_i \)
    \State \( \mathbf{F}_\text{val}[i] \gets \text{fold}_i \)
    \State Append \((\mathbf{F}_\text{train}[i], \mathbf{F}_\text{val}[i])\) to \(\mathbf{F}_\text{cv}\)
\EndFor
\State \label{line:remove_fold_column} Remove fold column from all datasets
\State \Return \( \mathbf{F}_\text{cv}, \mathbf{D}_\text{train}, \mathbf{D}_\text{test} \)
\end{algorithmic}
\end{algorithm}

The procedure begins by setting a random seed for reproducibility if one is provided (Line~\ref{line:seed}).
This ensures that the results are consistent across different runs of the algorithm.
Next, the dataset is processed to remove any duplicate entries based on the group column and then sorted by the target column (Line~\ref{line:remove_duplicates}).
This step ensures that each group is uniquely identified and ordered appropriately.
The dataset we illustrate in Table~\ref{tab:final_dataset_example} would require a group column $g$ of "\texttt{Target}" to group the data by target. The target column $t$ refers to the regression target, which would be the oxide for which we are predicting the concentration, e.g., \ce{SiO_2}.

Fold numbers are then assigned sequentially to the unique targets (Line~\ref{line:assign_folds}).
This initial assignment distributes the targets randomly across the folds.
If handling of extreme values is enabled, the algorithm identifies the top and bottom percentiles of the target values (Line~\ref{line:identify_extremes}) and reassigns these extreme values to the training folds (0 to \( k-2 \)) to ensure they are well-represented during model training (Line~\ref{line:reassign_extremes}).
For our purposes, it is always enabled.
Moreover, we determined that utilizing a percentile value of $p=5\%$ strikes an optimal balance between maintaining the representativeness of the test dataset and ensuring that all highly influential targets are included in the training dataset.

The fold assignments are then merged back into the original dataset (Line~\ref{line:merge_folds}), and the dataset is split into a test set (consisting of the highest fold number) and the remaining training data (Line~\ref{line:split_dataset}).
The training data is further divided into \( k-1 \) folds for cross-validation.
For each fold, the training data consists of all but one fold, which serves as the validation set.
These pairs of training and validation sets are then appended to the list of cross-validation folds \(\mathbf{F}_\text{cv}\) (Line~\ref{line:create_folds}).

Finally, the fold indicator column is removed from all datasets before returning the final partitions (Line~\ref{line:remove_fold_column}).
This cleanup step ensures that the fold information does not interfere with subsequent data processing or model training.

The final output of this procedure consists of:
\begin{itemize}
    \item The cross-validation folds \(\mathbf{F}_\text{cv}\), each containing a tuple of training and validation sets.
    \item The training set \(\mathbf{D}_\text{train}\) in its entirety.
    \item The test set \(\mathbf{D}_\text{test}\), distinct from the training set.
\end{itemize}

By following this detailed procedure, we ensure that our cross-validation is robust against data leakage, maintains the integrity of grouped targets, and carefully handles extreme values to improve the representativeness of the training set.


\subsubsection{Evaluation Metrics}
As mentioned in Section~\ref{sec:problem_definition}, the performance of the models was quantitatively assessed using the \gls{rmse} and the standard deviation of the residuals.
These metrics are calculated for each fold and averaged across all folds to provide comprehensive indicators of model accuracy and variability.
In addition, we also compute the metrics for the test set to provide a measure of the model's performance on unseen data.
Therefore, we have the following metrics for each experiment:
\begin{enumerate}
    \item \textbf{Fold-specific RMSE and Standard Deviation:} For each of the $k$ folds, we calculate both the RMSE and standard deviation, denoted as \texttt{rmse\_cv\_n} and \texttt{std\_dev\_cv\_n}, where \texttt{n} ranges from 1 to $k$.
    \item \textbf{Average RMSE and Standard Deviation:} The overall cross-validation RMSE (\texttt{rmse\_cv}) and standard deviation (\texttt{std\_dev\_cv}) are computed as the mean of the fold-specific values.
    \item \textbf{Test Set RMSE and Standard Deviation:} The RMSE and standard deviation are also computed for the test set, denoted as \texttt{rmsep} and \texttt{std\_dev}, to provide a measure of the model's performance on unseen data.
\end{enumerate}

\subsubsection{Conclusion}
The implementation of a tailored validation and testing framework in this study ensures that the models developed are both accurate and generalizable.
By integrating custom k-fold cross-validation and carefully selecting performance metrics, the methodology effectively addresses potential issues of data leakage and overfitting.
These measures reinforce the reliability of the model evaluations and support the overarching goal of enhancing the accuracy and robustness of chemical composition analysis using \gls{libs} data.

% - Is it excessive to use both holdout and cross validation?
% - What are we missing here?
% - What is self-evident to us, but not to the reader?


\section{Problem Definition}\label{sec:problem_definition}
The objective of this research is to predict major oxide compositions from \gls{libs} data.
We aim to enhance the accuracy and robustness of these predictions by developing and validating a computational methodology that addresses the challenges of such quantification of elements in soil samples from \gls{libs} data. 
This objective presents several significant challenges, including the high dimensionality of spectral data, multicollinearity, matrix effects, and limited data availability.

A fundamental premise of this research posits that by effectively addressing these challenges, the accuracy and robustness of predicting elemental concentrations from \gls{libs} data can be significantly enhanced. This assumption is supported by several key studies in the field.
For instance, \citet{andersonPostlandingMajorElement2022} demonstrated that preprocessing, normalization, and the use of advanced machine learning models significantly improved the prediction accuracy of major oxides from \gls{libs} data collected by the SuperCam instrument on the Mars 2020 Perseverance rover. Their work highlights the importance of selecting appropriate models and preprocessing techniques to handle high-dimensional spectral data effectively.
Similarly, \citet{song_DF-K-ELM} showed that incorporating domain knowledge into machine learning models enhances both the interpretability and performance of \gls{libs} quantification. By addressing challenges such as high dimensionality and multicollinearity, their approach improved the accuracy and generalizability of the models across different tasks.
The effectiveness of dimensionality reduction techniques in improving model performance was highlighted by \citet{rezaei_dimensionality_reduction}, who demonstrated that methods like \gls{pca} can manage noise and computational inefficiency in high-dimensional \gls{libs} data. This supports the notion that reducing data dimensionality can lead to more stable and accurate predictions.
Furthermore, \citet{jeonEffectsFeatureEngineering2024} emphasized the importance of feature engineering in enhancing model robustness, particularly under varying measurement conditions. This is crucial for extraterrestrial applications where consistent and reliable predictions are necessary despite the challenges posed by the environment.
Lastly, \citet{sunMachineLearningTransfer2021} demonstrated the efficacy of transfer learning in overcoming matrix effects and improving model robustness for rock classification on Mars. Their findings suggest that similar improvements can be achieved in oxide quantification by leveraging knowledge from related domains.
Studies such as these provide a strong foundation for our assumption that addressing the identified challenges will lead to significant improvements in the accuracy and robustness of predicting elemental concentrations from \gls{libs} data.

\subsection{Quantification based on \gls{libs} data}
\gls{libs} spectral data provides intensity readings across a spectrum of wavelengths in the form of Clean, Calibrated Spectra \cite{andersonImprovedAccuracyQuantitative2017}, as described by \citet{wiensPreflightCalibrationInitial2013}.
The wavelength intensities are quantified in units of photon/shot/mm\textsuperscript{2}/sr/nm.

The formal definition of the problem is as follows.
In a \gls{libs} dataset, we have:
\newcounter{listitem}
\begin{itemize}[topsep=0pt]
    \refstepcounter{listitem}
    \item \textbf{Concentration Tensor} $\;\mathbf{C}[\chi, o]$: This matrix denotes the chemical concentrations in weight percent for oxides. Each row represents a sample $\chi$, and each column represents an oxide $o$.
    \label{matrix:concentration}

    \refstepcounter{listitem}
    \item \textbf{Intensity Tensor} $\;\mathbf{I}[\chi, l, s, \lambda]$: Holds the spectral intensity data, where each entry represents the intensity recorded for a sample $\chi$ at location $l$, for shot $s$, at wavelength $\lambda$. $l$ indicates the location on the sample where the measurement is taken, and $\lambda$ is the index for wavelengths (specific wavelengths of light measured by the spectrometers).
    \label{matrix:intensity}

    \refstepcounter{listitem}
    \item \textbf{Averaged Intensity Tensor} $\;\mathbf{A}[\chi, l, \lambda]$: Derived from matrix $\mathbf{I}$ by averaging the intensities across shots to provide a clearer signal for each location and wavelength:
    \[
    A[\chi, l, \lambda] = \frac{1}{|S|} \sum_{s \in S} I[\chi, l, s, \lambda].
    \]
    \label{matrix:averaged_intensity}
\end{itemize}

The primary input to our computational models is the processed \gls{libs} spectral data.
Formally, we have:
\begin{itemize}
    \item \textbf{Masked Intensity Tensor} $\mathbf{M}[\chi, l, \lambda]$: This matrix represents the spectral intensity data after applying wavelength-specific masks to the Averaged Intensity Tensor $\mathbf{A}$. It serves as the main input to the models.
    \item \textbf{Feature Vectors} $\mathbf{x} \in \mathbb{R}^N$: These vectors are extracted from the Masked Intensity Tensor $\mathbf{M}$ and represent the processed \gls{libs} signals. Each feature vector corresponds to a sample and contains $N$ dimensions, where $N$ is the number of relevant spectral features.
\end{itemize}

The outputs of the computational models are the predicted concentrations of major oxides in the samples.
These outputs are represented as vectors of estimated oxide concentrations:

\begin{itemize}
    \item \textbf{Estimated Concentration Vectors} $\mathbf{v} \in \mathbb{R}^{n_o}$: Each vector $\mathbf{v}$ contains the predicted concentrations for $n_o$ target oxides. These predictions are derived from the mapping function $\mathcal{F}$ applied to the feature vectors $\mathbf{x}$.
\end{itemize}

The task of \gls{libs}-based quantification involves fitting the parameters of a mapping function $\mathcal{F}: \mathbb{R}^N \rightarrow \mathbb{R}^{n_o}$ to accurately predict oxide concentrations from processed \gls{libs} signals by optimizing these parameters to minimize a loss function.
To evaluate the performance of these models, we will use the \gls{rmse} to measure accuracy and the sample standard deviation of prediction errors to assess robustness.
We define accuracy as the ability of a model to predict the composition of major oxides in geological samples, while robustness refers to the stability of these predictions across samples.

The metric used to evaluate the accuracy of the models is the \gls{rmse}:
\[
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (\mathbf{v}_i - \hat{\mathbf{v}}_i)^2}
\]
where \( \mathbf{v}_i \) is the vector of actual oxide concentrations for the \( i \)-th sample, \( \hat{\mathbf{v}}_i \) is the corresponding vector of predicted oxide concentrations, and \( n \) is the total number of samples. 
This measure quantifies the average magnitude of the prediction error across all predicted values.

Robustness is evaluated using the sample standard deviation of prediction errors:
\[
\sigma_{error} = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (e_i - \bar{e})^2}
\]
where \( e_i = \mathbf{v}_i - \hat{\mathbf{v}}_i \) and \( \bar{e} \) is the mean error.
A lower standard deviation indicates a more robust model across different samples.

\subsection{Challenges}
As mentioned, quantifying chemical compositions from \gls{libs} spectral data involves several significant challenges that must be addressed to ensure accurate and robust predictions.

\subsubsection{Data Dimensionality}
The large number of dimensions, as seen by having many wavelengths $lambda$ in the Intensity Tensor $\mathbf{I}[\chi, l, s, \lambda]$, can lead to challenges such as the inclusion of irrelevant or redundant features.

High-dimensional datasets, like \gls{libs} datasets, may include irrelevant or redundant features that obscure the true signal, complicating the process of accurately estimating the target variables. Effective dimensionality reduction techniques can help ensure the reliability of predictions.

\subsubsection{Multicollinearity}
The overlapping nature of emission lines from different elements results in high correlation between intensity readings at different wavelengths, making it difficult to extract independent spectral features necessary for accurate quantitative analysis \cite{andersonImprovedAccuracyQuantitative2017}.

\subsubsection{Matrix Effects}
Matrix effects refer to variations in the intensity of emission lines of an element independent of its concentration, arising from the complex interplay of various physical processes within the plasma generated by the \gls{libs} technique. These effects can significantly alter emission intensities, complicating the extraction of accurate and independent spectral features. This makes it challenging to precisely map the processed \gls{libs} signal vector $\mathbf{x} \in \mathbb{R}^N$ to a vector $\mathbf{v} \in \mathbb{R}^{n_o}$ of estimated oxide concentrations \cite{cleggRecalibrationMarsScience2017, andersonImprovedAccuracyQuantitative2017}.
Matrix effects, along with other physical processes, can induce nonlinearity in the mapping function, thereby increasing the complexity of the task\cite{liuRecentAdvancesMachine2024}.

\subsubsection{Data Availability}
Due to the high cost of data collection, datasets are often small. This limits the number of samples available for evaluation, affecting the generalizability and robustness of the models\cite{p9_paper}.

\subsection{Problem Formulation}
The objective of this research is to develop a computational model, denoted as $\mathcal{F}: \mathbb{R}^N \rightarrow \mathbb{R}^{n_o}$, to predict major oxide concentrations in geological samples from processed \gls{libs} spectral data. 

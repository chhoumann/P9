\section{Problem Definition}\label{sec:problem_definition}
The objective of this research is to develop and validate a computational methodology to enhance the accuracy and robustness of predicting major oxide compositions from \gls{libs} data.
Such quantification of elements in soil samples from \gls{libs} data presents several significant challenges, including the high dimensionality of spectral data, multicollinearity, matrix effects, and limited data availability.

A fundamental premise of this research posits that by effectively addressing these challenges, the accuracy and robustness of predicting elemental concentrations from \gls{libs} data can be significantly enhanced. This assumption is supported by several key studies in the field.
For instance, \citet{andersonPostlandingMajorElement2022} demonstrated that preprocessing, normalization, and the use of advanced machine learning models significantly improved the prediction accuracy of major oxides from \gls{libs} data collected by the SuperCam instrument on the Mars 2020 Perseverance rover. Their work highlights the importance of selecting appropriate models and preprocessing techniques to handle high-dimensional spectral data effectively.
Similarly, \citet{song_DF-K-ELM} showed that incorporating domain knowledge into machine learning models enhances both the interpretability and performance of \gls{libs} quantification. By addressing challenges such as high dimensionality and multicollinearity, their approach improved the accuracy and generalizability of the models across different tasks.
The effectiveness of dimensionality reduction techniques in improving model performance was highlighted by \citet{rezaei_dimensionality_reduction}, who demonstrated that methods like \gls{pca} can manage noise and computational inefficiency in high-dimensional \gls{libs} data. This supports the notion that reducing data dimensionality can lead to more stable and accurate predictions.
Furthermore, \citet{jeonEffectsFeatureEngineering2024} emphasized the importance of feature engineering in enhancing model robustness, particularly under varying measurement conditions. This is crucial for extraterrestrial applications where consistent and reliable predictions are necessary despite the challenges posed by the environment.
Lastly, \citet{sunMachineLearningTransfer2021} demonstrated the efficacy of transfer learning in overcoming matrix effects and improving model robustness for rock classification on Mars. Their findings suggest that similar improvements can be achieved in oxide quantification by leveraging knowledge from related domains.
Studies such as these provide a strong foundation for our assumption that addressing the identified challenges will lead to significant improvements in the accuracy and robustness of predicting elemental concentrations from \gls{libs} data.

\subsection{Quantification based on \gls{libs} data}
\gls{libs} spectral data provides intensity readings across a spectrum of wavelengths in the form of Clean, Calibrated Spectra \cite{andersonImprovedAccuracyQuantitative2017}, as described by \citet{wiensPreflightCalibrationInitial2013}.
The wavelength intensities are quantified in units of photon/shot/mm\textsuperscript{2}/sr/nm.

The formal definition of the problem is as follows.
In a \gls{libs} dataset, we have:
\newcounter{listitem}
\begin{itemize}[topsep=0pt]
    \refstepcounter{listitem}
    \item \textbf{Concentration Matrix} $\;\mathbf{C}[\chi, o]$: This matrix denotes the chemical concentrations in weight percent for oxides indexed by $o$ across samples indexed by $\chi$. Here, $\chi$ represents the index for samples and $o$ denotes the index for oxides (different chemical compounds being quantified).
    \label{matrix:concentration}

    \refstepcounter{listitem}
    \item \textbf{Intensity Matrix} $\;\mathbf{I}[\chi, l, s, \lambda]$: Holds the spectral intensity data, where each entry represents the intensity recorded for a sample $\chi$ at location $l$, for shot $s$, at wavelength $\lambda$. $l$ indicates the location on the sample where the measurement is taken, and $\lambda$ is the index for wavelengths (specific wavelengths of light measured by the spectrometers).
    \label{matrix:intensity}

    \refstepcounter{listitem}
    \item \textbf{Averaged Intensity Matrix} $\;\mathbf{A}[\chi, l, \lambda]$: Derived from matrix $\mathbf{I}$ by averaging the intensities across shots to provide a clearer signal for each location and wavelength:
    \[
    A[\chi, l, \lambda] = \frac{1}{|S|} \sum_{s \in S} I[\chi, l, s, \lambda].
    \]
    \label{matrix:averaged_intensity}
\end{itemize}

The primary input to our computational models is the processed \gls{libs} spectral data.
Formally, we have:
\begin{itemize}
    \item \textbf{Masked Intensity Matrix} $\mathbf{M}[\chi, l, \lambda]$: This matrix represents the spectral intensity data after applying wavelength-specific masks to the Averaged Intensity Matrix $\mathbf{A}$. It serves as the main input to the models.
    \item \textbf{Feature Vectors} $\mathbf{x} \in \mathbb{R}^N$: These vectors are extracted from the Masked Intensity Matrix $\mathbf{M}$ and represent the processed \gls{libs} signals. Each feature vector corresponds to a sample and contains $N$ dimensions, where $N$ is the number of relevant spectral features.
\end{itemize}

The outputs of the computational models are the predicted concentrations of major oxides in the samples.
These outputs are represented as vectors of estimated oxide concentrations:

\begin{itemize}
    \item \textbf{Estimated Concentration Vectors} $\mathbf{v} \in \mathbb{R}^{n_o}$: Each vector $\mathbf{v}$ contains the predicted concentrations for $n_o$ target oxides. These predictions are derived from the mapping function $\mathcal{F}$ applied to the feature vectors $\mathbf{x}$.
\end{itemize}

The task of \gls{libs}-based quantification involves training the mapping function $\mathcal{F}: \mathbb{R}^N \rightarrow \mathbb{R}^{n_o}$ to accurately predict the oxide concentrations from the processed \gls{libs} signals.
To evaluate the performance of these models, we will use the \gls{rmse} to measure accuracy and the sample standard deviation of prediction errors to assess robustness.
We define accuracy as the ability of a model to predict the composition of major oxides in geological samples, while robustness refers to the stability of these predictions across samples.

The metric used to evaluate the accuracy of the models is the \gls{rmse}:
\[
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (\mathbf{v}_i - \hat{\mathbf{v}}_i)^2}
\]
where \( \mathbf{v}_i \) is the vector of actual oxide concentrations for the \( i \)-th sample, \( \hat{\mathbf{v}}_i \) is the corresponding vector of predicted oxide concentrations, and \( n \) is the total number of samples. This measure quantifies the average magnitude of the prediction error across all predicted values.

Robustness is evaluated using the sample standard deviation of prediction errors:
\[
\sigma_{error} = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (e_i - \bar{e})^2}
\]
where \( e_i = \mathbf{v}_i - \hat{\mathbf{v}}_i \) and \( \bar{e} \) is the mean error.
A lower standard deviation indicates a more robust model across different samples.

\textbf{Problem formulation}:
The objective of this research is to develop a computational model, denoted as $\mathcal{F}: \mathbb{R}^N \rightarrow \mathbb{R}^{n_o}$, to predict major oxide concentrations in geological samples from processed \gls{libs} spectral data. The model will use feature vectors $\mathbf{x} \in \mathbb{R}^N$ derived from the Masked Intensity Matrix $\mathbf{M}[\chi, l, \lambda]$ and output Estimated Concentration Vectors $\mathbf{v} \in \mathbb{R}^{n_o}$. Effectiveness will be measured by high accuracy, quantified by \gls{rmse}, and robustness, indicated by a low standard deviation of prediction errors across samples.


\subsection{Challenges}
As mentioned, quantifying chemical compositions from \gls{libs} spectral data involves several significant challenges that must be addressed to ensure accurate and robust predictions.
Several of the following challenges can be addressed through known methodologies, such as sample preprocessing, ensuring the repeatability of observations, and selecting calibration samples that are matrix-matched to specific targets of interest\cite{andersonPostlandingMajorElement2022}.
However, these strategies are not feasible for the extraterrestrial applications relevant to this research.

\subsubsection{Data Dimensionality}
The large number of dimensions, as seen by having many wavelengths $\lambda$ in the Intensity Matrix $\mathbf{I}[\chi, l, s, \lambda]$, can lead to challenges such as noise management, computational inefficiency, and instability in model predictions.
\begin{itemize}
    \item \textbf{Noise Management:} While noise itself is a consequence of instrumental measurements\cite{wiensPreflightCalibrationInitial2013} and not directly caused by high dimensionality, having many dimensions can exacerbate the impact of noise. High-dimensional datasets, like \gls{libs} datasets, may include irrelevant or redundant features that obscure the true signal, complicating the process of accurately estimating the target variables. Therefore, effective noise management techniques can help ensure the reliability of predictions.
    
    \item \textbf{Computational Inefficiency:} Processing high-dimensional data requires substantial computational resources, which can be a bottleneck in model training and prediction. Efficient algorithms and dimensionality reduction techniques can be used to handle this complexity and improve computational efficiency. However, this problem is slightly alleviated due to the limited data availability.

    \item \textbf{Instability in Model Predictions:} High-dimensional data can lead to models that are sensitive to slight changes in the input data, resulting in significantly different outputs. This instability reduces the robustness and reliability of the predictions, making it challenging to achieve consistent and accurate results.
\end{itemize}

\subsubsection{Multicollinearity}
The overlapping nature of emission lines from different elements results in high correlation between intensity readings at different wavelengths, making it difficult to extract independent spectral features necessary for accurate quantitative analysis \cite{andersonImprovedAccuracyQuantitative2017}.

\subsubsection{Matrix Effects}
Matrix effects refer to variations in the intensity of emission lines of an element independent of its concentration, arising from the complex interplay of various physical processes within the plasma generated by the \gls{libs} technique. These effects can significantly alter emission intensities, complicating the extraction of accurate and independent spectral features. This makes it challenging to precisely map the processed \gls{libs} signal vector $\mathbf{x} \in \mathbb{R}^N$ to a vector $\mathbf{v} \in \mathbb{R}^{n_o}$ of estimated oxide concentrations \cite{cleggRecalibrationMarsScience2017, andersonImprovedAccuracyQuantitative2017}.
Matrix effects, along with other physical processes, can induce nonlinearity in the mapping function, thereby increasing the complexity of the task\cite{liuRecentAdvancesMachine2024}.

\subsubsection{Data Availability}
Due to the high cost of data collection, datasets are often small. This limits the number of samples available for evaluation, affecting the generalizability and robustness of the models\cite{p9_paper}.

\subsection{Proposed Approach}
To address the challenges in predicting major oxide compositions from \gls{libs} data, we propose the development of advanced computational models capable of effectively handling the multifaceted challenges arising from the high dimensionality of spectral data, multicollinearity, matrix effects, and limited data availability.
These issues complicate the accurate and robust prediction of elemental concentrations, necessitating advanced computational methodologies. 

Our proposed approach aims to address these challenges by leveraging state-of-the-art machine learning models, dimensionality reduction techniques, and feature engineering.
By preprocessing the spectral data, we can enhance the signal quality and handle noise.
The development of robust models that can generalize well across different samples and conditions is crucial, particularly for extraterrestrial applications where data collection is constrained.

The effectiveness of our models will be evaluated using metrics such as \gls{rmse} for accuracy and the standard deviation of prediction errors for robustness.
These metrics will provide a comprehensive assessment of the models' performance in predicting the concentrations of major oxides in geological samples.

\subsection{Motivating Example: NASA's Mars Missions}
NASA's exploration of Mars, beginning with the Viking missions in the 1970s, has progressively deepened our understanding of Mars \cite{marsnasagov_vikings}.
The \gls{msl} mission, which landed the Curiosity rover in Gale Crater in 2012, represents a pivotal step in this journey.
Curiosity is equipped with the \gls{chemcam} instrument, a tool that uses \gls{libs} to analyze the chemical composition of Martian rocks and soils directly and non-invasively \cite{chemcamNasaWebsite}.

\gls{libs} is particularly suitable for the Martian environment because of its ability to perform rapid chemical analyses remotely, creating a plasma that can be spectrally analyzed to determine the elemental composition of the vaporized material.
This capability allows scientists to quickly and efficiently assess the geochemistry of multiple sites without physically moving the rover, thus conservatively managing the rover's limited energy and resources.
The mission's focus has been on assessing past habitability, and the data gathered by \gls{chemcam} has been instrumental in identifying environments that could have supported life \cite{chemcamNasaWebsite, curiosityNasaWebsite}.

The task of quantifying the oxides in Martian rock and soil samples begins with the \gls{libs} spectral data collected by Curiosity.
This data comprises high-dimensional spectra with thousands of potential features, each corresponding to a specific element's emission lines.
The computational challenge lies in accurately interpreting these complex data sets to deduce the concentrations of various elements, especially major oxides like iron, magnesium, and silicon, which are crucial for understanding Martian geology.

Following preprocessing to correct instrumental effects and calibrate spectra, the cleaned data is input into machine learning models.
These models, trained on databases of Earth-based and synthetic Martian analogs, output quantitative analyses of chemical compositions in weight percentages of the target oxides \cite{wiensPreflightCalibrationInitial2013, cleggRecalibrationMarsScience2017}.
\citet{cleggRecalibrationMarsScience2017} undertook this task and created a pipeline for predicting the concentration of oxides in Martian soil samples, referred to as \gls{moc}.

More recently, in 2022, the Perseverance rover landed on Mars, equipped with advanced instruments designed to continue the exploration and analysis of the Martian surface.
This rover also uses a \gls{libs} instrument, called SuperCam, which is the successor to \gls{chemcam}.
The Perseverance mission highlighted the ongoing research effort in developing elemental quantification models using \gls{libs} data \cite{andersonPostlandingMajorElement2022}, demonstrating its continued importance as a research field.

The use of \gls{libs} on the Curiosity rover within the MSL mission shows how computational advancements can enhance our understanding of extraterrestrial geology.
By effectively quantifying chemical compositions from \gls{libs} data, we can infer the historical climatic conditions of Mars, offering clues to its past habitability.

\subsection{Model and Preprocessing Selection}

Choosing the right models and preprocessing techniques for \gls{libs} data analysis is a challenging task. 
As the literature highlighted in Section~\ref{sec:related-work} suggests, a variety of models and preprocessing techniques promise to be adept at handling data that exhibit high-dimensionality, multi-collinearity, and matrix effects.
The literature also indicates that different machine learning models perform better on some oxides than others.
These challenges and model-specific strengths suggests that an optimal approach would involve combining multiple models. 
This notion is supported by the advent of models such as the \gls{moc}~\cite{cleggRecalibrationMarsScience2017} model, which combines the predictions of multiple models using a predetermined weighting for each model's predictions on a per-oxide basis.
While this approach improved accuracy compared to individual models, it required manual tuning of the weights for each model.
This manual tuning presents limitations, including the analysis required to determine appropriate weights and the risk of suboptimal weighting.
Given these limitations, it is reasonable to explore techniques that can automate the weighting process while still leveraging the strengths of multiple models.
To fulfill these criteria, we chose to adopt a stacking ensemble approach. 
Stacking, as described in Section~\ref{subsec:stacked-generalization}, is a method that utilizes multiple base estimators trained on the same data, whose predictions are then used to train a meta-learner.
By combining a diverse set of base models, stacking can correct for the biases of individual models.
Since each model focuses on different patterns within the data, stacking mitigates the inherent biases of individual models by estimating and correcting for these biases.
This approach of leveraging the strength of multiple models that each model the problem differently can lead to better generalization on unseen data by automating and potentially improving upon manual tuning through the use of a meta-learner to discern patterns in the base predictors' outputs. \cite{wolpertstacked_1992, survey_of_ensemble_learning}
However, some consideration has be made towards training of the base models in order to prevent data leakage and overfitting.
As emphasized by \citet{cvstacking}, if the base models are trained on the same dataset, the meta learner might favor certain base models over others.
This can cause the meta learner to be influenced by the same patterns and biases that the base models are susceptible to, leading to overfitting.
To mitigate this risk and ensure generalizability, a cross-validation strategy should be employed to ensure that the meta learner's training data accurately reflects the true performance of the base learners.

We adopted an experimental approach to empirically evaluate the potential of various models and preprocessing techniques, to be used in our stacking ensemble, ensuring that our selections were informed by our literature review while also allowing for independent assessment and validation.

We had several considerations to guide our selection of preprocessing techniques.
Firstly, our review of the literature revealed that there seems to be no consensus on a single, most effective normalization method for \gls{libs} data.
This led us to include the traditional normalization methods, such as z-score normalization, Min-Max scaling, and Max Absolute scaling, in our experiments.
This approach allowed us to determine which normalization method was most effective for our dataset. 
Additionally, dimensionality reduction techniques are considered by the literature to be effective techniques for \gls{libs} data due to its high dimensionality. Specifically, \gls{pca} has been widely adopted by the spectroscopic community as an established dimensionality reduction technique~\cite{pca_review_paper}. However, \citet{pca_review_paper} makes the case that the assumptions for \gls{pca} regarding linearity of the data are only met up to a certain point, after which it breaks. They argue that this non-linearity inherent in the data makes \gls{kernel-pca} a valid candidate for \gls{libs} data. Based on their review of the field, and our own review of the literature, not many have studied the effectiveness of \gls{kernel-pca} in the context of \gls{libs} data. Therefore, we decided to include this in our experiments to further assess its potential. In addition to the non-linearity, \citet{pca_review_paper} also argue that the assumptions of normality in the data are not always met in \gls{libs} data. For this reason, we decided to include power transformation and quantile transformation in our experiments as models such as \gls{pca} benefit from a normal distribution of the data. We assume that models such as \gls{pls} may also benefit from a more Gaussian-like distribution of the data, by virtue of the model being partly based on \gls{pca}.

While these preprocessing techniques are not an exhaustive list, they represent a diverse set of methods.
Techniques such as feature selection were not considered, in order to limit the scope of the study and due to time constraints.

We also had several requirements for the selection of models.
The models selected for experimentation had to be diverse to ensure that we had enough breadth in our results to make informed decisions about which models should be included in the final stacking ensemble pipeline.
Additionally, the models had to be suitable for regression tasks and if no research was available in the context of \gls{libs} data, they had to be models that have shown promise in other domains.
Our literature review found that a variety of models fit this criteria.
For example, \citet{andersonPostlandingMajorElement2022} demonstrated that that models such as \gls{gbr}, \gls{pls}, \gls{lasso}, \gls{rf} were each effective at predicting different major oxides.
Additionaly, \citet{svrforlibs} demonstrated in their work that \gls{svr} outperforms \gls{plsr} in predicting \ce{Si}, \ce{Ca}, \ce{Mg}, \ce{Fe} and \ce{Al}.
As a result, we have included \gls{gbr}, \gls{pls}, \gls{lasso}, \gls{rf}, and \gls{svr} in our experiments.

In the neural network domain, \cite{ann_libs_soil_analysis} showed that on \gls{libs} data their 3-layer \gls{ann} relative error of prediction for \ce{Ca}, \ce{Fe}, \ce{Al} was below 20\%.
Likewise, \citet{yangConvolutionalNeuralNetwork2022} showed that \gls{cnn} outperformed methods such as \gls{logreg}, \gls{svm} and linear discriminant analysis at correctly classifying twelve different types of rocks based on \gls{libs} data. 
While this example for \gls{cnn} is a classification task, \gls{cnn} can be adjusted for regression tasks by changing the loss function and output layer.
Based on these factors, we decided to include \gls{ann} and \gls{cnn} in our experiments to further increase the diversity of our model selection.

To further bolster our selection pool, we chose to include models that were in the same family of models as those that showed promise in the literature.

\gls{xgboost} was included as an option based on its promising accuracy in various settings.
For example \citet{xgboost_in_biomedicie} showed that \gls{xgboost} outperformed models such as \gls{rf} and \gls{svm} in predicting biological activity based on quantitative description of the compound's molecular structure. 
Another example is \citet{xgboost_in_heart_disease} that used \gls{xgboost} to predict heart disease, where \gls{xgboost} outperformed \gls{rf} and \gls{etr} at correctly classifying patients with heart disease.
Due to these factors and the limited study of \gls{xgboost} in the context of \gls{libs} data, we decided to include it in our experiments.

Following the same logic, we included \gls{ngboost}. 
\gls{ngboost} is a recent model introduced by \citet{duan_ngboost_2020} that according to their work improves upon \gls{gbr} by using a more sophisticated loss function and a more advanced gradient boosting algorithm.
A very limited amount of research has been conducted using this algorithm in the context of \gls{libs} data, however, \citet{ngboost_landslide} showed that \gls{ngboost} outperformed \gls{xgboost} and \gls{rf} and correctly determining landslide-prone fields with an AUC of 0.898 compared to an AUC of 0.871 and 0.863 of \gls{xgboost} and \gls{rf} respectively.

Finally, \gls{ridge}, \gls{enet}, \gls{etr} and \gls{lasso} were used in various studies and showed promising results, despite not necessarily being best performing in their respective studies.
Therefore, we chose to include these in our experiments to further diversify our model selection. 

Table~\ref{tab:preprocessing-models} summarizes the preprocessing techniques and models selected for our experimentation.

\begin{table}[ht]
\centering
\begin{tabularx}{\columnwidth}{>{\raggedright\arraybackslash}X}
\toprule
\textbf{Normalization / Scaling:} \\
\midrule
Z-Score Normalization \\
Min-Max Normalization \\
Max Absolute Scaling \\
Robust Scaling \\
Norm 3 \\
\midrule
\textbf{Transformation Methods:} \\
\midrule
Power Transformation \\
Quantile Transformation \\
\midrule
\textbf{Dimensionality Reduction Methods:} \\
\midrule
PCA \\
Kernel PCA \\
\midrule
\textbf{Model Types:} \\
\midrule
\textbf{Regression Models:} \\
\quad Partial Least Squares \\
\quad Support Vector Regression \\
\quad Elastic Nets \\
\quad Least Absolute Shrinkage and Selection Operator \\
\quad Ridge Regression \\
\textbf{Ensemble Models:} \\
\quad Random Forest \\
\quad Gradient Boost Regression \\
\quad Extra Trees Regression \\
\quad XGBoost \\
\quad Natural Gradient Boosting \\
\textbf{Neural Networks:} \\
\quad Artificial Neural Networks \\
\quad Convolutional Neural Networks \\\bottomrule
\end{tabularx}
\caption{Overview of Preprocessing Techniques and Models}
\label{tab:preprocessing-models}
\end{table}
\subsection{Optimization Framework}\label{sec:optimization-framework}
One of the primary challenges in developing a stacking ensemble is determining the optimal choice of base estimators.
\citet{wolpert1992stacked} highlighted that this can be considered a 'black art' and that the choice usually relies on intelligent guesses.
In our case, this problem is further exacerbated by the fact that the optimal choice of base estimator may vary depending on the target oxide and the fact that we are also considering the optimal preprocessing techniques for each base estimator.
This means, that we are considering an entire pipeline of preprocessing techniques and base estimators for each target oxide.
Therefore, we need a systematic approach to determine the optimal configuration for each pipeline.
To guide this process we have developed a working assumption.
Namely, that we assume that selecting the top-$n$ best pipelines for each oxide, given different preprocessors and models per pipeline, will yield the best pipelines for a given oxide in our stacking ensemble.
Here, $n$ is a heuristic based on the results and \textit{best} is evaluated in terms of the metrics outlined in Section~\ref{sec:evaluation_metrics}.
This reduces the problem to finding a selection of preprocessors and models, and a method of testing multiple permutations of these to find the top-$n$ best pipeline for each oxide.
In Section~\ref{sec:model-selection} we outlined the models and preprocessing techniques that we intend to use and in the following section we will describe the optimization framework that we have developed to address this challenge.
% To address this challenge we have developed a working assumption, namely; that the top-n best performing pipelines for each oxide, given different preprocessors and models per pipeline, are the best pipelines for a given oxide in our stacking ensemble.
% How to determine the optimal configuration for predictor (model & preprocessors) that we can use in our stacking pipeline?
% Assumption: finding the top-n best predictors, given different models per predictor, is assumed to be the best configuration per-oxide for our stacking pipeline.
% Maybe call back to original stacking paper (Wolpert) regarding there not being a tried-and-true method for determining the best models for a stacking approach
% What about doing a search that checks as many configurations as feasible for their performance?! ← Our approach
% But you'd still need an initial list of models and preprocessors to consider (← lit review + initial experiments – model selection callback)


















Optuna is a hyperparameter optimization framework designed for direct integration with Python. Its dynamic embedding capability allows hyperparameters to be defined and adjusted within the code during execution, providing additional flexibility and ease of debugging. 
Optuna uses advanced sampling strategies to explore promising areas of the search space and employs pruning techniques to terminate unpromising trials early, optimizing computational resource use. 
It also supports scaling across multiple machines by distributing the optimization process, allowing for concurrent optimization. 
Additionally, tools are provided to identify and focus on the most impactful parameters, aiding in the efficient handling of high-dimensional search spaces. \cite{optuna_2019} 
Because an essential aspect of our study is to identify the optimal configuration of model and preprocessing techniques for that model on a per-oxide basis, these factors make Optuna an ideal choice as the basis of our optimization framework.
Optunas flexibility meant that it was easy to customize the optimization process to solve this problem and integrate it with our existing codebase.

At the core of the framework is what is referred to as the objective function.
The objective function acts as the primary building block, where the sampling parameters are set for the preprocessing techniques and the models and is the function that is minimized. % rephrase
Sampling parameters in this context relate to the range of values or options that the optimizer can choose from.

Because we want to identify not only the best model configuration, but also the best preprocessing configuration for that model, we have combined the sampling of the model and preprocessing parameters into a single objective function.
By doing this, we can optimize the entire pipeline in a single step, rather than optimizing the model and preprocessing steps separately.
% We 
With such a setup, Optuna will attempt to optimize any permutation of scaler, transformer, dimensionality reduction, and model hyperparameters throughout the optimization process.
This allows us to fully utilize Optunas sampling and pruning strategies, minimizing wasted time and computational resources on poor configurations.

\begin{algorithm}
\caption{Hyperparameter Optimization Framework}
\label{alg:hyperparameter_optimization_framework}
\begin{algorithmic}[1]
\Require Dataset $D$, Model $m$, Number of Trials $N$, Random Seed $seed$, Sampler \texttt{sampler}, Pruner \texttt{pruner}
\Ensure Trial data, including metrics and configuration for each trial

\State \textbf{Initialize:} Set random seed for reproducibility if seed is not None \label{step:initialize}
\For{each trial $t$ from 1 to $N$} \label{step:trial_loop}
    \State $hp \gets \texttt{sample\_hyperparameters}(m, \texttt{sampler})$
    \State $m' \gets \texttt{instantiate\_model}(m, hp)$ \label{step:instantiate_model}
    \Statex
    \State $s\_params \gets \texttt{sample\_scaler\_params}(\texttt{sampler})$ \State $s \gets \texttt{instantiate\_scaler}(s\_params)$
    \State $t\_params \gets \texttt{sample\_transformer\_params}(\texttt{sampler})$
    \State $t \gets \texttt{instantiate\_transformer}(t\_params)$ \newline \hspace*{3em} \textbf{or} NONE
    \State $dr\_params \gets \texttt{sample\_dim\_reduction\_params}(\texttt{sampler})$
    \State $dr \gets \texttt{instantiate\_dim\_reduction}(dr\_params)$ \newline \hspace*{3em}\textbf{or} NONE
    \Statex     
    \State $pipeline \gets [s, t, dr]$ \textbf{or} $[s, NONE, NONE]$
    \State $T_{cv}, D_{train}, D_{test} \gets \text{apply data partitioning to } D$
    \Statex
    \State $D_{train}'$ $\gets$ \texttt{pipeline\_apply}($D_{train}$, \texttt{pipeline})
    \State $T_{cv}'$ $\gets$ \texttt{pipeline\_apply}($T_{cv}$, \texttt{pipeline})
    \Statex
    \State $CV_{metrics} \gets \texttt{cross\_validate}(m', T_{cv}')$
    \State $rmse_{cv} \gets \texttt{average}(CV_{metrics}.\texttt{rmse\_values})$
    \State $std\_dev_{cv} \gets \texttt{average}(CV_{metrics}.\texttt{std\_dev\_values})$
    \Statex
    \State $m'_{train}$ $\gets$ \texttt{train}($m'$, $D_{train}'$)
    \State $rmsep$, $\sigma_{error, test}$ $\gets$ \texttt{evaluate}($m'_{train}$, $D_{test}$)
    \Statex
    % function that stores the metrics
    \State \texttt{store\_metrics}($t$, $m'$, $pipeline$, $rmse\_cv$, \newline \hspace*{8em}$std\_dev\_cv$, $rmsep$, $\sigma_{error, test}$)
\EndFor
\State \Return \texttt{rmse\_cv}, \texttt{std\_dev\_cv}, \texttt{rmsep}, $\sigma_{error, test}$
\end{algorithmic}
\end{algorithm}% This increases the likelyhood at identifying the 

% create names for evaluation metrics
% 



% Optuna for its flexibility and efficiency in exploring the vast search space of configurations.
% Optuna allows for great flexiblity in exploring various search spaces due to its modular design.
% In addition to this modularity, Optuna uses a "define-by-run" optimization strategy.
% Rather than being confined to a fixed order and range of exploration, Optuna can dynamically adjust regions based on the results of previous trials.
% Introduction of the optimization framework
    % What is the goal of the optimization framework?
    % Why Optuna?
        % Explanation of what optuna is and why it is useful for our purposes
        % Should elaborate on this:
            % This framework facilitates automated hyperparameter optimization, allowing us to  efficiently explore a vast search space of model and preprocessing configurations.
% how did we do it?
    % Maybe talk about how we designed it in this way
    % pros cons ** maybe
    % Considerations at the very least
    % Talk about how the objective function was constructed, the order of things, considerations with this, the fact that we are trying to minimize and what metrics we are minimizing
    % 
% show diagram or pseudocode
% Explain why we did it this way
    % Why did we choose this approach?
    % What are the benefits of this approach?
    % How does this approach help us achieve our goal?

   
    
% Next, we implemented an experimental framework using the Optuna optimization library~\cite{optuna_2019}.
% This framework facilitates automated hyperparameter optimization, allowing us to efficiently explore a vast search space of model and preprocessing configurations.
% The specifics of this framework are discussed in Section~\ref{sec:optimization_framework}.


% Optuna uses python syntax instead of some propriety syntax
% Optuna brings the optimization into the space of the program rather than outside
    % Meaning you can embed it directly in functions etc.
    % Easier to debug
    % Can use python language such as looping etc.
% Instead of having parameters set you change them so they are sampled using the suggest_* methods
    % This is also helpful because it allows you to optimize any parameter, even for preprocessing etc. in the same objective function
% There are two parts of the hyperparameters optimizer: Sampling strategy and Pruning strategy
    % Sampling strategy determines where to look
        % Uses bayesian filtering to find places where it has had the best results and focus in on those
        % As Optuna tries to minimize/maximize the objective function it focuses in on the best areas and makes more trials there
        % There are multiple samplers - Even the traditional ones
        % Choosing the right sampler is sort of a heuristic 
    % Pruning strategy can terminate trials early that are not promising so that the compute can be dedicated to more promising trials
        % Basically trials that have a slow start and will never be able to make up for that slow start, are pruned
% Optuna is very easy to scale up. It allows you to use a single database across multiple machines
    % This means that Optuna will let you use multiple computers to optimize over the search space at the same time
    % Its called asynchronous parallelization of trials - One trial could start later than the other
    % Near linear scaling with the number of machines
% Has tools to help you determine most important parameters to avoid curse of dimensionality
    % get_param_importances
    % It works by running a small number of trials and then returns this information to you
    % Helps you dial in where optuna should be focusing to get most optimal results

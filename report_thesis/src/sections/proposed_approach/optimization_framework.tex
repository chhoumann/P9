\subsection{Optimization Framework}\label{sec:optimization_framework}
One of the primary challenges in developing a stacking ensemble is determining the optimal choice of base estimators. \citet{wolpertstacked_1992} highlighted that this can be considered a 'black art' and that the choice usually relies on intelligent guesses. 
In our case, this problem is further exacerbated by the fact that the optimal choice of base estimator may vary depending on the target oxide. 
The complexity of the problem is increased because different oxides require different models, and the optimal preprocessing techniques will depend on both the model and the specific oxide being predicted. 
Additionally, each estimator may require different variations of preprocessing depending on the oxide. 
Due to challenges such as matrix effects, multicollinearity, and high dimensionality caused by very complex physical processes, it is difficult to determine which configuration is optimal.
Selecting the appropriate preprocessing steps for each base estimator is essential, as incorrect preprocessing can significantly degrade performance and undermine the model's effectiveness
Furthermore, choosing the right hyperparameters for each base estimator introduces additional complexity, as these decisions also significantly impact model performance and must be carefully tuned for each specific oxide. 
Some estimators might require very little tuning to achieve accurate and robust predictions, while others might require extensive tuning, depending on the target oxide. 
For instance, simpler models like Elastic Net and Ridge Regression might quickly yield satisfactory results with minimal hyperparameter adjustments. 
In contrast, more complex models like Convolutional Neural Networks or Gradient Boosting Regression involve a larger number of hyperparameters that need fine-tuning to perform well. 
The extent of tuning required is also influenced by the characteristics of the target oxide, such as its data distribution, noise levels, and feature interactions. 
These factors can affect how sensitive an estimator is to its hyperparameters.
Finally, hyperparameters cannot be considered in isolation, because depending on the preprocessing steps applied to the data, the optimal hyperparameters may vary.
Given these complexities, we need a systematic approach to determine the optimal configuration of hyperparameters and preprocessing steps tailored to each estimator and oxide.

To guide this process we have developed a working assumption.
Namely, that we assume that selecting the top-$n$ best pipelines for each oxide, given different preprocessors and models per pipeline, will yield the best pipelines for a given oxide in our stacking ensemble.
Here, $n$ is a heuristic based on the results and \textit{best} is evaluated in terms of the metrics outlined in Section~\ref{subsec:evaluation_metrics}.
Additionaly, each permutation will utilize our proposed data partitioning and cross-validation strategy outlined in Section~\ref{subsec:validation_testing_procedures}.
Utilizing our proposed data partitioning and cross-validation strategy, along with the aformentioned evaluation metrics, will ensure that the top-$n$ pipelines align with our goals of generalization, robustness, and accuracy outlined in Section~\ref{sec:problem_definition}.
This narrows our focus to two key tasks: selecting suitable preprocessors and models, and devising a guided search strategy to evaluate various permutations and identify the top-$n$ pipelines for each oxide. 
First, we curated a diverse set of models and preprocessing techniques, as detailed in Section~\ref{sec:model-selection}.
Next, we developed an optimization framework to systematically explore and optimize these pipeline configurations, which will be described in the following section.

\subsubsection{The Framework}
To systematically explore and optimize pipeline configurations, the search process should be guided by an ojective function.
Based on the evaluation process outlined in Section~\ref{subsec:validation_testing_procedures}, whereby we argue that solely evaluating on the RMSEP may lead to misleading and poor results, we define the objective function we wish to optimize as a multi-objective optimization on minimizing the \texttt{rmse\_cv} and \texttt{std\_dev\_cv}. 

Given these goals, traditional methods like grid search and random search could be used, but they often fall short due to several inherent limitations. 
Grid search involves exhaustively evaluating all possible combinations of hyperparameters within specified ranges. 
While thorough, this method quickly becomes computationally prohibitive as the number of hyperparameters increases. 
The curse of dimensionality means that the search space grows exponentially, making it impractical for models with many hyperparameters or when fine granularity in parameter values is needed. 

Random search, on the other hand, selects hyperparameter values at random within predefined ranges. 
It is generally more efficient than grid search and can cover a broader area of the hyperparameter space. 
However, random search can miss optimal regions, especially in high-dimensional spaces where the probability of sampling near-optimal configurations by chance is low. 

These limitations make the traditional methods unsuitable for our problem and highlight the need for a more sophisticated optimization methods.
Both grid search and random search could be enhanced using adaptive techniques, such as Bayesian optimization, to greatly improve on these approaches.
However, while very feasible, implementing such enhancements and integrating it with our existing tooling would be time-consuming.
The ambition was therefore to find tools that would provide similar or better hyperparameter optimization capabilities, while being easy to integrate with our existing framework.
For this reason we chose to use Optuna as the basis for our optimization framework.

Optuna provides well-defined abstraction which allowed us to more quickly construct a framework that would help us efficiently explore and optimize pipeline configurations.\cite{optuna_2019}
Optuna provides Bayesian optimization search algorithms, but with additional configurable parameters that allow us to customize the search process to our specific needs.
Optuna also provides a pruning mechanism that provides early stopping, if the evaluation of a set of hyperparameters is not promising, which reduces the computational costs of the optimization process.

Using Optuna as the foundation for our optimization framework, we were able to design an end-to-end system for \gls{libs} data that handles the entire process from data preparation to partitioning, cross-validation, and hyperparameter optimization.
Using this framework, we are able to effeciently compute the aforementioned objective function, which is the minimization of the \texttt{rmse\_cv} and \texttt{std\_dev\_cv}.

The framework we developed can be divided into two main components.
A function responsible for running and managing the optimization process, seen in Algorithm~\ref{alg:study_function}(\nameref{alg:study_function}), and a function containing the optimization logic, seen in Algorithm~\ref{alg:combined_objective}(\nameref{alg:combined_objective}).

The purpose of the \nameref{alg:study_function} function is to ensure that the optimization process is done for one model at a time, for each oxide.
By managing the optimization process in this way, we obtain the flexibility to evaluate each model separately with different preprocessors and hyperparameters.
This means that each model is being evaluated fairly against each oxide and that the resulting configurations are optimized specifically for the model and oxide in question.
We believe that this approach will allow us to best identify the top-$n$ pipelines to be used in our stacking ensemble.

To manage the optimization process the function receives the number of trials to run, a list of models, and a list of oxides, seen in line~\ref{step:initialize_run_process}, and initializes the optimization environment by setting up the sampler, pruner, and the objective function, seen in lines~\ref{step:initialize_sampler} to~\ref{step:initialize_objective}.

The sampler is responsible for managing the search space of the hyperparameters for the optimization process.
This means that any hyperparameters being evaluated, for any preprocessor or model, will be managed by this sampler, which allows us to optimize for all hyperparameters at the same time.
Optuna provides several options for samplers that have different characteristics and each have their strengths and weaknesses.
However, because we require multi-objective optimization, this naturally limits the choice of sampler to those that support this.
For our framework we chose to use the Tree-structured Parzen Estimator (TPE) sampler, due to its stated optimization efficiency and its ability to handle all use cases. 
Additionally, the TPE sampler allows us to control how many of the trials to be reserved for exploration, which is beneficial when the search space is large. \cite{optuna_2019}

The pruner is responsible for stopping unpromising trials early, to reduce the computational costs of the optimization process and is initialized in line~\ref{step:initialize_pruner}.
Similar to the sampler, there exist several options for the choice of pruner, and as such the choice of parameters is dependent on the pruner chosen.

Guiding both the sampler and the pruner is the objective function, which evaluates the performance of each trial, seeking to either minimize or maximize some specified metrics.
In our case we are seeking to minimize the \texttt{rmse\_cv} and \texttt{std\_dev\_cv}, defined previously.
By informing the sampler and pruner of the performance of each trial, the objective function allows the search strategy to be dynamically adjusted. 
This information helps the sampler focus on promising regions of the hyperparameter space to exploit, while still exploring new regions. 
Simultaneously, the pruner can terminate underperforming trials early, optimizing the use of computational resources.


It receives the number of trials to run, a list of models, and a list of oxides, set by the user, as seen in line~\ref{step:initialize_run_process}.
% Start up trials should be explained when introducing sampler params in the text
% Need to clarify that m' gets its learned hyperparameters reset in each iteration of CV and again before train() 
% Make a point about why we fetch the data for every trial
\begin{algorithm}
\caption{Run Optimization Process}
\label{alg:study_function}
\begin{algorithmic}[1]
\Require Number of Trials $N$, List of Models $M$, List Oxides $O$ \label{step:initialize_run_process}
\Ensure The optimization process is run for each model and oxide. 
\State \textbf{Initialize:} $\texttt{sampler} \gets \texttt{Sampler(\texttt{sampler\_params})}$ \label{step:initialize_sampler}
\State \textbf{Initialize:} $\texttt{pruner} \gets \texttt{Pruner(\texttt{pruner\_params})}$ \label{step:initialize_pruner}
\State \textbf{Initialize:} $\texttt{objective} \gets \texttt{Minimize}(\texttt{rmse\_cv}, \texttt{std\_dev\_cv})$ \label{step:initialize_objective}

\For{each oxide $o$ in $O$} \label{step:oxide_loop}
    \For{each model $m$ in $M$} \label{step:model_loop}
        \State \texttt{objective}(\texttt{Optimize\_Combined\_Objective}(
        \State \hspace{1.5em} $m$,
        \State \hspace{1.5em} $N$,
        \State \hspace{1.5em} $o$,
        \State \hspace{1.5em} \texttt{sampler},
        \State \hspace{1.5em} \texttt{pruner},
        \State )) \label{step:optimize_combined_objective}
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Combined Objective}
\label{alg:combined_objective}
\begin{algorithmic}[1]
\Require Model $m$, Number of Trials $N$, Sampler \texttt{sampler}, Pruner \texttt{pruner}
\Ensure Returns $rmse_{cv}$ and $std\_dev_{cv}$ for each trial

\For{each trial $t$ from 1 to $N$} \label{step:trial_loop}
    \State $hp \gets \texttt{sample\_hyperparameters}(\texttt{sampler})$ \label{step:sample_hyperparameters}
    \State $m \gets \texttt{instantiate\_model}(m, hp)$ \label{step:instantiate_model}
    \Statex
    \State $s\_params \gets \texttt{sample\_scaler\_params}(\texttt{sampler})$ \label{step:sample_scaler_params}
    \State $s \gets \texttt{instantiate\_scaler}(s\_params)$ \label{step:instantiate_scaler}
    \State $t\_params \gets \texttt{sample\_transformer\_params}(\texttt{sampler})$ \label{step:sample_transformer_params}
    \State $t \gets \texttt{instantiate\_transformer}(t\_params)$ \label{step:instantiate_transformer}
    \Statex \hspace{3em} \textbf{or} \texttt{Identity}
    \State $dim\_params \gets \texttt{sample\_dim\_reduction\_params}(\texttt{sampler})$ \label{step:sample_dim_reduction_params}
    \State $dim \gets \texttt{instantiate\_dim\_reduction}(dim\_params)$ \label{step:instantiate_dim_reduction}
    \Statex \hspace{3em} \textbf{or} \texttt{Identity}
    \Statex     
    \State $pipeline \gets [s, t, dim]$ \label{step:construct_pipeline}
    \Statex
    \State \textbf{Dataset: }$D \gets \texttt{get\_data}()$ \label{step:get_data} 
    \State $T_{cv}, D_{train}, D_{test} \gets \text{apply data partitioning to } D$ \label{step:data_partitioning}
    \Statex
    \State $D_{train}'$ $\gets$ \texttt{pipeline\_apply}($D_{train}$, \texttt{pipeline}) \label{step:apply_pipeline_train}
    \State $T_{cv}'$ $\gets$ \texttt{pipeline\_apply}($T_{cv}$, \texttt{pipeline}) \label{step:apply_pipeline_cv}
    \Statex
    \State $CV_{metrics} \gets \texttt{cross\_validate}(m, T_{cv}')$ \label{step:cross_validate}
    \State $rmse_{cv} \gets \texttt{mean}(CV_{metrics}.\texttt{rmse\_values})$ \label{step:mean_rmse_cv}
    \State $std\_dev_{cv} \gets \texttt{std}(CV_{metrics}.\texttt{rmse\_values})$ \label{step:std_dev_cv}
    \Statex
    \State $m' \gets \texttt{train}(m, D_{train}')$ \label{step:train_model}
    \State $rmsep, std\_dev_{test} \gets \texttt{evaluate}(m', D_{test})$ \label{step:evaluate_model}
    \Statex
    \State \texttt{store\_metrics}($t$, $m$, $pipeline$, $rmse_{cv}$, 
    \Statex \hspace{8em} $std\_dev_{cv}$, $rmsep$, $std\_dev_{test}$) \label{step:store_metrics}
    \Statex
    \State $\texttt{should\_prune} \gets \texttt{pruner}(rmse_{cv}, std\_dev_{cv})$ \label{step:prune_check}
    \If{$\texttt{should\_prune}$} \label{step:prune}
        \State \textbf{continue}
    \EndIf
\EndFor
\State \Return $rmse_{cv}$, $std\_dev_{cv}$ \label{step:return_metrics}
\end{algorithmic}
\end{algorithm}

\subsection{Optimization Framework}\label{sec:optimization_framework}
One of the primary challenges in developing a stacking ensemble is determining the optimal choice of base estimators.
\citet{wolpert1992stacked} highlighted that this can be considered a 'black art' and that the choice usually relies on intelligent guesses.
In our case, this problem is further exacerbated by the fact that the optimal choice of base estimator may vary depending on the target oxide and the fact that we are also considering the optimal preprocessing techniques for each base estimator, as well as, the hyperparameters for each.
This means, that we are considering an entire pipeline of preprocessing techniques and base estimators for each target oxide.
Therefore, we need a systematic approach to determine the optimal configuration for each pipeline.
To guide this process we have developed a working assumption.
Namely, that we assume that selecting the top-$n$ best pipelines for each oxide, given different preprocessors and models per pipeline, will yield the best pipelines for a given oxide in our stacking ensemble.
Here, $n$ is a heuristic based on the results and \textit{best} is evaluated in terms of the metrics outlined in Section~\ref{sec:evaluation_metrics}.
Additionaly, each permutation will utilize our proposed data partitioning and cross-validation strategy outlined in Section~\ref{subsec:validation_testing_procedures}.
Utilizing our proposed data partitioning and cross-validation strategy, along with the aformentioned evaluation metrics, will ensure that the top-$n$ pipelines align with our goals of generalization, robustness, and accuracy outlined in Section~\ref{sec:problem-definition}.
This reduces the problem to finding a selection of preprocessors and models, and a method of testing multiple permutations of these to identify the top-$n$ best pipeline for each oxide.
In Section~\ref{sec:model-selection} we outlined the models and preprocessing techniques that we intend to use and in the following section we will describe the optimization framework that we have developed to address this challenge.

\subsubsection{The Framework}
The framework developed to address this challenge was constructed based on a specific set of criteria.
The framework had to be able to search through a combined search space of models and preprocessing techniques.
This could be done by traditional methods such as grid search or random search, but due to the vast search space, these methods would require an infeasible amount of time to correctly identify the optimal pipelines for each oxide.
This meant that the framework also needed to have an efficient search strategy.
Ideally, the framework should be able to balance exploration and exploitation of the search space.
This would allow the framework to identify multiple promising configurations, but also focus on the most promising.
Additionally, not all configurations will be promising. 
Some permutations of hyperparameters, combination of models and preprocessing techniques or both, may not be optimal.
This could to scenarios where the framework is wasting computational resources on configurations that should be terminated early.
Therefore, the framework should have a mechanism to terminate unpromising trials early.
Finally, the framework should be able to optimize for multiple metrics simultaneously, because as outlined in Section~\ref{subsec:validation_testing_procedures} evaluating solely on the RMSEP may lead to misleading and poor results.
For these reasons, we chose to use Optuna as the basis of our optimization framework.

Optuna is a hyperparameter optimization framework designed for direct integration with Python.
The framework's flexible search space definition allows it to dynamically adjust and optimize hyperparameters during execution. 
This flexibility enables the exploration of a wide range of hyperparameter configurations, adapting to the specific needs and complexities of different modeling scenarios. 
By supporting conditional and dependent hyperparameters, the framework can handle more sophisticated models and optimization strategies, ensuring that the best possible configurations are identified even in highly complex spaces.
Additionally, Optuna uses advanced sampling strategies to explore promising areas of the search space and employs pruning techniques to terminate unpromising trials early, optimizing computational resource use. These combined features enable efficient and effective hyperparameter optimization, balancing exploration and exploitation while conserving computational resources. \cite{optuna_2019}
However, it should be noted that while Optuna is an ideal choice for our optimization framework, it is not without its limitations.
Because there is no one-size-fits-all solution for selecting the search strategy, how frequently to explore versus exploiting the search space and which pruning strategy to use there will still be choices that are heuristic in nature.
Nevertheless, Optuna provides solid options for all of these and the framework makes adjusting these choices easy.
Therefore, using Optuna as the basis for our optimization framework, combining it with our data partitioning and cross-validation strategy, we have a systematic approach to identifying the top-$n$ best pipelines for each oxide.

The complete optimization framework is outlined in Algorithm~\ref{alg:hyperparameter_optimization_framework}.

\begin{algorithm}
\caption{Hyperparameter Optimization Framework}
\label{alg:hyperparameter_optimization_framework}
\begin{algorithmic}[1]
\Require Dataset $D$, Model $m$, Number of Trials $N$, Random Seed $seed$, Sampler \texttt{sampler}, Pruner \texttt{pruner}
\Ensure Trial data, including metrics and configuration for each trial

\State \textbf{Initialize:} Set random seed for reproducibility if seed is not None \label{step:initialize}
\For{each trial $t$ from 1 to $N$} \label{step:trial_loop}
    \State $hp \gets \texttt{sample\_hyperparameters}(m, \texttt{sampler})$ \label{step:sample_hyperparameters}
    \State $m' \gets \texttt{instantiate\_model}(m, hp)$ \label{step:instantiate_model}
    \Statex
    \State $s\_params \gets \texttt{sample\_scaler\_params}(\texttt{sampler})$ \label{step:sample_scaler_params}
    \State $s \gets \texttt{instantiate\_scaler}(s\_params)$ \label{step:instantiate_scaler}
    \State $t\_params \gets \texttt{sample\_transformer\_params}(\texttt{sampler})$ \label{step:sample_transformer_params}
    \State $t \gets \texttt{instantiate\_transformer}(t\_params)$ \newline \hspace*{3em} \textbf{or} NONE \label{step:instantiate_transformer}
    \State $dr\_params \gets \texttt{sample\_dim\_reduction\_params}(\texttt{sampler})$ \label{step:sample_dim_reduction_params}
    \State $dr \gets \texttt{instantiate\_dim\_reduction}(dr\_params)$ \newline \hspace*{3em}\textbf{or} NONE \label{step:instantiate_dim_reduction}
    \Statex     
    \State $pipeline \gets [s, t, dr]$ \textbf{or} $[s, NONE, NONE]$ \label{step:construct_pipeline}
    \State $T_{cv}, D_{train}, D_{test} \gets \text{apply data partitioning to } D$ \label{step:data_partitioning}
    \Statex
    \State $D_{train}'$ $\gets$ \texttt{pipeline\_apply}($D_{train}$, \texttt{pipeline}) \label{step:apply_pipeline_train}
    \State $T_{cv}'$ $\gets$ \texttt{pipeline\_apply}($T_{cv}$, \texttt{pipeline}) \label{step:apply_pipeline_cv}
    \Statex
    \State $CV_{metrics} \gets \texttt{cross\_validate}(m', T_{cv}')$ \label{step:cross_validate}
    \State $rmse_{cv} \gets \texttt{average}(CV_{metrics}.\texttt{rmse\_values})$ \label{step:average_rmse_cv}
    \State $std\_dev_{cv} \gets \texttt{average}(CV_{metrics}.\texttt{std\_dev\_values})$ \label{step:average_std_dev_cv}
    \Statex
    \State $m'_{train}$ $\gets$ \texttt{train}($m'$, $D_{train}'$) \label{step:train_model}
    \State $rmsep$, $\sigma_{error, test}$ $\gets$ \texttt{evaluate}($m'_{train}$, $D_{test}$) \label{step:evaluate_model}
    \Statex
    % function that stores the metrics
    \State \texttt{store\_metrics}($t$, $m'$, $pipeline$, $rmse\_cv$, \newline \hspace*{8em}$std\_dev\_cv$, $rmsep$, $\sigma_{error, test}$) \label{step:store_metrics}
\EndFor
\State \Return \texttt{rmse\_cv}, \texttt{std\_dev\_cv}, \texttt{rmsep}, $\sigma_{error, test}$ \label{step:return_metrics}
\end{algorithmic}
\end{algorithm}

Before describing the algorithm, we will first explain the details that are not covered in the algorithm.
Two central constructs of the Optuna framework is the \texttt{Study} object and the objective function.
This \texttt{Study} object is responsible for orchestrating the optimization process and is responsible for instantiating the search strategy, the pruning strategy, and the objective for the objective function, which are hyperparameters.
Because we are trying to identify the top-$n$ best pipelines on a per-oxide basis and since we are optimizing with one model at a time, we will have one \texttt{Study} object per model.
This means that each oxide will be optimizied for, with one model at a time, but with many permutations of preprocessors and hyperparameters. 
The objective function contains the logic for our optimization process and has a hyperparameter that controls the number of trials to be run.
It is also the objective which we are trying to minimize, which in our case is the \texttt{rmse\_cv} and \texttt{std\_dev\_cv}.

The algorithm starts by initializing the random seed for reproducibility (line~\ref{step:initialize}).
For each trial, the algorithm samples hyperparameters for the model (line~\ref{step:sample_hyperparameters}) and instantiates the model with the sampled hyperparameters (line~\ref{step:instantiate_model}).
Next, it samples hyperparameters for the scaler (line~\ref{step:sample_scaler_params}), transformer (line~\ref{step:sample_transformer_params}), and dimensionality reduction (line~\ref{step:sample_dim_reduction_params}) techniques and instantiates them (lines~\ref{step:instantiate_scaler} to \ref{step:instantiate_dim_reduction}).
The algorithm constructs the preprocessor pipeline in the order that they are defined (line~\ref{step:construct_pipeline}).
The data is partitioned using our partitioning strategy, yielding the cross-validation folds, as well as, the training and test set (line~\ref{step:data_partitioning}).
Recall that, to ensure that we are able to evaluate the models generalizability and accuracy on unseen data, we have to generate both the cross-validation folds and the training and test set.
It then applies the pipeline to the training and cross-validation data (lines~\ref{step:apply_pipeline_train} and~\ref{step:apply_pipeline_cv}).
Next, it cross-validates the model (line~\ref{step:cross_validate}) and calculates the average RMSE and standard deviation of the RMSE (lines~\ref{step:average_rmse_cv} and~\ref{step:average_std_dev_cv}).
The model is then trained on the training data (line~\ref{step:train_model}) and evaluated on the test data (line~\ref{step:evaluate_model}).
Finally, the metrics for the trial are stored (line~\ref{step:store_metrics}) and the metrics for all trials are returned (line~\ref{step:return_metrics}).
\subsection{Optimization Framework}\label{sec:optimization_framework}
One of the primary challenges in developing a stacking ensemble is determining the optimal choice of base estimators. \citet{wolpertstacked_1992} highlighted that this can be considered a 'black art' and that the choice usually relies on intelligent guesses. 
In our case, this problem is further exacerbated by the fact that the optimal choice of base estimator may vary depending on the target oxide. 
The complexity of the problem is increased because different oxides require different models, and the optimal preprocessing techniques will depend on both the model and the specific oxide being predicted. 
Additionally, each estimator may require different variations of preprocessing depending on the oxide. 
Due to challenges such as matrix effects, multicollinearity, and high dimensionality caused by very complex physical processes, it is difficult to determine which configuration is optimal.
Selecting the appropriate preprocessing steps for each base estimator is essential, as incorrect preprocessing can significantly degrade performance and undermine the model's effectiveness
Furthermore, choosing the right hyperparameters for each base estimator introduces additional complexity, as these decisions also significantly impact model performance and must be carefully tuned for each specific oxide. 
Some estimators might require very little tuning to achieve accurate and robust predictions, while others might require extensive tuning, depending on the target oxide. 
For instance, simpler models like Elastic Net and Ridge Regression might quickly yield satisfactory results with minimal hyperparameter adjustments. 
In contrast, more complex models like Convolutional Neural Networks or Gradient Boosting Regression involve a larger number of hyperparameters that need fine-tuning to perform well. 
The extent of tuning required is also influenced by the characteristics of the target oxide, such as its data distribution, noise levels, and feature interactions. 
These factors can affect how sensitive an estimator is to its hyperparameters.
Finally, hyperparameters cannot be considered in isolation, because depending on the preprocessing steps applied to the data, the optimal hyperparameters may vary.
Given these complexities, we need a systematic approach to determine the optimal configuration of hyperparameters and preprocessing steps tailored to each estimator and oxide.

To guide this process we have developed a working assumption.
Namely, that we assume that selecting the top-$n$ best pipelines for each oxide, given different preprocessors and models per pipeline, will yield the best pipelines for a given oxide in our stacking ensemble.
Here, $n$ is a heuristic based on the results and \textit{best} is evaluated in terms of the metrics outlined in Section~\ref{subsec:evaluation_metrics}.
Additionaly, each permutation will utilize our proposed data partitioning and cross-validation strategy outlined in Section~\ref{subsec:validation_testing_procedures}.
Utilizing our proposed data partitioning and cross-validation strategy, along with the aformentioned evaluation metrics, will ensure that the top-$n$ pipelines align with our goals of generalization, robustness, and accuracy outlined in Section~\ref{sec:problem_definition}.
This narrows our focus to two key tasks: selecting suitable preprocessors and models, and devising a guided search strategy to evaluate various permutations and identify the top-$n$ pipelines for each oxide. 
First, we curated a diverse set of models and preprocessing techniques, as detailed in Section~\ref{sec:model-selection}.
Next, we developed an optimization framework to systematically explore and optimize these pipeline configurations, which will be described in the following section.

\subsubsection{The Framework}
To systematically explore and optimize pipeline configurations, the search process should be guided by an ojective function.
Based on the evaluation process outlined in Section~\ref{subsec:validation_testing_procedures}, whereby we argue that solely evaluating on the RMSEP may lead to misleading and poor results, we define the objective function we wish to optimize as a multi-objective optimization on minimizing the \texttt{rmse\_cv} and \texttt{std\_dev\_cv}. 

Given these goals, traditional methods like grid search and random search could be used, but they often fall short due to several inherent limitations. 
Grid search involves exhaustively evaluating all possible combinations of hyperparameters within specified ranges. 
While thorough, this method quickly becomes computationally prohibitive as the number of hyperparameters increases. 
The curse of dimensionality means that the search space grows exponentially, making it impractical for models with many hyperparameters or when fine granularity in parameter values is needed. 

Random search, on the other hand, selects hyperparameter values at random within predefined ranges. 
It is generally more efficient than grid search and can cover a broader area of the hyperparameter space. 
However, random search can miss optimal regions, especially in high-dimensional spaces where the probability of sampling near-optimal configurations by chance is low. 

These limitations make the traditional methods unsuitable for our problem and highlight the need for a more sophisticated optimization methods.
Both grid search and random search could be enhanced using adaptive techniques, such as Bayesian optimization, to greatly improve on these approaches.
However, while very feasible, implementing such enhancements and integrating it with our existing data partioning and cross-validation strategy would be time-consuming.
The ambition was therefore to find tools that would provide similar or better hyperparameter optimization capabilities, while being easy to integrate with our existing framework.
For this reason we chose to use Optuna as the basis for our optimization framework.

Optuna provides well-defined abstraction which allowed us to more quickly construct a framework that would help us efficiently explore and optimize pipeline configurations.\cite{optuna_2019}
Optuna provides Bayesian optimization search algorithms, but with additional configurable parameters that allow us to customize the search process to our specific needs.
Optuna also provides a pruning mechanism that provides early stopping, if the evaluation of a set of hyperparameters is not promising, which reduces the computational costs of the optimization process.




\begin{algorithm}
\caption{Hyperparameter Optimization Framework}
\label{alg:hyperparameter_optimization_framework}
\begin{algorithmic}[1]
\Require Dataset $D$, Model $m$, Number of Trials $N$, Random Seed $seed$, Sampler \texttt{sampler}, Pruner \texttt{pruner}
\Ensure Trial data, including metrics and configuration for each trial

\State \textbf{Initialize:} Set random seed for reproducibility if seed is not None \label{step:initialize}
\For{each trial $t$ from 1 to $N$} \label{step:trial_loop}
    \State $hp \gets \texttt{sample\_hyperparameters}(m, \texttt{sampler})$ \label{step:sample_hyperparameters}
    \State $m' \gets \texttt{instantiate\_model}(m, hp)$ \label{step:instantiate_model}
    \Statex
    \State $s\_params \gets \texttt{sample\_scaler\_params}(\texttt{sampler})$ \label{step:sample_scaler_params}
    \State $s \gets \texttt{instantiate\_scaler}(s\_params)$ \label{step:instantiate_scaler}
    \State $t\_params \gets \texttt{sample\_transformer\_params}(\texttt{sampler})$ \label{step:sample_transformer_params}
    \State $t \gets \texttt{instantiate\_transformer}(t\_params)$ \newline \hspace*{3em} \textbf{or} NONE \label{step:instantiate_transformer}
    \State $dr\_params \gets \texttt{sample\_dim\_reduction\_params}(\texttt{sampler})$ \label{step:sample_dim_reduction_params}
    \State $dr \gets \texttt{instantiate\_dim\_reduction}(dr\_params)$ \newline \hspace*{3em}\textbf{or} NONE \label{step:instantiate_dim_reduction}
    \Statex     
    \State $pipeline \gets [s, t, dr]$ \textbf{or} $[s, NONE, NONE]$ \label{step:construct_pipeline}
    \State $T_{cv}, D_{train}, D_{test} \gets \text{apply data partitioning to } D$ \label{step:data_partitioning}
    \Statex
    \State $D_{train}'$ $\gets$ \texttt{pipeline\_apply}($D_{train}$, \texttt{pipeline}) \label{step:apply_pipeline_train}
    \State $T_{cv}'$ $\gets$ \texttt{pipeline\_apply}($T_{cv}$, \texttt{pipeline}) \label{step:apply_pipeline_cv}
    \Statex
    \State $CV_{metrics} \gets \texttt{cross\_validate}(m', T_{cv}')$ \label{step:cross_validate}
    \State $rmse_{cv} \gets \texttt{average}(CV_{metrics}.\texttt{rmse\_values})$ \label{step:average_rmse_cv}
    \State $std\_dev_{cv} \gets \texttt{average}(CV_{metrics}.\texttt{std\_dev\_values})$ \label{step:average_std_dev_cv}
    \Statex
    \State $m'_{train}$ $\gets$ \texttt{train}($m'$, $D_{train}'$) \label{step:train_model}
    \State $rmsep$, $\sigma_{error, test}$ $\gets$ \texttt{evaluate}($m'_{train}$, $D_{test}$) \label{step:evaluate_model}
    \Statex
    % function that stores the metrics
    \State \texttt{store\_metrics}($t$, $m'$, $pipeline$, $rmse\_cv$, \newline \hspace*{8em}$std\_dev\_cv$, $rmsep$, $\sigma_{error, test}$) \label{step:store_metrics}
\EndFor
\State \Return \texttt{rmse\_cv}, \texttt{std\_dev\_cv}, \texttt{rmsep}, $\sigma_{error, test}$ \label{step:return_metrics}
\end{algorithmic}
\end{algorithm}

Before describing the algorithm, we will first explain the details that are not covered in the algorithm.
Two central constructs of the Optuna framework is the \texttt{Study} object and the objective function.
This \texttt{Study} object is responsible for orchestrating the optimization process and is responsible for instantiating the search strategy, the pruning strategy, and the objective for the objective function, which are hyperparameters.
Because we are trying to identify the top-$n$ best pipelines on a per-oxide basis and since we are optimizing with one model at a time, we will have one \texttt{Study} object per model.
This means that each oxide will be optimizied for, with one model at a time, but with many permutations of preprocessors and hyperparameters. 
The objective function contains the logic for our optimization process and has a hyperparameter that controls the number of trials to be run.
It is also the objective which we are trying to minimize, which in our case is the \texttt{rmse\_cv} and \texttt{std\_dev\_cv}.

The algorithm starts by initializing the random seed for reproducibility (line~\ref{step:initialize}).
For each trial, the algorithm samples hyperparameters for the model (line~\ref{step:sample_hyperparameters}) and instantiates the model with the sampled hyperparameters (line~\ref{step:instantiate_model}).
Next, it samples hyperparameters for the scaler (line~\ref{step:sample_scaler_params}), transformer (line~\ref{step:sample_transformer_params}), and dimensionality reduction (line~\ref{step:sample_dim_reduction_params}) techniques and instantiates them (lines~\ref{step:instantiate_scaler} to \ref{step:instantiate_dim_reduction}).
The algorithm constructs the preprocessor pipeline in the order that they are defined (line~\ref{step:construct_pipeline}).
The data is partitioned using our partitioning strategy, yielding the cross-validation folds, as well as, the training and test set (line~\ref{step:data_partitioning}).
Recall that, to ensure that we are able to evaluate the models generalizability and accuracy on unseen data, we have to generate both the cross-validation folds and the training and test set.
It then applies the pipeline to the training and cross-validation data (lines~\ref{step:apply_pipeline_train} and~\ref{step:apply_pipeline_cv}).
Next, it cross-validates the model (line~\ref{step:cross_validate}) and calculates the average RMSE and standard deviation of the RMSE (lines~\ref{step:average_rmse_cv} and~\ref{step:average_std_dev_cv}).
The model is then trained on the training data (line~\ref{step:train_model}) and evaluated on the test data (line~\ref{step:evaluate_model}).
Finally, the metrics for the trial are stored (line~\ref{step:store_metrics}) and the metrics for all trials are returned (line~\ref{step:return_metrics}).
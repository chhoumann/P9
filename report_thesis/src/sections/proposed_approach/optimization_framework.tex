\subsection{Optimization Framework}\label{sec:optimization-framework}




















In this section we introduce our optimization framework built on the Optuna framework.
Optuna is a hyperparameter optimization framework designed for direct integration with Python. Its dynamic embedding capability allows hyperparameters to be defined and adjusted within the code during execution, providing additional flexibility and ease of debugging. 
Optuna uses advanced sampling strategies to explore promising areas of the search space and employs pruning techniques to terminate unpromising trials early, optimizing computational resource use. 
It also supports scaling across multiple machines by distributing the optimization process, allowing for concurrent optimization. 
Additionally, tools are provided to identify and focus on the most impactful parameters, aiding in the efficient handling of high-dimensional search spaces. \cite{optuna_2019} 
Because an essential aspect of our study is to identify the optimal configuration of model and preprocessing techniques for that model on a per-oxide basis, these factors make Optuna an ideal choice as the basis of our optimization framework.
Optunas flexibility meant that it was easy to customize the optimization process to solve this problem and integrate it with our existing codebase.

At the core of the framework is what is referred to as the objective function.
The objective function acts as the primary building block, where the sampling parameters are set for the preprocessing techniques and the models and is the function that is minimized. % rephrase
Sampling parameters in this context relate to the range of values or options that the optimizer can choose from.

Because we want to identify not only the best model configuration, but also the best preprocessing configuration for that model, we have combined the sampling of the model and preprocessing parameters into a single objective function.
By doing this, we can optimize the entire pipeline in a single step, rather than optimizing the model and preprocessing steps separately.
% We 
With such a setup, Optuna will attempt to optimize any permutation of scaler, transformer, dimensionality reduction, and model hyperparameters throughout the optimization process.
This allows us to fully utilize Optunas sampling and pruning strategies, minimizing wasted time and computational resources on poor configurations.

\begin{algorithm}
\caption{Hyperparameter Optimization Framework}
\label{alg:hyperparameter_optimization_framework}
\begin{algorithmic}[1]
\Require Dataset $D$, Model $m$, Number of Trials $N$, Random Seed $seed$, Sampler \texttt{sampler}, Pruner \texttt{pruner}
\Ensure Trial data, including metrics and configuration for each trial

\State \textbf{Initialize:} Set random seed for reproducibility if seed is not None \label{step:initialize}
\For{each trial $t$ from 1 to $N$} \label{step:trial_loop}
    \Statex // Sample hyperparameters for $m$ using \texttt{sampler} and \newline \hspace*{1.2em} instantiate it 
    \State $m'$ $\gets$ \texttt{sample\_hyperparameters()} \label{step:sample_hyperparameters}
    \Statex
    \Statex // Sample and instantiate preprocessors using \texttt{sampler}
    \State \texttt{scaler} $\gets$ \texttt{sample\_scaler()}
    \State \texttt{transformer} $\gets$ \texttt{sample\_transformer()} \textbf{or} \text{NONE}
    \State \texttt{dim\_reduction} $\gets$ \texttt{sample\_dim\_reduction()} \textbf{or} \text{NONE}
    \Statex
    \Statex // Construct pipeline of preprocessors in that order
    \State \texttt{pipeline} $\gets$ [\texttt{scaler}, \texttt{transformer}, \texttt{dim\_reduction}]
    \Statex
    \Statex // Apply data partitioning
    \State $T_{cv}, D_{train}, D_{test} \gets \text{apply data partitioning to } D$
    \Statex
    \Statex // Apply preprocessing to $D_{train}$ and $T_{cv}$
    \State $D_{train}'$ $\gets$ \texttt{pipeline\_apply}($D_{train}$, \texttt{pipeline})
    \State $T_{cv}'$ $\gets$ \texttt{pipeline\_apply}($T_{cv}$, \texttt{pipeline})
    \Statex
    \State $CV_{metrics} \gets \texttt{cross\_validate}(m', T_{cv}')$
    \State $rmse_{cv} \gets \texttt{average}(CV_{metrics}.\texttt{rmse\_values})$
    \State $std\_dev_{cv} \gets \texttt{average}(CV_{metrics}.\texttt{std\_dev\_values})$
    \Statex
    \State $m'$ $\gets$ \texttt{train}($m'$, $D_{train}'$)
    \State $rmsep$, $\sigma_{error, test}$ $\gets$ \texttt{evaluate}($m'$, $D_{test}$)
    \Statex
    % function that stores the metrics
    \State \texttt{store\_metrics}($t$, $m'$, $pipeline$, $rmse\_cv$, \newline \hspace*{8em}$std\_dev\_cv$, $rmsep$, $\sigma_{error, test}$)
\EndFor
\State \Return \texttt{rmse\_cv}, \texttt{std\_dev\_cv}, \texttt{rmsep}, $\sigma_{error, test}$
\end{algorithmic}
\end{algorithm}% This increases the likelyhood at identifying the 

% create names for evaluation metrics
% 



% Optuna for its flexibility and efficiency in exploring the vast search space of configurations.
% Optuna allows for great flexiblity in exploring various search spaces due to its modular design.
% In addition to this modularity, Optuna uses a "define-by-run" optimization strategy.
% Rather than being confined to a fixed order and range of exploration, Optuna can dynamically adjust regions based on the results of previous trials.
% Introduction of the optimization framework
    % What is the goal of the optimization framework?
    % Why Optuna?
        % Explanation of what optuna is and why it is useful for our purposes
        % Should elaborate on this:
            % This framework facilitates automated hyperparameter optimization, allowing us to  efficiently explore a vast search space of model and preprocessing configurations.
% how did we do it?
    % Maybe talk about how we designed it in this way
    % pros cons ** maybe
    % Considerations at the very least
    % Talk about how the objective function was constructed, the order of things, considerations with this, the fact that we are trying to minimize and what metrics we are minimizing
    % 
% show diagram or pseudocode
% Explain why we did it this way
    % Why did we choose this approach?
    % What are the benefits of this approach?
    % How does this approach help us achieve our goal?

   
    
% Next, we implemented an experimental framework using the Optuna optimization library~\cite{optuna_2019}.
% This framework facilitates automated hyperparameter optimization, allowing us to efficiently explore a vast search space of model and preprocessing configurations.
% The specifics of this framework are discussed in Section~\ref{sec:optimization_framework}.


% Optuna uses python syntax instead of some propriety syntax
% Optuna brings the optimization into the space of the program rather than outside
    % Meaning you can embed it directly in functions etc.
    % Easier to debug
    % Can use python language such as looping etc.
% Instead of having parameters set you change them so they are sampled using the suggest_* methods
    % This is also helpful because it allows you to optimize any parameter, even for preprocessing etc. in the same objective function
% There are two parts of the hyperparameters optimizer: Sampling strategy and Pruning strategy
    % Sampling strategy determines where to look
        % Uses bayesian filtering to find places where it has had the best results and focus in on those
        % As Optuna tries to minimize/maximize the objective function it focuses in on the best areas and makes more trials there
        % There are multiple samplers - Even the traditional ones
        % Choosing the right sampler is sort of a heuristic 
    % Pruning strategy can terminate trials early that are not promising so that the compute can be dedicated to more promising trials
        % Basically trials that have a slow start and will never be able to make up for that slow start, are pruned
% Optuna is very easy to scale up. It allows you to use a single database across multiple machines
    % This means that Optuna will let you use multiple computers to optimize over the search space at the same time
    % Its called asynchronous parallelization of trials - One trial could start later than the other
    % Near linear scaling with the number of machines
% Has tools to help you determine most important parameters to avoid curse of dimensionality
    % get_param_importances
    % It works by running a small number of trials and then returns this information to you
    % Helps you dial in where optuna should be focusing to get most optimal results

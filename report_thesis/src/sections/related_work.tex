\section{Related Work}
In addressing the challenge of predicting major oxide compositions from \gls{libs} data, our investigation intersects with a broad spectrum of existing research that tackles similar computational hurdles, such as high dimensionality, multicollinearity, matrix effects, and the challenges of small datasets.
This section outlines how prior works relate to our problem, drawing from a range of techniques and methodologies that offer potential pathways for enhancing the accuracy and robustness of our predictions.

\citet{andersonPostlandingMajorElement2022} experimented with different machine learning models for quantifying major oxides on Mars using the SuperCam instrument on the Mars 2020 Perseverance rover.
They discuss preprocessing, normalization of \gls{libs} spectra, and the development of multivariate regression models to predict major element compositions.
For each oxide, they tested different models and selected the best performing one.
In some cases, they used a blend of models to improve the predictions.
The models they tested include: \gls{ols}, \gls{pls}, \gls{lasso}, Ridge, \gls{enet}, \gls{omp}, \gls{svr}, \gls{rf}, \gls{gbr}, and local \gls{enet} and blended submodels.
For \ce{SiO2}, they used a blend of \gls{gbr} and \gls{pls} models.
Interestingly, they found that PLS performed better at longer distances (4.25m), but GBR was better at 3m.
For \ce{TiO2}, they selected the \gls{rf} model for its superior performance at 4.25m and overall lower \gls{rmsep}.
For \ce{Al2O3}, they used an average of predictions from four models (Local \gls{enet}, \gls{rf}, two variants of \gls{pls}) to obtain the lowest \gls{rmsep}.
For \ce{FeO_T}, they initially selected \gls{rf} but later replaced it with \gls{gbr} due to its more realistic stoichiometry predictions for high-\ce{Ca} pyroxenes and overall performance.
For \ce{MgO}, they selected \gls{gbr} for having the lowest \gls{rmsep} and avoiding negative predictions, despite slightly overpredicting \ce{MgO} for high concentration samples.
For \ce{CaO}, they used a blend of \gls{rf} and \gls{pls} to address the bimodal distribution of \ce{CaO} predictions by the \gls{rf} model alone.
For \ce{Na2O}, they used a blend of \gls{gbr} and \gls{lasso} models to utilize \gls{gbr}'s accuracy at low concentrations and \gls{lasso}'s superior predictions at higher concentrations.
For \ce{K2O}, they selected \gls{lasso} for its better performance on high \ce{K2O} samples, despite the averaged model of five algorithms showing slight improvements at lower concentrations.
The findings of this paper are significant to us because they provide a benchmark for the performance of different machine learning models on \gls{libs} spectra.
We can use this information to guide our model selection and to compare our results with theirs.
Additionally, we might want to try out different models from the ones they tested to see if we can improve the predictions further, or perhaps find a model that is more suitable for our specific use case.
Also, SuperCam being the successor to \gls{chemcam} means that the findings of this paper are directly relevant to our work.

\citet{song_DF-K-ELM} present a novel approach to enhance the performance and interpretability of machine learning models in the context of \gls{libs} quantification.
The authors use "knowledge-based spectral lines, related to analyte compositions, to construct a linear physical principle based model and adopts K-ELM to account for the residuals of the linear model."
This method stands out by offering an intuitive explanation of how knowledge-based spectral lines impact prediction results, thereby enhancing model interpretability without compromising model complexity.
DF-K-ELM was tested across 10 regression tasks based on 3 \gls{libs} datasets, comparing its performance against six baseline methods using \gls{rmsep} as the evaluation metric.
They have 3 coal datasets, and they do regression tasks involving carbon, ash, volatile matter, and heat value analysis.
It achieved the best performance in 4 tasks and the second-best in 2 tasks, demonstrating its efficacy.
Incorporation of domain knowledge not only improved the accuracy of the models but also enhanced their generalizability across different tasks.
The method's design allows for a more interpretable machine learning model that adheres closer to the physical principles underlying \gls{libs} quantification.
The integration of domain knowledge into machine learning models addresses two critical challenges: improving the interpretability of complex models and enhancing their performance by leveraging specific domain insights.
The approach demonstrates a practical application of kernel extreme learning machines combined with domain-specific insights.
This is particularly valuable in fields like spectroscopy, where understanding the relationship between the spectral data and the analyte concentration is vital.
The DF-K-ELM method showcases how hybrid models can outperform traditional machine learning approaches.
The approach demonstrates a practical application of kernel extreme learning machines combined with domain-specific insights.
This is very relevant to our work considering interpretability is a key requirement for NASA and something they considered when choosing the PLS model for the \gls{chemcam} instrument.

\citet{rezaei_dimensionality_reduction} explore a variety of statistical and machine learning methods, including \gls{mlr}, \gls{svr}, \gls{ksvr}, and \gls{ann}, alongside their integrations with \gls{pca} to reduce dimensionality and improve model performance.
They use \gls{mse} and \gls{mae} as evaluation metrics to compare the performance of the models.
This paper clearly demonstrates the effectiveness of dimensionality reduction techniques in improving the performance of machine learning models because it compares the performance of many models with and without \gls{pca}: \gls{ann}, \gls{mlr}, \gls{svr}, \gls{ksvr}, \gls{pca}-\gls{ann}, \gls{pca}-\gls{mlr}, \gls{pca}-\gls{svr}, and \gls{pca}-\gls{ksvr}.
For all elements, a variant SVR performs the best.
For \ce{Si}, SVR performs the best.
For \ce{Zn}, PCA-SVR performs the best.
For the rest of the elements, \gls{pca}-\gls{ksvr} performs the best.
The superiority of \gls{ksvr} is attributed to the its ability to handle non-linear relationships in the data effectively, especially when combined with PCA's capability to compress and simplify the input data by focusing on the most relevant variations.

\citet{yang_laser-induced_2022} present a study on the application of a deep \gls{cnn} for classifying geochemical samples using \gls{libs}, with a particular focus on planetary exploration missions such as China's Tianwen-1 Mars mission.
The authors demonstrate the effectiveness of a deep CNN in classifying geochemical standard samples using \gls{libs} spectra collected at varying distances.
This addresses the challenge of spectral differences induced by distance, showcasing that \gls{cnn} can learn to classify samples without the need for traditional spectral preprocessing or distance correction.
Using a dataset of over 18,000 \gls{libs} spectra from 39 geochemical standard samples, the study compares the \gls{cnn} model's performance against four other machine learning models: \gls{bpnn} \gls{svm}, \gls{lda}, and \gls{lr}.
The \gls{cnn} model exhibits superior classification accuracy, emphasizing its potential for geochemical sample identification/classification in planetary exploration.
The paper includes a detailed comparative analysis, proving the \gls{cnn} model's superior performance.
With classification accuracies on the validation set for all models exceeding 95\%, the \gls{cnn} model demonstrated the highest overall accuracy.
This was particularly evident as the training set size increased, indicating the model's robustness to varying distances without requiring distance correction.
Statistical analysis further confirmed the \gls{cnn} model's superiority, with higher average Ncorr values compared to other models.
The \gls{cnn} model's ability to accurately classify geochemical samples without preprocessing for distance correction is quite impressive.
This is particularly relevant to our work because we are also working with \gls{libs} spectra collected at varying distances.
The comparative analysis underscores the \gls{cnn} model as a best-fit approach for \gls{libs} data analysis, potentially setting a new standard for future research and applications in the field.

\citet{jeonEffectsFeatureEngineering2024} investigated the effects of feature engineering on the robustness of \gls{libs} for steel classification.
They developed a remote \gls{libs} system to classify six steel types, using various feature-engineering and machine learning algorithms, including SVM and FCNN, to handle different laser energies in test datasets.
They found that using intensity ratios as input data resulted in more robust classification models.
It was better than \gls{pca} and \gls{rf}-based wavelength selection.
The study highlights the importance of selecting appropriate feature engineering methods to improve model robustness, especially under varying measurement conditions.
This is relevant to our project as it demonstrates how feature engineering can enhance the performance and robustness of models for classifying materials based on \gls{libs} data, addressing challenges similar to those we face in predicting major oxide compositions.

The study by \citet{fontanaLaserInducedBreakdown2023}.
explores using \gls{libs} for whole-rock geochemical analysis, specifically for major elements like \ce{Al}, \ce{Ca}, \ce{Fe}, \ce{K}, \ce{Mg}, \ce{Na}, \ce{Si}, and \ce{Ti}.
They averaged \gls{libs} spot analyses over 1-mm spaced transects on drill core intervals, demonstrating strong correlations with lab-based geochemistry for elements like \ce{Si}, \ce{Al}, and \ce{Na}.
Different predictive models were used for each element, such as \gls{pls}, \gls{enet}, \gls{lasso}, and \gls{pcr}, showing varied \gls{rmsecv} values indicating the precision of these models.
This method's relevance to our work lies in its potential for rapid, in-situ geochemical analysis, offering a way to overcome challenges related to high dimensionality and non-linearity in \gls{libs} data.

The paper by \citet{sunMachineLearningTransfer2021} introduces transfer learning to \gls{libs} spectral data analysis for rock classification on Mars, significantly improving model performance.
Previously, models trained on laboratory standards (pellets) struggled with physical matrix effects when applied to natural rock spectra.
Transfer learning, leveraging knowledge from one domain to address related problems in another, was applied to overcome this challenge.
The method showed remarkable improvement in \gls{tas} classification accuracy for both polished and raw rock samples, with rates increasing from 25\% and 33.3\% to 83.3\% respectively using machine learning models to 83.3\% with the transfer learning model.
This demonstrates the effectiveness of transfer learning in addressing the physical matrix effect and enhancing model robustness for rock classification on Mars.

\citet{wangDeterminationElementalComposition2023} discusses an advanced methodology for analyzing stream sediments using remote \gls{libs} combined with a \gls{mdsbpnn} algorithm.
This approach yielded highly accurate quantitative analyses of both major and trace elements, with determination coefficients (R2) for major elements exceeding 0.9996 and for trace elements greater than 0.9837, and RMSE less than 0.73 (major elements).
The study emphasizes the potential of remote \gls{libs} technology, especially for identifying biominerals in geological samples, highlighting its significance for studying ancient planetary environments.

% We had this one last semester IIRC. Might be good to have again?
\citet{leporeQuantitativePredictionAccuracies2022a} provides an in-depth analysis of the effectiveness of using \gls{libs} for geochemical analysis, focusing on the optimization of calibration datasets through the creation of submodels.
It outlines the methodology for collecting and processing \gls{libs} spectra, the development of multivariate models for predicting geochemical compositions, and compares the predictive accuracies of different submodel strategies.
The study finds that while submodels can improve prediction accuracies under certain conditions, the overall effectiveness is contingent upon having a large and diverse calibration dataset.
The research suggests that the optimal use of \gls{libs} for geochemical analysis requires a balance between the specificity of submodels and the breadth of the calibration dataset to ensure accurate and reliable predictions.

\citet{kepesImprovingLaserinducedBreakdown2022} discusses enhancing \gls{libs} model accuracy using transfer learning between ChemCam and SuperCam instruments.
It proposes a method where ChemCam data transforms to approximate SuperCam spectra, improving \gls{cnn} regression models' performance.
Key methods include data augmentation and fine-tuning of \gls{cnn}s with pre-processed and normalized spectra.
This approach outperforms some existing models for specific oxides, demonstrating transfer learning's potential in \gls{libs} analyses for more accurate quantitative models.

\citet{ferreiraComprehensiveComparisonLinear2022} presents an extensive comparison of various algorithms for quantifying lithium in geological samples, with a focus on both linear and non-linear methods.
The study tested algorithms on spectra acquired from a commercial handheld device and a laboratory prototype, highlighting the challenges in quantifying lithium due to effects like saturation and matrix interference.
The results showed that non-linear methodologies, such as K-nearest neighbors regression, Support Vector Machine regression, and Artificial Neural Networks regression, generally outperformed linear methods by effectively managing saturation and matrix effects, which are common in geological samples.
This research provides valuable insights for future applications in geological sample analysis and could potentially be generalized for other elements in similar contexts.
The paper's findings are particularly relevant to our project as it demonstrates the effectiveness of non-linear machine learning techniques in handling complex, non-linear relationships in high-dimensional \gls{libs} data, aligning with our research objectives of improving major oxide composition predictions from \gls{libs} data.

The study by \citet{liuComparisonQuantitativeAnalysis2022} explores the use of MarSCoDe \gls{libs} for quantitative analysis of olivine in a simulated Martian atmosphere, focusing on multivariate analysis methods to address challenges posed by \gls{libs} data, such as high dimensionality and multicollinearity.
The methods evaluated include univariate linear regression (ULR), multivariate linear regression (MLR), principal component regression (PCR), partial least squares regression (PLSR), ridge regression, \gls{lasso}, \gls{enet}, and back-propagation neural networks.
The findings demonstrate the effectiveness of dimension reduction techniques, especially PLSR, and nonlinear analysis for improving quantitative analysis accuracy of olivine using \gls{libs} data.
This approach is particularly relevant to our work due to the focus on advanced statistical methods and machine learning algorithms for handling complex, high-dimensional \gls{libs} data, aligning with our objectives of improving accuracy and robustness in predicting major oxide compositions.

\cite{yangConvolutionalNeuralNetwork2022} is a study where a \gls{cnn} model is designed to identify twelve types of rocks using \gls{libs} data from the Mars Surface Composition Detector (MarSCoDe) for the Tianwen-1 Mars exploration mission.
The classification performance of the \gls{cnn} is compared with logistic regression (LR), \gls{svm}, and linear discriminant analysis (LDA).
The \gls{cnn} model achieved the highest classification accuracy, demonstrating its efficiency in rock identification with \gls{libs} spectra collected in a simulated Martian environment.
This indicates that \gls{cnn}-supported \gls{libs} classification is a promising analytical technique for planetary exploration missions.

\citet{silvaRobustCalibrationModels2022} introduce clustered regression calibration algorithms for \gls{libs} to address quantification challenges in complex sample matrices or wide concentration ranges, focusing on lithium quantification in geology.
They employ unsupervised clustering to group similar samples before applying a tailored linear calibration model to each cluster.
This approach, tested on lithium in exploration drills, outperforms standard linear models, especially in lower concentration regions, and demonstrates good generalizability to unseen data from different rock veins.
The study highlights the potential of clustered regression methods in improving \gls{libs} quantification accuracy and robustness, particularly valuable in mining environments.
Uses Ridge regression, \gls{pls}, dimensionality reduction with UMAp and then k-means clustering, followed by a local model for each cluster, univariate calibration curves.

\citet{woldPrincipalComponentAnalysis1987} provides an in-depth tutorial on \gls{pca}, a fundamental method in multivariate data analysis used for dimensionality reduction, outlier detection, and uncovering the underlying structure in data sets.
The paper covers the history and development of \gls{pca}, its mathematical foundations, applications across various fields, and detailed instructions for data pre-treatment and \gls{pca} implementation.
It emphasizes \gls{pca}'s utility in simplifying complex data sets, enhancing interpretability, and supporting data analysis through the extraction of principal components that capture the most variance in the data.
This work is particularly relevant to our project as it underscores the importance of \gls{pca} in handling high-dimensional data, which aligns with our objectives of improving the prediction accuracy and interpretability of major oxide compositions from \gls{libs} data by effectively managing multicollinearity and high dimensionality challenges.

\citet{bankAutoencoders2021} details various autoencoder architectures and their applications in machine learning, emphasizing their role in dimensionality reduction, feature learning, and generative models.
It explores regularized, denoising, and variational autoencoders, highlighting their advantages in compressing data into lower-dimensional spaces while retaining essential information for reconstruction.
Autoencoders could significantly enhance our ability to handle the high-dimensional nature of \gls{libs} data.
By compressing spectral data into more manageable representations without losing critical information, we can improve model performance, especially in predicting major oxide compositions, by focusing on the most relevant features extracted from the compressed data representation.
This aligns with our objectives of efficient dimensionality reduction and robust predictive modeling.

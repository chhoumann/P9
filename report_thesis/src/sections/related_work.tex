\section{Related Work}
A sizeable body of methodologies has been developed to enhance the accuracy and robustness of \gls{libs} predictions.
Key strategies include the integration of machine learning and deep learning models, the incorporation of domain knowledge, effective preprocessing techniques, and dimensionality reduction methods.
These approaches collectively aim to manage the complexities inherent in \gls{libs} data and improve predictive performance.
We review existing and relevant work through a thematic taxonomy, highlighting their potential applications in our study.

\subsection{Machine Learning Models in LIBS Analysis}
Several studies have applied machine learning models to analyze \gls{libs} data, aiming to predict major oxide compositions with high accuracy.

\citet{andersonPostlandingMajorElement2022} utilized machine learning models to quantify major oxides on Mars using the SuperCam instrument on the Perseverance rover.
Their approach involved extensive preprocessing and normalization of LIBS spectra, followed by the development of multivariate regression models for each oxide.
They demonstrated that blending different models, such as \gls{gbr} and \gls{pls} for \ce{SiO2}, could improve prediction accuracy.
Their study serves as a benchmark for model performance on LIBS spectra and offers insights into model selection for similar datasets.

\citet{yangConvolutionalNeuralNetwork2022} demonstrated the effectiveness of a deep \gls{cnn} for classifying geochemical samples using \gls{libs} spectra collected at varying distances.
Their model outperformed traditional machine learning approaches, emphasizing the potential of \gls{cnn}s for geochemical sample identification in planetary exploration missions like China's Tianwen-1.

\subsection{Hybrid and Domain-Knowledge-Driven Models}
Incorporating domain knowledge into machine learning models can significantly enhance their interpretability and performance.

\citet{song_DF-K-ELM} introduced a hybrid model, \gls{df}-\gls{k-elm}, which integrates domain knowledge-based spectral lines with kernel extreme learning machines.
This method was particularly effective across multiple regression tasks, demonstrating improved accuracy and generalizability.
The integration of domain-specific insights allowed the model to adhere more closely to the physical principles underlying \gls{libs} quantification, making it highly relevant for applications requiring model interpretability.

\citet{sunMachineLearningTransfer2021} applied transfer learning to \gls{libs} spectral data analysis, significantly improving model performance in rock classification on Mars.
By leveraging knowledge from one domain to address related problems in another, their approach addressed the physical matrix effect, enhancing the robustness of the models for rock classification.

\subsection{Preprocessing and Feature Engineering}
Effective preprocessing and feature engineering are critical for enhancing the robustness of \gls{libs} models.

\citet{jeonEffectsFeatureEngineering2024} investigated the effects of various feature-engineering techniques on the robustness of \gls{libs} models for steel classification.
They developed a remote \gls{libs} system to classify six steel types, using various feature-engineering and machine learning algorithms, including \gls{svm} and \gls{fcnn}, to handle different laser energies in test datasets.
They found that using \gls{libs} signal intensity ratio as input data significantly improved model robustness under varying measurement conditions, demonstrating the importance of selecting appropriate feature-engineering methods.

\citet{Huang2015AnEA} conducted a systematic analysis of data preprocessing techniques, emphasizing the need for tailored strategies to enhance model accuracy.
Evaluating methods such as feature selection, case selection, scaling, and missing data treatments, they demonstrate that the effectiveness of these techniques varies markedly across different datasets and machine learning algorithms.
Their findings underscore the interdependent relationship between preprocessing techniques and model selection, which is crucial for optimizing predictive performance.

\subsection{Dimensionality Reduction Techniques}
Dimensionality reduction techniques such as \gls{pca} play a crucial role in managing the high-dimensional nature of \gls{libs} data.

\citet{woldPrincipalComponentAnalysis1987} provided a foundational understanding of \gls{pca}, highlighting its utility in simplifying complex datasets and enhancing interpretability.
\gls{pca}'s ability to extract principal components that capture the most variance in the data is particularly relevant for improving the prediction accuracy of major oxide compositions from \gls{libs} data.

\citet{scholkopftKPCA} introduced a method for nonlinear \gls{pca} using kernel functions, known as \gls{kernel-pca}.
While they do not apply it specifically to \gls{libs} data, this approach allows for efficient computation of principal components in high-dimensional feature spaces and the ability to handle nonlinear relationships in the data more effectively than traditional \gls{pca}.
This provides a powerful tool for dimensionality reduction in complex datasets, making it particularly relevant for \gls{libs} analytics.

\citet{rezaei_dimensionality_reduction} explored various machine learning techniques combined with \gls{pca} for dimensionality reduction. Their results demonstrated that \gls{svr} with \gls{pca} performed the best for different elements, highlighting the effectiveness of dimensionality reduction techniques in improving model performance by managing the complexities of high-dimensional data.

% \section{Related Work}
% In addressing the challenge of predicting major oxide compositions from \gls{libs} data, our investigation intersects with a broad spectrum of existing research that tackles similar computational hurdles, such as high dimensionality, multicollinearity, matrix effects, and the challenges of small datasets.
% This section outlines how prior works relate to our problem, drawing from a range of techniques and methodologies that offer potential pathways for enhancing the accuracy and robustness of our predictions.

% \citet{andersonPostlandingMajorElement2022} experimented with machine learning models to quantify major oxides on Mars using the SuperCam instrument on the Perseverance rover. They discussed preprocessing, normalization of \gls{libs} spectra, and developed multivariate regression models to predict major element compositions. They tested various models, selecting the best for each oxide and sometimes blending models to improve predictions.

% For \ce{SiO2}, they used a blend of \gls{gbr} and \gls{pls}, finding \gls{pls} better at longer distances (4.25m) and \gls{gbr} better at shorter distances (3m). For \ce{TiO2}, the \gls{rf} model performed best at 4.25m, with the lowest root mean square error of prediction (\gls{rmsep}). For \ce{Al2O3}, an average of predictions from local \gls{enet}, \gls{rf}, and two \gls{pls} variants yielded the lowest \gls{rmsep}. They initially selected \gls{rf} for \ce{FeO_T} but switched to \gls{gbr} for its more realistic stoichiometry predictions. For \ce{MgO}, \gls{gbr} was chosen for its lowest \gls{rmsep} despite slight overpredictions at high concentrations. For \ce{CaO}, they blended \gls{rf} and \gls{pls} to address the bimodal distribution of \gls{rf} predictions. For \ce{Na2O}, a blend of \gls{gbr} and \gls{lasso} utilized \gls{gbr}'s accuracy at low concentrations and \gls{lasso}'s at higher concentrations. For \ce{K2O}, \gls{lasso} was preferred for its performance on high \ce{K2O} samples.

% Their findings provide a benchmark for model performance on \gls{libs} spectra, guiding our model selection and allowing us to compare our results. SuperCam, as the successor to \gls{chemcam}, ensures their insights are directly relevant to our work.

% \citet{song_DF-K-ELM} present a novel approach to enhance the performance and interpretability of machine learning models in the context of \gls{libs} quantification.
% The authors use "knowledge-based spectral lines, related to analyte compositions, to construct a linear physical principle based model and adopts \gls{k-elm} to account for the residuals of the linear model."
% The method is based on \gls{df} and \gls{k-elm} and is called \gls{df}-\gls{k-elm}.
% This method stands out by offering an intuitive explanation of how knowledge-based spectral lines impact prediction results, thereby enhancing model interpretability without compromising model complexity.
% \gls{df}-\gls{k-elm} was tested across 10 regression tasks based on 3 \gls{libs} datasets, comparing its performance against six baseline methods using \gls{rmsep} as the evaluation metric.
% They have 3 coal datasets, and they do regression tasks involving carbon, ash, volatile matter, and heat value analysis.
% It achieved the best performance in 4 tasks and the second-best in 2 tasks, demonstrating its efficacy.
% Incorporation of domain knowledge not only improved the accuracy of the models but also enhanced their generalizability across different tasks.
% The method's design allows for a more interpretable machine learning model that adheres closer to the physical principles underlying \gls{libs} quantification.
% The integration of domain knowledge into machine learning models addresses two critical challenges: improving the interpretability of complex models and enhancing their performance by leveraging specific domain insights.
% The approach demonstrates a practical application of kernel extreme learning machines combined with domain-specific insights.
% This is particularly valuable in fields like spectroscopy, where understanding the relationship between the spectral data and the analyte concentration is vital.
% The \gls{df}-\gls{k-elm} method showcases how hybrid models can outperform traditional machine learning approaches.
% The approach demonstrates a practical application of kernel extreme learning machines combined with domain-specific insights.
% This is very relevant to our work considering interpretability is a key requirement for NASA and something they considered when choosing the \gls{pls} model for the \gls{chemcam} instrument.

% \citet{rezaei_dimensionality_reduction} explore a variety of statistical and machine learning methods, including \gls{mulr}, \gls{svr}, \gls{ksvr}, and \gls{ann}, alongside their integrations with \gls{pca} to reduce dimensionality and improve model performance.
% They use \gls{mse} and \gls{mae} as evaluation metrics to compare the performance of the models.
% This paper clearly demonstrates the effectiveness of dimensionality reduction techniques in improving the performance of machine learning models because it compares the performance of many models with and without \gls{pca}: \gls{ann}, \gls{mulr}, \gls{svr}, \gls{ksvr}, \gls{pca}-\gls{ann}, \gls{pca}-\gls{mulr}, \gls{pca}-\gls{svr}, and \gls{pca}-\gls{ksvr}.
% For all elements, a variant of \gls{svr} performs the best.
% For \ce{Si}, \gls{svr} performs the best.
% For \ce{Zn}, \gls{pca}-\gls{svr} performs the best.
% For the rest of the elements, \gls{pca}-\gls{ksvr} performs the best.
% The superiority of \gls{ksvr} is attributed to the its ability to handle non-linear relationships in the data effectively, especially when combined with \gls{pca}'s capability to compress and simplify the input data by focusing on the most relevant variations.

% \citet{yang_laser-induced_2022} present a study on the application of a deep \gls{cnn} for classifying geochemical samples using \gls{libs}, with a particular focus on planetary exploration missions such as China's Tianwen-1 Mars mission.
% The authors demonstrate the effectiveness of a deep CNN in classifying geochemical standard samples using \gls{libs} spectra collected at varying distances.
% This addresses the challenge of spectral differences induced by distance, showcasing that \gls{cnn} can learn to classify samples without the need for traditional spectral preprocessing or distance correction.
% Using a dataset of over 18,000 \gls{libs} spectra from 39 geochemical standard samples, the study compares the \gls{cnn} model's performance against four other machine learning models: \gls{bpnn} \gls{svm}, \gls{lda}, and \gls{logreg}.
% The \gls{cnn} model exhibits superior classification accuracy, emphasizing its potential for geochemical sample identification/classification in planetary exploration.
% The paper includes a detailed comparative analysis, proving the \gls{cnn} model's superior performance.
% With classification accuracies on the validation set for all models exceeding 95\%, the \gls{cnn} model demonstrated the highest overall accuracy.
% This was particularly evident as the training set size increased, indicating the model's robustness to varying distances without requiring distance correction.
% Statistical analysis further confirmed the \gls{cnn} model's superiority, with higher average Ncorr values compared to other models.
% The \gls{cnn} model's ability to accurately classify geochemical samples without preprocessing for distance correction is quite impressive.
% This is particularly relevant to our work because we are also working with \gls{libs} spectra collected at varying distances.
% The comparative analysis underscores the \gls{cnn} model as a best-fit approach for \gls{libs} data analysis, potentially setting a new standard for future research and applications in the field.

% \citet{jeonEffectsFeatureEngineering2024} investigated the effects of feature engineering on the robustness of \gls{libs} for steel classification.
% They developed a remote \gls{libs} system to classify six steel types, using various feature-engineering and machine learning algorithms, including \gls{svm} and \gls{fcnn}, to handle different laser energies in test datasets.
% They found that using intensity ratios as input data resulted in more robust classification models.
% It was better than \gls{pca} and \gls{rf}-based wavelength selection.
% The study highlights the importance of selecting appropriate feature engineering methods to improve model robustness, especially under varying measurement conditions.
% This is relevant to our project as it demonstrates how feature engineering can enhance the performance and robustness of models for classifying materials based on \gls{libs} data, addressing challenges similar to those we face in predicting major oxide compositions.

% The study by \citet{fontanaLaserInducedBreakdown2023}.
% explores using \gls{libs} for whole-rock geochemical analysis, specifically for major elements like \ce{Al}, \ce{Ca}, \ce{Fe}, \ce{K}, \ce{Mg}, \ce{Na}, \ce{Si}, and \ce{Ti}.
% They averaged \gls{libs} spot analyses over 1-mm spaced transects on drill core intervals, demonstrating strong correlations with lab-based geochemistry for elements like \ce{Si}, \ce{Al}, and \ce{Na}.
% Different predictive models were used for each element, such as \gls{pls}, \gls{enet}, \gls{lasso}, and \gls{pcr}, showing varied \gls{rmsecv} values indicating the precision of these models.
% This method's relevance to our work lies in its potential for rapid, in-situ geochemical analysis, offering a way to overcome challenges related to high dimensionality and non-linearity in \gls{libs} data.

% The paper by \citet{sunMachineLearningTransfer2021} introduces transfer learning to \gls{libs} spectral data analysis for rock classification on Mars, significantly improving model performance.
% Previously, models trained on laboratory standards (pellets) struggled with physical matrix effects when applied to natural rock spectra.
% Transfer learning, leveraging knowledge from one domain to address related problems in another, was applied to overcome this challenge.
% The method showed remarkable improvement in \gls{tas} classification accuracy for both polished and raw rock samples, with rates increasing from 25\% and 33.3\% to 83.3\% respectively using machine learning models to 83.3\% with the transfer learning model.
% This demonstrates the effectiveness of transfer learning in addressing the physical matrix effect and enhancing model robustness for rock classification on Mars.

% \citet{wangDeterminationElementalComposition2023} discusses an advanced methodology for analyzing stream sediments using remote \gls{libs} combined with a \gls{mdsbpnn} algorithm.
% This approach yielded highly accurate quantitative analyses of both major and trace elements, with determination coefficients (R2) for major elements exceeding 0.9996 and for trace elements greater than 0.9837, and \gls{rmse} less than 0.73 (major elements).
% The study emphasizes the potential of remote \gls{libs} technology, especially for identifying biominerals in geological samples, highlighting its significance for studying ancient planetary environments.

% % We had this one last semester IIRC. Might be good to have again?
% \citet{leporeQuantitativePredictionAccuracies2022a} provides an in-depth analysis of the effectiveness of using \gls{libs} for geochemical analysis, focusing on the optimization of calibration datasets through the creation of submodels.
% It outlines the methodology for collecting and processing \gls{libs} spectra, the development of multivariate models for predicting geochemical compositions, and compares the predictive accuracies of different submodel strategies.
% The study finds that while submodels can improve prediction accuracies under certain conditions, the overall effectiveness is contingent upon having a large and diverse calibration dataset.
% The research suggests that the optimal use of \gls{libs} for geochemical analysis requires a balance between the specificity of submodels and the breadth of the calibration dataset to ensure accurate and reliable predictions.

% \citet{kepesImprovingLaserinducedBreakdown2022} discusses enhancing \gls{libs} model accuracy using transfer learning between ChemCam and SuperCam instruments.
% It proposes a method where ChemCam data transforms to approximate SuperCam spectra, improving \gls{cnn} regression models' performance.
% Key methods include data augmentation and fine-tuning of \gls{cnn}s with pre-processed and normalized spectra.
% This approach outperforms some existing models for specific oxides, demonstrating transfer learning's potential in \gls{libs} analyses for more accurate quantitative models.

% \citet{ferreiraComprehensiveComparisonLinear2022} presents an extensive comparison of various algorithms for quantifying lithium in geological samples, with a focus on both linear and non-linear methods.
% The study tested algorithms on spectra acquired from a commercial handheld device and a laboratory prototype, highlighting the challenges in quantifying lithium due to effects like saturation and matrix interference.
% The results showed that non-linear methodologies, such as \gls{knn} regression, \gls{svr}, and \gls{ann} regression, generally outperformed linear methods by effectively managing saturation and matrix effects, which are common in geological samples.
% This research provides valuable insights for future applications in geological sample analysis and could potentially be generalized for other elements in similar contexts.
% The paper's findings are particularly relevant to our project as it demonstrates the effectiveness of non-linear machine learning techniques in handling complex, non-linear relationships in high-dimensional \gls{libs} data, aligning with our research objectives of improving major oxide composition predictions from \gls{libs} data.

% The study by \citet{liuComparisonQuantitativeAnalysis2022} explores the use of \gls{marscode} \gls{libs} for quantitative analysis of olivine in a simulated Martian atmosphere, focusing on multivariate analysis methods to address challenges posed by \gls{libs} data, such as high dimensionality and multicollinearity.
% The methods evaluated include \gls{ulr}, \gls{mvlr}, \gls{pcr}, \gls{plsr}, ridge regression, \gls{lasso}, \gls{enet}, and \gls{bpnn}.
% The findings demonstrate the effectiveness of dimension reduction techniques, especially \gls{plsr}, and nonlinear analysis for improving quantitative analysis accuracy of olivine using \gls{libs} data.
% This approach is particularly relevant to our work due to the focus on advanced statistical methods and machine learning algorithms for handling complex, high-dimensional \gls{libs} data, aligning with our objectives of improving accuracy and robustness in predicting major oxide compositions.

% \cite{yangConvolutionalNeuralNetwork2022} is a study where a \gls{cnn} model is designed to identify twelve types of rocks using \gls{libs} data from the \gls{marscode} for the Tianwen-1 Mars exploration mission.
% The classification performance of the \gls{cnn} is compared with \gls{logreg}, \gls{svm}, and \gls{lda}.
% The \gls{cnn} model achieved the highest classification accuracy, demonstrating its efficiency in rock identification with \gls{libs} spectra collected in a simulated Martian environment.
% This indicates that \gls{cnn}-supported \gls{libs} classification is a promising analytical technique for planetary exploration missions.

% \citet{silvaRobustCalibrationModels2022} introduce clustered regression calibration algorithms for \gls{libs} to address quantification challenges in complex sample matrices or wide concentration ranges, focusing on lithium quantification in geology.
% They employ unsupervised clustering to group similar samples before applying a tailored linear calibration model to each cluster.
% This approach, tested on lithium in exploration drills, outperforms standard linear models, especially in lower concentration regions, and demonstrates good generalizability to unseen data from different rock veins.
% The study highlights the potential of clustered regression methods in improving \gls{libs} quantification accuracy and robustness, particularly valuable in mining environments.
% Uses Ridge regression, \gls{pls}, dimensionality reduction with \gls{umap} and then k-means clustering, followed by a local model for each cluster, univariate calibration curves.

% \citet{woldPrincipalComponentAnalysis1987} provides an in-depth tutorial on \gls{pca}, a fundamental method in multivariate data analysis used for dimensionality reduction, outlier detection, and uncovering the underlying structure in data sets.
% The paper covers the history and development of \gls{pca}, its mathematical foundations, applications across various fields, and detailed instructions for data pre-treatment and \gls{pca} implementation.
% It emphasizes \gls{pca}'s utility in simplifying complex data sets, enhancing interpretability, and supporting data analysis through the extraction of principal components that capture the most variance in the data.
% This work is particularly relevant to our project as it underscores the importance of \gls{pca} in handling high-dimensional data, which aligns with our objectives of improving the prediction accuracy and interpretability of major oxide compositions from \gls{libs} data by effectively managing multicollinearity and high dimensionality challenges.

% \citet{bankAutoencoders2021} details various autoencoder architectures and their applications in machine learning, emphasizing their role in dimensionality reduction, feature learning, and generative models.
% It explores regularized, denoising, and variational autoencoders, highlighting their advantages in compressing data into lower-dimensional spaces while retaining essential information for reconstruction.
% Autoencoders could significantly enhance our ability to handle the high-dimensional nature of \gls{libs} data.
% By compressing spectral data into more manageable representations without losing critical information, we can improve model performance, especially in predicting major oxide compositions, by focusing on the most relevant features extracted from the compressed data representation.
% This aligns with our objectives of efficient dimensionality reduction and robust predictive modeling.

% % Multi-task learning
% In their work \citet{caruana_no_1997}, presents a method called \textit{Multitask learning}, which is a method of learning machine models on several related datasets.
% The motivation for this approach stems from the assumption that utilizing multiple, albeit related, datasets can enhance a model's ability to discern patterns and shapes within the data.
% \citet{caruana_no_1997} suggests that leveraging shared representations for model training can enable the model to identify underlying attributes in other datasets, even when this new data is small.
% This is relevant for our work as one of the major challenges in analyzing LIBS calibration data for Mars is the scarcity of available data.
% This scarcity makes it difficult to construct robust models capable of comprehensively understanding the underlying patterns and physical principles within the data.
% Utilizing related LIBS data could help the models first learn the general outline, shape and patterns in the LIBS data, making it easier for it to grasp the deeper patterns in the Mars related data.

% \citet{Huang2015AnEA} conducted a systematic analysis of data preprocessing techniques for machine learning-based software cost estimation.
% Their study emphasizes the significant influence of tailored data preprocessing strategies on the prediction accuracy of models. Evaluating methods such as feature selection, case selection, scaling, and missing data treatments, they demonstrate that the effectiveness of these techniques varies markedly across different datasets and machine learning algorithms.
% Their findings highlight the necessity of customizing preprocessing approaches to the specific characteristics of each dataset and model.
% This is relevant to our work, as it emphasizes the need to consider the preprocessing techniques and model selection as interdependent components that can significantly impact the predictive performance of machine learning models.
\subsection{Initial Results}
As described in Section~\ref{sec:proposed_approach}, we conducted a series of initial experiments to evaluate the performance of various machine learning models on the prediction of major oxide compositions from our \gls{libs} dataset.
These experiments aimed to provide a preliminary assessment of the models' performance.
All models were trained on the same preprocessed data using the Norm 3 preprocessing method described in Section~\ref{sec:norm3}.
This ensured that the models' performance could be evaluated under consistent and comparable conditions.

Table~\ref{tab:init_results} presents the results of these experiments, including the \gls{rmsep}, \gls{rmsecv}, standard deviation, and standard deviation of cross-validation prediction errors for each model across all oxides.
The means of each metric are also provided to give an overall indication of the models' performance.
Furthermore, we present an overview of these mean values in Figure~\ref{fig:init_results_rmses} to facilitate a visual comparison of the models' general performance.

The results show that the \gls{xgboost} and \gls{svr} models perform the best across all oxides.
These models exhibit the lowest mean \gls{rmsep} and \gls{rmsecv} values, indicating their high accuracy.
Additionally, their mean standard deviation values are among the lowest, underscoring their robustness.
Their good performance is further underscored by Tables~\ref{tab:best_results} and \ref{tab:best_model_occurrences}, which show the best performing model for each oxide and the number of times each model was the best performing model according to any of the metrics, respectively.

Tables~\ref{tab:best_results} and \ref{tab:best_model_occurrences} which list the best-performing model for each oxide and the frequency with which each model achieves top performance according to various metrics, respectively.
It is essential to clarify that these tables are intended to provide a comprehensive overview of model performance rather than to determine an overall 'winner by majority'.
Their purpose is to illustrate the general trends and behavior of different models across various metrics and oxides.
Although \gls{xgboost} and \gls{svr} appear the most frequently in Table~\ref{tab:best_model_occurrences}, this does not imply that they are the best models for every oxide.

On the other hand, the \gls{enet}, \gls{cnn}, and \gls{ann} models perform the worst across all oxides, exhibiting the highest mean \gls{rmsep} and \gls{rmsecv} values, as well as the highest standard deviation values.
This poor performance is further highlighted in Table~\ref{tab:relative_performance}, which shows the relative performance of each model compared to the best-performing model, \gls{xgboost}.
The table also includes the difference in performance relative to the next best model, with \gls{xgboost} serving as the baseline for comparison, assigned a relative performance of 100\%.
From this table, it is evident that the \gls{enet}, \gls{cnn}, and \gls{ann} models experience significant drops in performance compared to the top-performing models.
Consequently, based on their high errors, large standard deviations, and overall poor accuracy and robustness across all oxides, we decided to exclude these models from further experiments.
This decision was made to maintain focus and prevent the inclusion of models that are unlikely to yield significant improvements in performance.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{images/init_results_means.png}
    \caption{Mean \gls{rmsep}, \gls{rmsecv}, standard deviation of prediction errors, and standard deviation of cross-validation prediction errors for each model across all oxides.}
    \label{fig:init_results_rmses}
\end{figure*}

\begin{table}[ht]
\centering
\begin{tabularx}{\linewidth}{lrr}
\toprule
Model & Relative Performance (\%) & Diff. vs Next (\%) \\
\midrule
\gls{xgboost} & 100.00 & 0.85 \\
\gls{svr} & 100.85 & 2.22 \\
\gls{gbr} & 103.07 & 0.87 \\
\gls{ngboost} & 103.94 & 0.51 \\
\gls{rf} & 104.45 & 0.39 \\
\gls{etr} & 104.84 & 3.89 \\
Ridge & 108.74 & 2.92 \\
\gls{pls} & 111.66 & 2.73 \\
\gls{lasso} & 114.38 & 13.44 \\
\gls{ann} & 127.82 & 15.36 \\
\gls{cnn} & 143.18 & 39.18 \\
\gls{enet} & 182.36 & - \\
\bottomrule
\end{tabularx}
\caption{Relative performance of each model compared to the best performing model, and the difference in performance compared to the next best model.}
\label{tab:relative_performance}
\end{table}

\input{sections/results/init_results_table.tex}

\begin{table*}[h]
\centering
\begin{minipage}{.7\textwidth}
  \centering
  \input{sections/results/best_results_table.tex}
  \caption{Lowest metric and corresponding model for each oxide.}
  \label{tab:best_results}
\end{minipage}%
\hspace{0.03\textwidth}
\begin{minipage}{.25\textwidth}
  \centering
  \input{sections/results/best_model_occurrences_table.tex}
  \caption{Occurrences of the best model for each oxide.}
  \label{tab:best_model_occurrences}
\end{minipage}
\end{table*}

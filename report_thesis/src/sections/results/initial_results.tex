\subsection{Initial Results}
As described in Section~\ref{sec:proposed_approach}, we conducted a series of initial experiments to evaluate the performance of various machine learning models on the prediction of major oxide compositions from our \gls{libs} dataset.
These experiments aimed to provide a preliminary assessment of the models' performance.
All models were trained on the same preprocessed data using the Norm 3 preprocessing method described in Section~\ref{sec:norm3}.
This ensured that the models' performance could be evaluated under consistent and comparable conditions.

Table~\ref{tab:init_results} presents the results of these experiments, including the \gls{rmsep}, \gls{rmsecv}, standard deviation, and standard deviation of cross-validation prediction errors for each model across all oxides.
The means of each metric are also provided to give an overall indication of the models' performance.
Furthermore, we present an overview of these mean values in Figure~\ref{fig:init_results_rmses} to facilitate a visual comparison of the models' general performance.

The results show that the \gls{xgboost} and \gls{svr} models perform the best across all oxides.
These models exhibit the lowest mean \gls{rmsep} and \gls{rmsecv} values, indicating their high accuracy.
Additionally, their mean standard deviation values are among the lowest, underscoring their robustness.
Their good performance is further underscored by Tables~\ref{tab:best_results} and \ref{tab:best_model_occurrences}, which show the best performing model for each oxide and the number of times each model was the best performing model according to any of the metrics, respectively.

Tables~\ref{tab:best_results} and \ref{tab:best_model_occurrences} which list the best-performing model for each oxide and the frequency with which each model achieves top performance according to various metrics, respectively.
It is essential to clarify that these tables are intended to provide a comprehensive overview of model performance rather than to determine an overall 'winner by majority'.
Their purpose is to illustrate the general trends and behavior of different models across various metrics and oxides.
Although \gls{xgboost} and \gls{svr} appear the most frequently in Table~\ref{tab:best_model_occurrences}, this does not imply that they are the best models for every oxide.

On the other hand, the \gls{enet}, \gls{cnn}, and \gls{ann} models perform the worst across all oxides, exhibiting the highest mean \gls{rmsep} and \gls{rmsecv} values, as well as the highest standard deviation values.
This poor performance is further highlighted in Table~\ref{tab:relative_performance}, which shows the relative performance of each model compared to the best-performing model, \gls{xgboost}.
The table also includes the difference in performance relative to the next best model, with \gls{xgboost} serving as the baseline for comparison, assigned a relative performance of 100\%.
From this table, it is evident that the \gls{enet}, \gls{cnn}, and \gls{ann} models experience significant drops in performance compared to the top-performing models.

While deep learning models such as \gls{cnn} and \gls{ann} have the theoretical potential to perform well with \gls{libs} data, given their ability to learn complex patterns and relationships, the relatively small size of our dataset may limit their effectiveness.
Furthermore, these models could potentially outperform the other models if tuned more effectively.
However, the sheer number of potential hyperparameters, configurations, architectures, and techniques makes it impractical to conduct a comprehensive search.
Hyperparameter tuning akin to the method detailed in Section~{ref:optuna} could be pursued, but the required computational time would be prohibitive.

% The unexpectedly poor performance of the \gls{enet} model is particularly notable.
% Given that \gls{enet} is designed as a combination of Ridge and \gls{lasso} regression, leveraging both $L_1$ and $L_2$ regularization to balance the benefits of each, it was anticipated to perform at least as well as, if not better than, its individual components.
% Surprisingly, both ridge and \gls{lasso} models outperformed \gls{enet}.
% We expected \gls{enet} to perform better than or at least as well as ridge and \gls{lasso} regression, so this result was unexpected.
% One possible explanation for this anomaly could be the selection of hyperparameters

% We don't know why enet performed so poorly! Ridge and Lasso performed better than enet, which is surprising because enet is supposed to be a combination of the two. Maybe the hyperparameters used were very bad. We keep this in mind for the optimization phase.

% We have a few thoughts on this:
% Deep learning models such as \gls{cnn} and \gls{ann} should in theory work well for LIBS data, given their ability to learn complex patterns and relationships in data, but because the dataset is relatively small, they may not have enough data to learn these patterns effectively.
% Also, it is possible that these models could outperform the other models if they were tuned more effectively, but because of the all the possible permutations of hyperparameters, configurations, architectures and techniques, the search space is too large to explore exhaustively.
% While we could have done hyperparameter tuning similar to what we described in the Optuna section, it would have taken way too long.

% Consequently, based on their high errors, large standard deviations, and overall poor accuracy and robustness across all oxides, we decided to exclude these models from further experiments.
% This decision was made to maintain focus and prevent the inclusion of models that are unlikely to yield significant improvements in performance.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{images/init_results_means.png}
    \caption{Mean \gls{rmsep}, \gls{rmsecv}, standard deviation of prediction errors, and standard deviation of cross-validation prediction errors for each model across all oxides.}
    \label{fig:init_results_rmses}
\end{figure*}

\begin{table}[ht]
\centering
\begin{tabularx}{\linewidth}{lrr}
\toprule
Model & Relative Performance (\%) & Diff. vs Next (\%) \\
\midrule
\gls{xgboost} & 100.00 & 0.85 \\
\gls{svr} & 100.85 & 2.22 \\
\gls{gbr} & 103.07 & 0.87 \\
\gls{ngboost} & 103.94 & 0.51 \\
\gls{rf} & 104.45 & 0.39 \\
\gls{etr} & 104.84 & 3.89 \\
Ridge & 108.74 & 2.92 \\
\gls{pls} & 111.66 & 2.73 \\
\gls{lasso} & 114.38 & 13.44 \\
\gls{ann} & 127.82 & 15.36 \\
\gls{cnn} & 143.18 & 39.18 \\
\gls{enet} & 182.36 & - \\
\bottomrule
\end{tabularx}
\caption{Relative performance of each model compared to the best performing model, and the difference in performance compared to the next best model.}
\label{tab:relative_performance}
\end{table}

\input{sections/results/init_results_table.tex}

\begin{table*}[h]
\centering
\begin{minipage}{.7\textwidth}
  \centering
  \input{sections/results/best_results_table.tex}
  \caption{Lowest metric and corresponding model for each oxide.}
  \label{tab:best_results}
\end{minipage}%
\hspace{0.03\textwidth}
\begin{minipage}{.25\textwidth}
  \centering
  \input{sections/results/best_model_occurrences_table.tex}
  \caption{Occurrences of the best model for each oxide.}
  \label{tab:best_model_occurrences}
\end{minipage}
\end{table*}

\subsection{Initial Results}
As described in Section~\ref{sec:proposed_approach}, we conducted a series of initial experiments to evaluate the performance of various machine learning models on the prediction of major oxide compositions from our \gls{libs} dataset.
These experiments aimed to provide a preliminary assessment of the models' performance.
All models were trained on the same preprocessed data using the Norm 3 preprocessing method described in Section~\ref{sec:norm3}.
This ensured that the models' performance could be evaluated under consistent and comparable conditions.

Table~\ref{tab:init_results} presents the results of these experiments, including the \gls{rmsep}, \gls{rmsecv}, standard deviation, standard deviation of cross-validation, and the mean of these metrics for each model and oxide.
Figure~\ref{fig:init_results_rmses} shows the mean \gls{rmsep}, \gls{rmsecv}, standard deviation of prediction errors, and standard deviation of cross-validation prediction errors for each model across all oxides.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{images/init_results_means.png}
    \caption{Mean \gls{rmsep}, \gls{rmsecv}, standard deviation of prediction errors, and standard deviation of cross-validation prediction errors for each model across all oxides.}
    \label{fig:init_results_rmses}
\end{figure*}

Table~\ref{tab:relative_performance} shows the relative performance of each model compared to the best performing model, \gls{xgboost}.
The table also shows the difference in performance compared to the next best model.
The \gls{xgboost} model is used as the baseline for comparison, with a relative performance of 100\%.

\begin{table}[ht]
\centering
\begin{tabularx}{\linewidth}{lrr}
\toprule
Model & Relative Performance (\%) & Diff. vs Next (\%) \\
\midrule
\gls{xgboost} & 100.00 & 0.85 \\
\gls{svr} & 100.85 & 2.22 \\
\gls{gbr} & 103.07 & 0.87 \\
\gls{ngboost} & 103.94 & 0.51 \\
\gls{rf} & 104.45 & 0.39 \\
\gls{etr} & 104.84 & 3.89 \\
Ridge & 108.74 & 2.92 \\
\gls{pls} & 111.66 & 2.73 \\
\gls{lasso} & 114.38 & 13.44 \\
\gls{ann} & 127.82 & 15.36 \\
\gls{cnn} & 143.18 & 39.18 \\
\gls{enet} & 182.36 & - \\
\bottomrule
\end{tabularx}
\caption{Relative performance of each model compared to the best performing model, and the difference in performance compared to the next best model.}
\label{tab:relative_performance}
\end{table}

The results show that the \gls{xgboost}, \gls{svr}, and \gls{gbr} models perform the best across all oxides.
These models have the lowest mean \gls{rmsep} and \gls{rmsecv} values, indicating that they provide the most accurate predictions.
In addition, the mean standard deviation values for these models are also among the lowest, which underscores their robustness.

On the other hand, the \gls{enet}, \gls{cnn}, and \gls{ann} models perform the worst across all oxides.
These models have the highest mean \gls{rmsep} and \gls{rmsecv} values, and their standard deviation values are also the highest.
From Table~\ref{tab:relative_performance}, it is evident that these models also make drastic jumps in performance compared to the best performing models.
Based on this and the general poor accuracy and robustness observed across all oxides as evidenced by the high errors and standard deviations, we decided to exclude these models from further experiments.

% Notes on the results:
% \begin{itemize}
%     \item \textbf{Best performing models}: XGBoost, SVR and GBR perform the best.
%     \item \textbf{Worst performing models}: Elastic Net, CNN and ANN perform the worst.
%     \item \textbf{Oxide specific performance}: \ce{SiO2} and \ce{FeO_T} generally have higher \gls{rmsep} and \gls{rmsecv} values across most models, which is expected based on previous results. These oxides are known to be difficult to predict. On the other hand, \ce{MgO} and \ce{Na2O} have lower \gls{rmsep} and \gls{rmsecv} values across most models.
%     \item \textbf{Mention mean values}: The mean values for RMSEP and RMSECV provide an overall performance indicator for each model. For example, XGBoost, GBR, and SVR have the lowest mean RMSEP and RMSECV values. The mean standard deviation values for these models are also lower - underscores reliability.
%     \item \textbf{Relative performance}: Discuss this - mention that the jump between the three worst models is really high. Use this as an argument to justify the exclusion of these models from further experiments.
% \end{itemize}



\input{sections/results/init_results_table.tex}
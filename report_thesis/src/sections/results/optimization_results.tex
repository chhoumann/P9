\subsection{Optimization Results}\label{sec:optimization_results}
In this section, we present and analyze the results of running the Optimization experiment that we described in Section~\ref{subsec:optimization_experiment_design}.
As mentioned, our primary objective was to identify the optimal configurations for predicting the concentration of various oxides in our dataset.
We systematically evaluated a range of machine learning models, preprocessing techniques, and hyperparameter settings to determine the most effective combinations for each oxide.

The results of the experiment were ~$16.000$ trials worth of data on the configurations used, hyperparameters, as well as metrics.

Our data cleaning for this dataset primarily included filtering out failed runs, which was caused by configurations that did not work well together, as well as filtering out extreme error values.
We filter out any runs that had a \gls{rmsecv} above 50.
Approaches like \gls{svr} could occasionally yield this kind of outlier result in specific configurations.
We selected 50 as threshold to include as many trials that were not obvious outliers.

Our experiment underwent mostly without issues.
Some issues are to be expected given the scale of the experiment.
Unfortunately, we encountered an issue with a server we were using, resulting in some oxides and models having to be re-run.
We managed to recover and re-run most of these.
However, \gls{ngboost} for \ce{MgO} was only partially finished.
Given that each model of the ten models would undergo 200 trials, for each oxide, this resulted in 2000 runs per oxide.
The exception is \ce{MgO}, for which \gls{ngboost} ran 143 trials, making the total trials for \ce{MgO} 1943.
After the filtering process, we are left with a total of 15245 trials to analyze.

Given that we stored the configurations as well as each hyperparameter value for the trials, we had \~100 variables to consider during our analysis.
Of these, the primary variables of interest are the metrics, as well as overall configuration variables, namely \texttt{Model Type}, \texttt{Scaler Type}, \texttt{PCA Type}, \texttt{Transformer Type}.

We use this data to identify the best configurations for each oxide, as measured by \gls{rmsecv}.
Our analysis starts broad, examining the usage of preprocessors across trials. Then we narrow in and look at the top 100 trials for each oxide to identify the best model, scaler, and transformer for each oxide.
Finally, we look at the single best performing configurations across oxides, showing one configuration per model, for each oxide.

As we described in Section~\ref{sec:optimization_framework}, our optimization system searches for the best configurations by through multi-objective optimization.
The optimization process is facilitated by adjusting the configuration and hyperparameters of the machine learning model and preprocessing pipeline, therefore minimizing the objective.
Because the sampler will do initial exploration, and then attempt to find the optimal configuration through exploitation, we'll see that the values of variables that frequently appear in the results are the ones that are most likely to be optimal.
Looking at the results in tables \ref{tab:scalers_comparison}, \ref{tab:pca_comparison}, and \ref{tab:transformers_comparison}, we can see these values for the various preprocessors.
The optimization results show that the configurations with the highest total values across oxides are indicative of being the most frequently exploited, hence most likely to optimize performance. 
\texttt{Norm3Scaler} was used in 5090 trials, and therefore appears to be the most effective scaler.
This was somewhat expected, given the method was created for this particular type of \gls{libs} dataset, as discussed in Section~\ref{sec:norm3}.
For dimensionality reduction, we see that \texttt{None}, indicating no \gls{pca}, was used in 9419 trials, which is \~59\% of all trials.
This suggests that either no dimensionality reduction is optimal, or a better method should be identified.
Neither \gls{pca} nor \gls{kernel-pca} are very effective for this dataset.
\texttt{QuantileTransformer} was used in 5710 trials, and therefore seems to be the most optimal transformer.
We note that \texttt{PowerTransformer} was used in 5277 trials, with and no transformer in 4956. As opposed to the other preprocessing method types, there does not seem to be a clear winner for the transformers.

\input{sections/results/oxide_comparison_table.tex}

We wish to examine how the top 100 trials performed for each configuration.
We present the results in Figure~\ref{fig:top100_models}, \ref{fig:top100_scalers}, \ref{fig:top100_transformers}, and \ref{fig:top100_pca}.
These figures and their corresponding subplots illustrate the performance of various configuration elements (models, scalers, transformers, PCA techniques) for each oxide, based on their \gls{rmsecv}.
Any elements that do not appear in a subplot were not used in the top 100 trials for the given oxide.
It is important to note that the variance in performance is influenced by multiple factors, not solely by the variable depicted in each plot.
Factors such as the interaction between different preprocessing techniques, the specific hyperparameter settings, and the inherent variability in the data can all contribute to the observed performance.
Therefore, while the plots provide valuable insights into the effectiveness of individual configuration elements, the overall performance is a result of complex interactions within the entire machine learning pipeline.
We therefore emphasize the top performing trials and analyze a larger sample of these to draw generalizable conclusions about the optimal configurations for each oxide.

From Figure~\ref{fig:top100_models}, it becomes clear that \gls{svr}, gradient boosting methods, and \gls{pls} perform the best.
Figure~\ref{fig:top100_pca} confirms our previous hypothesis that not using any \gls{pca} or \gls{kernel-pca} results in the lowest \gls{rmsecv} values.
We do, however, see some form of \gls{pca} or \gls{kernel-pca} appear in four of the plots, with \gls{kernel-pca} being the most frequently used of them.
This indicates that they do see use among the top performing configurations.
Interestingly, Figure~\ref{fig:top100_scalers} shows that, while \texttt{Norm3Scaler} is the most frequently used scaler and often best-performing scaler, it isn't always the case.
Min-max scaling seems to produce better results for \ce{SiO2} and \ce{CaO}, and robust scaling for \ce{MgO}.
In \ce{Al2O3}, Norm 3 scaling has the lowest \gls{rmsecv} values, but a worse mean \gls{rmsecv} value than the other scalers.
Finally, Figure~\ref{fig:top100_transformers} another nuanced result.
Power transformations seem to most frequently produce the best result across oxides, with the quantile transformation or none being showing the lowest \gls{rmsecv} values for the remaining oxides.

These results seem to further reinforce our hypothesis that a tailored configuration is needed for each oxide, and there isn't a single configuration that works well for all oxides.

\input{sections/results/top100.tex}

We conclude our analysis by showing the best configurations for each oxide in Section~\ref{subsec:best_model_configurations}.
The section shows the single top-performing configurations, shown for each model, for each oxide, via tables~\ref{tab:SiO2_best_configurations} through~\ref{tab:K2O_best_configurations}.
Like the previous plots, we use the \gls{rmsecv} values to determine the best configurations.
Notably, these tables illustrate how some configurations can have low \gls{rmsecv} values, but relatively high \gls{rmsep} values.
This could indicate that they generalize well to the dataset containing extreme values, yet struggle values closer to the mean.
For instance, the top performing \ce{SiO2} configuration is \gls{pls} with \gls{kernel-pca} and \texttt{MinMaxScaler}.
This configuration has the lowest \gls{rmsecv} value of 4.55, but a relatively high \gls{rmsep} value of 4.08.
The next best performing configuration for \ce{SiO2} is \gls{svr} with \texttt{MinMaxScaler}.
This configuration has a \gls{rmsecv} value of 4.59, but a \gls{rmsep} value of 3.53.
The difference in \gls{rmsecv} is negligible, however the \gls{rmsep} value is much lower than the \gls{rmsecv} value for the \gls{pls} configuration.
This indicates that the \gls{svr} configuration is likely a better overall predictor for \ce{SiO2} than the \gls{pls} configuration.

The analysis of the best configurations for each oxide reveals that certain models and preprocessing techniques consistently outperform others. SVR and PLS models, in particular, frequently appear among the top configurations. The use of transformers such as the Power Transformer and scalers like Norm3 and Min-Max Scaler are also common among the best configurations.

Finally, we use a combination of these top-performing configurations by selecting the top-$n$ performing configurations per oxide for our stacking ensemble.
This will be further described in Section~\ref{subsec:stacking_ensemble}.